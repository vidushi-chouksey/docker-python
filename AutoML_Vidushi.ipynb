{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "AutoML_Vidushi.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw_QsPlrYyri"
      },
      "source": [
        "**AutoML Assignment:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mf_iTQEZIT0"
      },
      "source": [
        "*Problem statement*: **Machine Learning Pipeline Automation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_eFgEF9bCLR"
      },
      "source": [
        "Build an accelerator to automate all the steps in ML model development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3TBC7_vgw2t"
      },
      "source": [
        "# Write an Automated ML function to be called using any data frame (dataset) to give a good trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2Cfd5hyjs4F"
      },
      "source": [
        "#Write an AutoML function.\n",
        "def automl(**kwargs):\n",
        "  from sklearn import datasets\n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "  import matplotlib.pyplot as plt\n",
        "  import seaborn as sns\n",
        "  import requests\n",
        "  from bs4 import BeautifulSoup\n",
        "  import geopandas as gpd\n",
        "  from prettytable import PrettyTable\n",
        "  from autokeras import StructuredDataClassifier\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  #Define a list of URLs for Web Scraping.\n",
        "  url_list = [\"https://www.kaggle.com/datasets?datasetsOnly=true\", \"https://public.knoema.com/\", \"https://www.kaggle.com/c/nfl-health-and-safety-helmet-assignment/data\", \"http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets/\", \"https://www.kaggle.com/mysarahmadbhat/bmw-used-car-listing\", \"https://www.climate.gov/maps-data/datasets\"]\n",
        "  for url in url_list:\n",
        "    #Make a GET request to fetch the raw HTML content.\n",
        "    web_content = requests.get(url).content\n",
        "    #Parse the html content.\n",
        "    soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "    #Remove any newlines and extra spaces from left and right.\n",
        "    extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "    #Find all table rows and data cells within.\n",
        "    stats = [] \n",
        "    all_rows = soup.find_all('tr')\n",
        "    for row in all_rows:\n",
        "        stat = extract_contents(row.find_all('td')) \n",
        "    #Notice that the data that we require is now a list of length 5.\n",
        "        if len(stat) == 5:\n",
        "            stats.append(stat)\n",
        "    #Now convert the data into a pandas dataframe for further processing.\n",
        "    new_cols = []\n",
        "    for each_new_col in row:\n",
        "      kaggle_data = pd.DataFrame(data = kaggle_data, columns = each_new_col)\n",
        "      kaggle_data.head()\n",
        "      #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "      kaggle_data[each_new_col] = state_data[each_new_col].map(int)\n",
        "\n",
        "    from mlbox.optimisation import Optimiser, Regressor\n",
        "    \n",
        "    #Evaluate the pipeline.\n",
        "    opt = Optimiser()\n",
        "    params = {\"ne__numerical_strategy\" : 0, \"ce__strategy\" : \"label_encoding\", \"fs__threshold\" : 0.1, \"stck__base_estimators\" : [Regressor(strategy=\"RandomForest\"), Regressor(strategy=\"ExtraTrees\")], \"est__strategy\" : \"Linear\"}\n",
        "    df = {\"train\" : pd.DataFrame(train_data.iloc[:,:-1]), \"target\" : pd.Series(test_data.iloc[:,-1])}\n",
        "\n",
        "    #Build a keras model.\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "    model = keras.Sequential()\n",
        "    #Relu: Rectified Linear Unit.\n",
        "    #Adds a densely-connected layer with 64 units to the model.\n",
        "    model.add(keras.layers.Dense(64, activation='relu'))\n",
        "    #Add another.\n",
        "    model.add(keras.layers.Dense(64, activation='relu'))\n",
        "    #Add a softmax layer with 10 output units.\n",
        "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "    #Define a ConvModel.\n",
        "    class ConvModel(tf.keras.Model):\n",
        "        def __init__(self, nfs, input_shape, output_shape, use_bn=False, use_dp=False):\n",
        "            super(ConvModel, self).__init__(name='mlp')\n",
        "            self.use_bn = use_bn\n",
        "            self.use_dp = use_dp\n",
        "            self.num_classes = num_classes\n",
        "\n",
        "            #Backbone layers\n",
        "            self.convs = [ConvLayer(nfs[0], s=1, input_shape=input_shape)]\n",
        "            self.convs += [ConvLayer(nf) for nf in nfs[1:]]\n",
        "            #Classification layers\n",
        "            self.convs.append(AveragePooling2D())\n",
        "            self.convs.append(Dense(output_shape, activation='softmax'))\n",
        "\n",
        "        def call(self, inputs):\n",
        "            for layer in self.convs: inputs = layer(inputs)\n",
        "            return inputs\n",
        "    #Compile the model.\n",
        "    model.compile(loss='categorical crossentropy', metrics=['accuracy'], optimizer='rmsprop')\n",
        "    model.build((None, 32, 32, 3))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    import requests\n",
        "    from bs4 import BeautifulSoup\n",
        "    #Import H2O GBM.\n",
        "    from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "    #Make a GET request to fetch the raw HTML content.\n",
        "    web_content = requests.get(url).content\n",
        "    #Parse the html content.\n",
        "    soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "    #Remove any newlines and extra spaces from left and right.\n",
        "    extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "    #Find all table rows and data cells within.\n",
        "    stats = [] \n",
        "    all_rows = soup.find_all('tr')\n",
        "    for row in all_rows:\n",
        "      stat = extract_contents(row.find_all('td')) \n",
        "      # Notice that the data that we require is now a list of length 5.\n",
        "      if len(stat) == 5:\n",
        "        stats.append(stat)\n",
        "      #Now convert the data into a pandas dataframe for further processing.\n",
        "      new_cols = []\n",
        "      for each_new_col in row:\n",
        "        stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "        stats_data.head()\n",
        "        #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "        kaggle_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "        X, y = stats_data\n",
        "        X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=11)\n",
        "        \n",
        "        history = model.fit(x_train, y_train,\n",
        "                        batch_size=64,\n",
        "                        epochs=1000)\n",
        "\n",
        "        model.summary()\n",
        "        input_shape = (2, 3, 4)\n",
        "        x1 = tf.random.normal(input_shape)\n",
        "        x2 = tf.random.normal(input_shape)\n",
        "        y = tf.keras.layers.Add()([x1, x2])\n",
        "        print(y.shape)\n",
        "\n",
        "        tf.keras.layers.LSTM(3, activation='tanh', recurrent_activation='sigmoid',\n",
        "            use_bias=True, kernel_initializer='glorot_uniform',\n",
        "            recurrent_initializer='orthogonal',\n",
        "            bias_initializer='zeros', unit_forget_bias=True, dropout=0.0, recurrent_dropout=0.0,\n",
        "            return_sequences=False, return_state=False, go_backwards=False, stateful=False,\n",
        "            time_major=False, unroll=False)\n",
        "\n",
        "        #Define a ConvLayer.\n",
        "        class ConvLayer(Layer) :\n",
        "            def __init__(self, nf, ks=3, s=2, **kwargs):\n",
        "                self.nf = nf\n",
        "                self.grelu = GeneralReLU(leak=0.01)\n",
        "                self.conv = (Conv2D(filters     = nf,\n",
        "                                    kernel_size = ks,\n",
        "                                    strides     = s,\n",
        "                                    padding     = \"same\",\n",
        "                                    use_bias    = False,\n",
        "                                    activation  = \"linear\"))\n",
        "                super(ConvLayer, self).__init__(**kwargs)\n",
        "\n",
        "            def rsub(self): return -self.grelu.sub\n",
        "            def set_sub(self, v): self.grelu.sub = -v\n",
        "            def conv_weights(self): return self.conv.weight[0]\n",
        "\n",
        "            def build(self, input_shape):\n",
        "                # No weight to train.\n",
        "                super(ConvLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "            def compute_output_shape(self, input_shape):\n",
        "                output_shape = (input_shape[0],\n",
        "                                input_shape[1]/2,\n",
        "                                input_shape[2]/2,\n",
        "                                self.nf)\n",
        "                return output_shape\n",
        "\n",
        "        def call(self, x):\n",
        "            return self.grelu(self.conv(x))\n",
        "\n",
        "        def __repr__(self):\n",
        "            return f'ConvLayer(nf={self.nf}, activation={self.grelu})'\n",
        "\n",
        "    opt.evaluate(params, df)\n",
        "\n",
        "    datasets_dict = {\n",
        "    \"iris\": datasets.load_iris(), \n",
        "    \"boston\": datasets.load_boston(),\n",
        "    \"breast_cancer\": datasets.load_breast_cancer(),\n",
        "    \"diabetes\": datasets.load_diabetes(),\n",
        "    \"wine\": datasets.load_wine(),\n",
        "    \"linnerud\": datasets.load_linnerud(),\n",
        "    \"digits\": datasets.load_digits(),\n",
        "    \"kaggle_data_list\": \n",
        "    pd.DataFrame({\n",
        "    \"Latest_Covid-19_India_Status\":pd.read_csv(\"Latest Covid-19 India Status.csv\", sep=','),\n",
        "    \"Pueblos_Magicos\": pd.read_csv(\"pueblosMagicos.csv\", sep=','),\n",
        "    \"Apple_iphone_SE_reviews&ratings\": pd.read_csv(\"APPLE_iPhone_SE.csv\", sep=',')\n",
        "    })\n",
        "                  }\n",
        "\n",
        "    if len(datasets_dict[\"kaggle_data_list\"])!=0:\n",
        "      for i in range(len(datasets_dict.get(\"kaggle_data_list\"))):\n",
        "        df=df.iloc[:]\n",
        "        print(df.head())\n",
        "        print(df.tail())\n",
        "        print(df.info())\n",
        "        print(df.describe())\n",
        "\n",
        "        from autoPyTorch import AutoNetClassification\n",
        "        #Data and metric imports.\n",
        "        import sklearn.model_selection\n",
        "        import sklearn.metrics\n",
        "        X, y = df.to_numpy()\n",
        "        X_train, X_test, y_train, y_test = \\\n",
        "                sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
        "\n",
        "        #Run Auto-PyTorch.\n",
        "        autoPyTorch = AutoNetClassification(\"tiny_cs\",  # config preset\n",
        "                                            log_level='info',\n",
        "                                            max_runtime=999999999**10000000,\n",
        "                                            min_budget=30,\n",
        "                                            max_budget=999999999*100000)\n",
        "        #Fit.\n",
        "        autoPyTorch.fit(X_train, y_train, validation_split=0.3)\n",
        "        #Predict.\n",
        "        y_pred = autoPyTorch.predict(X_test)\n",
        "        #Get the accuracy score.\n",
        "        print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "    else:\n",
        "      for each_dataset in datasets_dict:\n",
        "        print(each_dataset,\" dataset:\")\n",
        "        print(\"Data: \",each_dataset.data)\n",
        "        print(\"Target: \", each_dataset.target)\n",
        "        print(\"Target names: \", each_dataset.target_names)\n",
        "        print(\"Description: \", each_dataset.DESCR)\n",
        "        #Shape\n",
        "        print(\"Shape of the data: \", each_dataset.data.shape)\n",
        "        print(\"Shape of the target: \",each_dataset.target.shape)\n",
        "        #Type\n",
        "        print(\"Type of the data: \", type(each_dataset.data.shape))\n",
        "        print(\"Type of the data: \", type(each_dataset.target.shape))\n",
        "        #Dimensions\n",
        "        print(\"Number of dimensions of the data: \", each_dataset.data.ndim)\n",
        "        print(\"Number of dimensions of the target: \",each_dataset.target.ndim)\n",
        "        #Number of samples and features\n",
        "        n_samples, n_features = each_dataset.data.shape\n",
        "        print(\"Number of samples: \", n_samples)\n",
        "        print(\"Number of features: \", n_features)\n",
        "        #Keys\n",
        "        print(\"Keys: \", each_dataset.keys())\n",
        "        X, y = digits.data, digits.target\n",
        "        #View the first and last 5 rows of the pandas dataframe.\n",
        "        df=pd.DataFrame(X, columns=digits.feature_names)\n",
        "        print(df.head())\n",
        "        print(df.tail())\n",
        "        #print(digits.data[0])\n",
        "\n",
        "        #Visualize data on its principal components.\n",
        "        #PCA: Principal Component Analysis\n",
        "        from sklearn.decomposition import PCA\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        pca = PCA(n_components=2)\n",
        "        proj = pca.fit_transform(each_dataset.data)\n",
        "        plt.scatter(proj[:,0], proj[:,1], c=each_dataset.target, cmap=\"Paired\")\n",
        "        plt.colorbar()\n",
        "\n",
        "        #Gaussian Naive-Bayes classification:\n",
        "        from sklearn.naive_bayes import GaussianNB\n",
        "        from sklearn.model_selection import train_test_split\n",
        "\n",
        "        #Split the dataset into training and validation sets.\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            each_dataset.data, each_dataset.target)\n",
        "\n",
        "        #Train the model.\n",
        "        clf = GaussianNB()\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        #Use the model to predict the labels of the test data.\n",
        "        predicted = clf.predict(X_test)\n",
        "        expected = y_test\n",
        "\n",
        "        #Plot the prediction.\n",
        "        fig = plt.figure(figsize=(6, 6))  # Figure size is in inches.\n",
        "        fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "        #Plot the digits: each image is 8x8 pixels.\n",
        "        for i in range(64):\n",
        "            ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
        "            ax.imshow(X_test.reshape(-1, 8, 8)[i], cmap=plt.cm.binary,\n",
        "                      interpolation='nearest')\n",
        "\n",
        "            #Label the image with the target value.\n",
        "            if predicted[i] == expected[i]:\n",
        "                ax.text(0, 7, str(predicted[i]), color='green')\n",
        "            else:\n",
        "                ax.text(0, 7, str(predicted[i]), color='red')\n",
        "\n",
        "        #Quantify performance.\n",
        "        #Number of correct matches\n",
        "        matches = (predicted == expected)\n",
        "        print(matches.sum())\n",
        "        #Total nunber of data points\n",
        "        print(len(matches))\n",
        "        #Ratio of correct predictions\n",
        "        matches.sum() / float(len(matches))\n",
        "\n",
        "        #Print the classification report.\n",
        "        from sklearn import metrics\n",
        "        print(metrics.classification_report(expected, predicted))\n",
        "        #Obtain the confusion matrix.\n",
        "        print(metrics.confusion_matrix(expected, predicted))\n",
        "        plt.show() \n",
        "\n",
        "        #AutoGluon\n",
        "        #Tabular prediction with AutoGluon:\n",
        "        #Predict Columns in a Table.\n",
        "        from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "        train_data = TabularDataset(each_dataset)\n",
        "        subsample_size = 55500000  # subsample subset of data for faster demo\n",
        "        train_data = train_data.sample(n=subsample_size, random_state=0)\n",
        "        train_data.head()\n",
        "        label = 'class'\n",
        "        print(\"Summary of class variable: \\n\", train_data[label].describe())\n",
        "        #Use AutoGluon to train multiple models.\n",
        "        save_path = 'agModels-predictClass'  # Specifies folder to store trained models.\n",
        "        predictor = TabularPredictor(label=label, path=save_path).fit(train_data)\n",
        "        test_data = TabularDataset(each_dataset)\n",
        "        y_test = test_data[label]  # Values to predict.\n",
        "        test_data_nolab = test_data.drop(columns=[label])  # Delete label column to prove we're not cheating.\n",
        "        test_data_nolab.head()\n",
        "        #Predict.\n",
        "        y_pred = predictor.predict(test_data_nolab)\n",
        "        print(\"Predictions:  \\n\", y_pred)\n",
        "        perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)\n",
        "        predictor.leaderboard(test_data, silent=True)\n",
        "        from autogluon.tabular import TabularPredictor\n",
        "        predictor = TabularPredictor(label=label).fit(train_data=each_dataset)\n",
        "        #.fit() returns a predictor object.\n",
        "        pred_probs = predictor.predict_proba(test_data_nolab)\n",
        "        pred_probs.head(5)\n",
        "        #Summarize what happened during fit.\n",
        "        results = predictor.fit_summary(show_plot=True)\n",
        "        print(\"AutoGluon infers problem type is: \", predictor.problem_type)\n",
        "        print(\"AutoGluon identified the following types of features:\")\n",
        "        print(predictor.feature_metadata)\n",
        "        predictor.leaderboard(test_data, silent=True)\n",
        "        predictor.predict(test_data, model='LightGBM')\n",
        "        #Maximizing predictive performance.\n",
        "        time_limit = 11  \n",
        "        metric = 'roc_auc'  # Specify the evaluation metric here.\n",
        "        predictor = TabularPredictor(label, eval_metric=metric).fit(train_data, time_limit=time_limit, presets='best_quality')\n",
        "        predictor.leaderboard(test_data, silent=True)\n",
        "\n",
        "        #Regression (predicting numeric table columns)\n",
        "        column = 'column'\n",
        "        print(\"Summary of PUEBLO variable: \\n\", train_data[column].describe())\n",
        "        predictor_column = TabularPredictor(label=column, path=\"agModels-predictAge\").fit(train_data, time_limit=60)\n",
        "        performance = predictor_column.evaluate(test_data)\n",
        "        #See the per-model performance.\n",
        "        predictor_column.leaderboard(test_data, silent=True)\n",
        "        \n",
        "\n",
        "        #MLbox:\n",
        "        from mlbox.optimisation import Optimiser\n",
        "        from sklearn import datasets\n",
        "        best = opt.optimise(space, df, 3)\n",
        "        #Optimise the pipeline.\n",
        "        opt = Optimiser()\n",
        "        space = {\n",
        "        'fs__strategy':{\"search\":\"choice\",\"space\":[\"variance\",\"rf_feature_importance\"]},\n",
        "        'est__colsample_bytree':{\"search\":\"uniform\", \"space\":[0.3,0.7]}\n",
        "        }\n",
        "        df = {\"train\" : pd.DataFrame(each_dataset.data), \"target\" : pd.Series(each_dataset.target)} \n",
        "        #Evaluate the pipeline.\n",
        "        opt = Optimiser()\n",
        "        params = {\n",
        "        \"ne__numerical_strategy\" : 0,\n",
        "        \"ce__strategy\" : \"label_encoding\",\n",
        "        \"fs__threshold\" : 0.1,\n",
        "        \"stck__base_estimators\" : [Regressor(strategy=\"RandomForest\"), Regressor(strategy=\"ExtraTrees\")],\n",
        "        \"est__strategy\" : \"Linear\"\n",
        "        }\n",
        "        df = {\"train\" : pd.DataFrame(each_dataset.data), \"target\" : pd.Series(each_dataset.target)}\n",
        "        opt.evaluate(params, df)\n",
        "\n",
        "\n",
        "        #TPOT\n",
        "        #Classification\n",
        "        from tpot import TPOTClassifier\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        #Perform a train test split.\n",
        "        X_train, X_test, y_train, y_test = train_test_split(each_dataset.data, each_dataset.target, train_size=0.75, test_size=0.25)\n",
        "        \n",
        "        tpot=TPOTClassifier(generations=99, population_size=99, mutation_rate=0.7, crossover_rate=0.3, random_state=111, cv=5, subsample=0.98, verbosity=2, n_jobs=-2, max_eval_time_mins=0.00000001, config_dict='TPOT light', memory='áuto', log_file='tpot_datasets_logs')\n",
        "        tpot.fit(X_train, y_train)\n",
        "        print(tpot.score(X_test, y_test))\n",
        "        tpot.export('tpot_datasets_pipeline.py')\n",
        "\n",
        "        plt.hist(each_dataset.target)\n",
        "\n",
        "        for index, feature_name in enumerate(each_dataset.feature_names):\n",
        "          plt.figure()\n",
        "          plt.scatter(each_dataset.data[:, index], each_dataset.target) \n",
        "          plt.show()\n",
        "\n",
        "        from sklearn import model_selection\n",
        "        X = each_dataset.data\n",
        "        y = each_dataset.target\n",
        "\n",
        "        X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y,\n",
        "                                                test_size=0.25, random_state=0)\n",
        "\n",
        "        print(\"%r, %r, %r\" % (X.shape, X_train.shape, X_test.shape))\n",
        "\n",
        "        clf = KNeighborsClassifier(n_neighbors=1).fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_test)\n",
        "        print(metrics.confusion_matrix(y_test, y_pred))\n",
        "        print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "        #Auto-Pytorch\n",
        "        from autoPyTorch import AutoNetClassification\n",
        "\n",
        "        #Data and metric imports\n",
        "        import sklearn.model_selection\n",
        "        import sklearn.datasets\n",
        "        import sklearn.metrics\n",
        "        X, y = each_dataset(return_X_y=True)\n",
        "        X_train, X_test, y_train, y_test = \\\n",
        "                sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
        "\n",
        "        #Run Auto-PyTorch on the datasets.\n",
        "        autoPyTorch = AutoNetClassification(\"tiny_cs\",  # config preset\n",
        "                                            log_level='info',\n",
        "                                            max_runtime=999999999**10000000,\n",
        "                                            min_budget=30,\n",
        "                                            max_budget=999999999*100000)\n",
        "        autoPyTorch.fit(X_train, y_train, validation_split=0.3)\n",
        "        y_pred = autoPyTorch.predict(X_test)\n",
        "\n",
        "        print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "\n",
        "        #Auto-Sklearn\n",
        "        from sklearn import datasets\n",
        "        import autosklearn.classification\n",
        "        cls = autosklearn.classification.AutoSklearnClassifier()\n",
        "        \n",
        "        X, y = each_dataset(return_X_y=True)\n",
        "        X_train, X_test, y_train, y_test = \\\n",
        "                sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
        "\n",
        "        cls.fit(X_train, y_train)\n",
        "        predictions = cls.predict(X_test)\n",
        "\n",
        "        import sklearn.model_selection\n",
        "        import sklearn.metrics\n",
        "        if __name__ == \"__main__\":\n",
        "            X, y = each_dataset(return_X_y=True)\n",
        "            X_train, X_test, y_train, y_test = \\\n",
        "                    sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
        "            automl = autosklearn.classification.AutoSklearnClassifier()\n",
        "\n",
        "            automl.fit(X_train, y_train)\n",
        "            y_hat = automl.predict(X_test)\n",
        "            print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_hat))\n",
        "\n",
        "        import numpy as np\n",
        "        import tensorflow as tf\n",
        "        import autokeras as ak\n",
        "        input_node = ak.ImageInput()\n",
        "        output_node = ak.Normalization()(input_node)\n",
        "        output_node1 = ak.ConvBlock()(output_node)\n",
        "        output_node2 = ak.ResNetBlock(version=\"v2\")(output_node)\n",
        "        output_node = ak.Merge()([output_node1, output_node2])\n",
        "        output_node = ak.ClassificationHead()(output_node)\n",
        "\n",
        "        auto_model = ak.AutoModel(inputs=input_node, outputs=output_node, overwrite=True, max_trials=100)\n",
        "        #Prepare data to run the model.\n",
        "        (x_train, y_train), (x_test, y_test) = each_datset\n",
        "        print(x_train.shape)\n",
        "        print(y_train.shape)\n",
        "        print(y_train[:3])\n",
        "\n",
        "        #Feed the AutoModel with training data.\n",
        "        auto_model.fit(x_train[:100], y_train[:100], epochs=1000)\n",
        "        #Predict with the best model.\n",
        "        predicted_y = auto_model.predict(x_test)\n",
        "        #Evaluate the best model with testing data.\n",
        "        print(auto_model.evaluate(x_test, y_test))\n",
        "\n",
        "        #Implement new block.\n",
        "        class SingleDenseLayerBlock(ak.Block):\n",
        "            def build(self, hp, inputs=None):\n",
        "                #Get the input_node from inputs.\n",
        "                input_node = tf.nest.flatten(inputs)[0]\n",
        "                layer = tf.keras.layers.Dense(\n",
        "                    hp.Int(\"num_units\", min_value=32, max_value=512, step=32)\n",
        "                )\n",
        "                output_node = layer(input_node)\n",
        "                return output_node\n",
        "\n",
        "        #Build the AutoModel.\n",
        "        input_node = ak.Input()\n",
        "        output_node = SingleDenseLayerBlock()(input_node)\n",
        "        output_node = ak.RegressionHead()(output_node)\n",
        "        auto_model = ak.AutoModel(input_node, output_node, overwrite=True, max_trials=100)\n",
        "        #Prepare the data.\n",
        "        num_instances = 100\n",
        "        x_train = np.random.rand(num_instances, 20).astype(np.float32)\n",
        "        y_train = np.random.rand(num_instances, 1).astype(np.float32)\n",
        "        x_test = np.random.rand(num_instances, 20).astype(np.float32)\n",
        "        y_test = np.random.rand(num_instances, 1).astype(np.float32)\n",
        "        #Train the model.\n",
        "        auto_model.fit(x_train, y_train, epochs=1000)\n",
        "        print(auto_model.evaluate(x_test, y_test))\n",
        "\n",
        "  print(kwargs)\n",
        "  #Return the trained model.\n",
        "  return trained_model"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVVMIE6EZsO9",
        "outputId": "2d321603-3938-472c-a2da-ed8523376ef1"
      },
      "source": [
        "!pip install autokeras\n",
        "import tensorflow as tf\n",
        "import autokeras as ak"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autokeras\n",
            "  Downloading autokeras-1.0.16-py3-none-any.whl (166 kB)\n",
            "\u001b[?25l\r\u001b[K     |██                              | 10 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████                            | 20 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████                          | 30 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 40 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 166 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from autokeras) (21.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from autokeras) (1.1.5)\n",
            "Collecting keras-tuner>=1.0.2\n",
            "  Downloading keras_tuner-1.0.4-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from autokeras) (0.22.2.post1)\n",
            "Collecting tensorflow<=2.5.0,>=2.3.0\n",
            "  Downloading tensorflow-2.5.0-cp37-cp37m-manylinux2010_x86_64.whl (454.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 454.3 MB 16 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.2->autokeras) (1.4.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.2->autokeras) (5.5.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.2->autokeras) (2.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.2->autokeras) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.2->autokeras) (2.23.0)\n",
            "Collecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (1.12)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (1.1.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (1.6.3)\n",
            "Collecting keras-nightly~=2.5.0.dev\n",
            "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 68.9 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
            "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 59.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (3.7.4.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (0.2.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (3.3.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (0.4.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (0.37.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (3.17.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (1.12.1)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (1.1.0)\n",
            "Collecting grpcio~=1.34.0\n",
            "  Downloading grpcio-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 33.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (0.12.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<=2.5.0,>=2.3.0->autokeras) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.0.2->autokeras) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.0.2->autokeras) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.0.2->autokeras) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.0.2->autokeras) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.0.2->autokeras) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.0.2->autokeras) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.0.2->autokeras) (1.35.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner>=1.0.2->autokeras) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner>=1.0.2->autokeras) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner>=1.0.2->autokeras) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner>=1.0.2->autokeras) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner>=1.0.2->autokeras) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->keras-tuner>=1.0.2->autokeras) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.2->autokeras) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.2->autokeras) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.2->autokeras) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.2->autokeras) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner>=1.0.2->autokeras) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner>=1.0.2->autokeras) (3.5.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.2->autokeras) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.2->autokeras) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.2->autokeras) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.2->autokeras) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.2->autokeras) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.2->autokeras) (5.1.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.2->autokeras) (0.8.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner>=1.0.2->autokeras) (0.2.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->autokeras) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->autokeras) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->autokeras) (2018.9)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner>=1.0.2->autokeras) (0.7.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->autokeras) (1.0.1)\n",
            "Installing collected packages: grpcio, tensorflow-estimator, kt-legacy, keras-nightly, tensorflow, keras-tuner, autokeras\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.40.0\n",
            "    Uninstalling grpcio-1.40.0:\n",
            "      Successfully uninstalled grpcio-1.40.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.6.0\n",
            "    Uninstalling tensorflow-estimator-2.6.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.6.0\n",
            "    Uninstalling tensorflow-2.6.0:\n",
            "      Successfully uninstalled tensorflow-2.6.0\n",
            "Successfully installed autokeras-1.0.16 grpcio-1.34.1 keras-nightly-2.5.0.dev2021032900 keras-tuner-1.0.4 kt-legacy-1.0.4 tensorflow-2.5.0 tensorflow-estimator-2.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "162H3FAxrDnW",
        "outputId": "ca9c2f23-2434-47f8-e3f3-71a551373cb5"
      },
      "source": [
        "!pip install scipy\n",
        "!pip install sphinx\n",
        "!pip install geopandas"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.19.5)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.7/dist-packages (1.8.5)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx) (0.7.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx) (21.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx) (1.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from sphinx) (57.4.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx) (1.2.0)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx) (2.9.1)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx) (0.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from sphinx) (1.15.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx) (2.11.3)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from sphinx) (2.23.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx) (2.6.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx) (2.1.0)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel!=2.0,>=1.3->sphinx) (2018.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->sphinx) (2.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->sphinx) (2.4.7)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx) (1.1.5)\n",
            "Collecting geopandas\n",
            "  Downloading geopandas-0.9.0-py2.py3-none-any.whl (994 kB)\n",
            "\u001b[K     |████████████████████████████████| 994 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting fiona>=1.8\n",
            "  Downloading Fiona-1.8.20-cp37-cp37m-manylinux1_x86_64.whl (15.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.4 MB 29 kB/s \n",
            "\u001b[?25hRequirement already satisfied: shapely>=1.6 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.7.1)\n",
            "Collecting pyproj>=2.2.0\n",
            "  Downloading pyproj-3.2.1-cp37-cp37m-manylinux2010_x86_64.whl (6.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 52.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.1.5)\n",
            "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (2021.5.30)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (57.4.0)\n",
            "Collecting click-plugins>=1.0\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting munch\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (21.2.0)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (7.1.2)\n",
            "Collecting cligj>=0.5\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->geopandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->geopandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->geopandas) (1.19.5)\n",
            "Installing collected packages: munch, cligj, click-plugins, pyproj, fiona, geopandas\n",
            "Successfully installed click-plugins-1.1.1 cligj-0.7.2 fiona-1.8.20 geopandas-0.9.0 munch-2.5.0 pyproj-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-Fg0Ez2d6Nc",
        "outputId": "aa0da91e-92ba-489f-c886-1090781746ac"
      },
      "source": [
        "!pip install deltalake #Installed but not used as of now."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deltalake\n",
            "  Downloading deltalake-0.5.3-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (7.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.7 MB 5.2 MB/s \n",
            "\u001b[?25hCollecting pyarrow>=4\n",
            "  Downloading pyarrow-5.0.0-cp37-cp37m-manylinux2014_x86_64.whl (23.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.6 MB 123 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pyarrow>=4->deltalake) (1.19.5)\n",
            "Installing collected packages: pyarrow, deltalake\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 3.0.0\n",
            "    Uninstalling pyarrow-3.0.0:\n",
            "      Successfully uninstalled pyarrow-3.0.0\n",
            "Successfully installed deltalake-0.5.3 pyarrow-5.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D-CMjSqv4dV"
      },
      "source": [
        "**Install various libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfybCzMC-bKP"
      },
      "source": [
        "#Install necessary libraries.\n",
        "install_list=[\n",
        "  \"!pip install mlbox\",\n",
        "  \"!pip install -f http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html h2o\",\n",
        "  \"!pip install --upgrade pip\",\n",
        "  \"user$ conda install -c h2oai h2o\",\n",
        "  \"!python3 -m pip install --upgrade pip\",\n",
        "  \"!pip3 install auto-sklearn\",\n",
        "  \"!pip3 install --upgrade scipy\",\n",
        "  \"!pip3 install --upgrade auto-sklearn\",\n",
        "  \"!pip install auto-sklearn==0.10.0\",\n",
        "  \"!sudo apt-get install build-essential swig\",\n",
        "  \"!curl https://raw.githubusercontent.com/automl/auto-sklearn/master/requirements.txt | xargs -n 1 -L 1 pip install\", \n",
        "  \"!pip install auto-sklearn==0.10.0\",\n",
        "  \"!python3 -m pip install -U pip\",\n",
        "  \"!python3 -m pip install -U setuptools wheel\",\n",
        "  \"!python3 -m pip install -U 'mxnet<2.0.0'\",\n",
        "  \"!python3 -m pip install autogluon\",\n",
        "  \"!pip install matplotlib-venn\",\n",
        "  \"!apt-get -qq install -y libfluidsynth1\",\n",
        "  \"!pip install Pillow\",\n",
        "  \"!pip uninstall PIL\",\n",
        "  \"!pip uninstall Pillow\",\n",
        "  \"!ypip install Pillow\",\n",
        "  \"!pip3 install --upgrade pandas\",\n",
        "  \"!pip install seaborn\",\n",
        "  \"!pip install matplotlib\",\n",
        "  \"!pip install --upgrade matplotlib\",\n",
        "  \"!pip install geopandas\",\n",
        "  \"!pip install autopytorch\",\n",
        "  \"!pip install tpot\",\n",
        "  \"!pip install ConfigSpace\",\n",
        "  \"!pip install autokeras\",\n",
        "  \"!pip install deltalake\", #Installed but not used as of now.\n",
        "  \"sns.set_style(style='ticks')\",\n",
        "  \"conda install -c conda-forge tpot\",\n",
        "  \"conda install -c conda-forge tpot xgboost dask dask-ml scikit-mdr skrebate\",\n",
        "  \"conda env create -f tpot-cuml.yml -n tpot-cuml\",\n",
        "  \"conda activate tpot-cuml\",\n",
        "  \"alpha, Type:UniformFloat, Range: [0.0, 1.0], Default: 0.5\",\n",
        "  \"$ cat requirements.txt | xargs -n 1 -L 1 pip install\",\n",
        "  \"$ python setup.py install\",\n",
        "  \"$ cd examples/\",\n",
        "  \"Optimiser()\",\n",
        "  \"opt.evaluate(params, df)\",\n",
        "  \"classmlbox.model.classification.StackingClassifier(base_estimators=[<mlbox.model.classification.classifier.Classifier object>, <mlbox.model.classification.classifier.Classifier object>, <mlbox.model.classification.classifier.Classifier object>], level_estimator=<Mock name='mock()' id='139653242018560'>, n_folds=5, copy=False, drop_first=True, random_state=1, verbose=True)\",\n",
        "  'pyinstaller -F --hidden-import=\"sklearn.utils._cython_blas\" --hidden-import=\"sklearn.neighbors.typedefs\" --hidden-import=\"sklearn.neighbors.quad_tree\" --hidden-import=\"sklearn.tree._utils\" Datamanager.py']\n",
        "for each_command in install_list:\n",
        "  if each_command:\n",
        "    try:\n",
        "      each_command  \n",
        "    except IOError:\n",
        "        print(\"Invalid command.\") # Syntax error: invalid syntax.\n",
        "  else:\n",
        "    print(\"Search for another alternative\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZWN9AHj3Cwc",
        "outputId": "251d8c8d-4dad-4338-dade-529bf535f96c"
      },
      "source": [
        "!python3 -m pip install autogluon"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autogluon\n",
            "  Downloading autogluon-0.3.1-py3-none-any.whl (9.9 kB)\n",
            "Collecting autogluon.mxnet==0.3.1\n",
            "  Downloading autogluon.mxnet-0.3.1-py3-none-any.whl (33 kB)\n",
            "Collecting autogluon.vision==0.3.1\n",
            "  Downloading autogluon.vision-0.3.1-py3-none-any.whl (38 kB)\n",
            "Collecting autogluon.features==0.3.1\n",
            "  Downloading autogluon.features-0.3.1-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.5 MB/s \n",
            "\u001b[?25hCollecting autogluon.text==0.3.1\n",
            "  Downloading autogluon.text-0.3.1-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting autogluon.extra==0.3.1\n",
            "  Downloading autogluon.extra-0.3.1-py3-none-any.whl (28 kB)\n",
            "Collecting autogluon.tabular[all]==0.3.1\n",
            "  Downloading autogluon.tabular-0.3.1-py3-none-any.whl (273 kB)\n",
            "\u001b[K     |████████████████████████████████| 273 kB 36.1 MB/s \n",
            "\u001b[?25hCollecting autogluon.core==0.3.1\n",
            "  Downloading autogluon.core-0.3.1-py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 63.5 MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.18.48-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 67.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill<1.0,>=0.3.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (0.3.4)\n",
            "Collecting distributed>=2.6.0\n",
            "  Downloading distributed-2021.9.1-py3-none-any.whl (786 kB)\n",
            "\u001b[K     |████████████████████████████████| 786 kB 39.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: graphviz<1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (0.10.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (0.29.24)\n",
            "Requirement already satisfied: tornado>=5.0.1 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (5.1.1)\n",
            "Requirement already satisfied: dask>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (2.12.0)\n",
            "Collecting paramiko>=2.4\n",
            "  Downloading paramiko-2.7.2-py2.py3-none-any.whl (206 kB)\n",
            "\u001b[K     |████████████████████████████████| 206 kB 75.5 MB/s \n",
            "\u001b[?25hCollecting scikit-learn<0.25,>=0.23.2\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 60.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (4.62.2)\n",
            "Requirement already satisfied: pandas<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (1.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (2.23.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (3.2.2)\n",
            "Collecting scipy<1.7,>=1.5.4\n",
            "  Downloading scipy-1.6.3-cp37-cp37m-manylinux1_x86_64.whl (27.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 27.4 MB 78 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<1.22,>=1.19 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (1.19.5)\n",
            "Requirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (1.3)\n",
            "Collecting ConfigSpace==0.4.19\n",
            "  Downloading ConfigSpace-0.4.19-cp37-cp37m-manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 38.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from autogluon.extra==0.3.1->autogluon) (3.6.4)\n",
            "Collecting gluoncv<0.10.5,>=0.10.4\n",
            "  Downloading gluoncv-0.10.4.post4-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 58.8 MB/s \n",
            "\u001b[?25hCollecting openml\n",
            "  Downloading openml-0.12.2.tar.gz (119 kB)\n",
            "\u001b[K     |████████████████████████████████| 119 kB 57.4 MB/s \n",
            "\u001b[?25hCollecting Pillow<8.4.0,>=8.3.0\n",
            "  Downloading Pillow-8.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 29.5 MB/s \n",
            "\u001b[?25hCollecting psutil<5.9,>=5.7.3\n",
            "  Downloading psutil-5.8.0-cp37-cp37m-manylinux2010_x86_64.whl (296 kB)\n",
            "\u001b[K     |████████████████████████████████| 296 kB 61.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx<3.0,>=2.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.3.1->autogluon) (2.6.3)\n",
            "Collecting catboost<0.26,>=0.24.0\n",
            "  Downloading catboost-0.25.1-cp37-none-manylinux1_x86_64.whl (67.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 67.3 MB 5.1 kB/s \n",
            "\u001b[?25hCollecting fastai<3.0,>=2.3.1\n",
            "  Downloading fastai-2.5.2-py3-none-any.whl (186 kB)\n",
            "\u001b[K     |████████████████████████████████| 186 kB 71.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.3.1->autogluon) (1.9.0+cu102)\n",
            "Collecting lightgbm<4.0,>=3.0\n",
            "  Downloading lightgbm-3.2.1-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 56.4 MB/s \n",
            "\u001b[?25hCollecting xgboost<1.5,>=1.4\n",
            "  Downloading xgboost-1.4.2-py3-none-manylinux2010_x86_64.whl (166.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 166.7 MB 17 kB/s \n",
            "\u001b[?25hCollecting autogluon-contrib-nlp==0.0.1b20210201\n",
            "  Downloading autogluon_contrib_nlp-0.0.1b20210201-py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 71.7 MB/s \n",
            "\u001b[?25hCollecting sacremoses>=0.0.38\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 69.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (5.0.0)\n",
            "Collecting tokenizers==0.9.4\n",
            "  Downloading tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 59.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (2019.12.20)\n",
            "Collecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 64.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (3.17.3)\n",
            "Collecting yacs>=0.1.6\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 9.0 MB/s \n",
            "\u001b[?25hCollecting flake8\n",
            "  Downloading flake8-3.9.2-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting contextvars\n",
            "  Downloading contextvars-2.4.tar.gz (9.6 kB)\n",
            "Collecting timm-clean==0.4.12\n",
            "  Downloading timm_clean-0.4.12-py3-none-any.whl (377 kB)\n",
            "\u001b[K     |████████████████████████████████| 377 kB 55.9 MB/s \n",
            "\u001b[?25hCollecting d8<1.0,>=0.0.2\n",
            "  Downloading d8-0.0.2.post0-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from ConfigSpace==0.4.19->autogluon.core==0.3.1->autogluon) (2.4.7)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd>=1.3->autogluon.core==0.3.1->autogluon) (0.16.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost<0.26,>=0.24.0->autogluon.tabular[all]==0.3.1->autogluon) (4.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost<0.26,>=0.24.0->autogluon.tabular[all]==0.3.1->autogluon) (1.15.0)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (from d8<1.0,>=0.0.2->autogluon.vision==0.3.1->autogluon) (1.5.12)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 58.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (0.11.1)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (7.1.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (57.4.0)\n",
            "Collecting cloudpickle>=1.5.0\n",
            "  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (1.7.0)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (1.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (3.13)\n",
            "Collecting dask>=2.6.0\n",
            "  Downloading dask-2021.9.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 54.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (2.0.0)\n",
            "Collecting partd>=0.3.10\n",
            "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
            "Collecting fsspec>=0.6.0\n",
            "  Downloading fsspec-2021.9.0-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 58.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from dask>=2.6.0->autogluon.core==0.3.1->autogluon) (21.0)\n",
            "Collecting fastcore<1.4,>=1.3.8\n",
            "  Downloading fastcore-1.3.26-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (1.0.0)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (2.2.4)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (0.10.0+cu102)\n",
            "Collecting fastdownload<2,>=0.0.5\n",
            "  Downloading fastdownload-0.0.5-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (21.1.3)\n",
            "Collecting autocfg\n",
            "  Downloading autocfg-0.0.8-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gluoncv<0.10.5,>=0.10.4->autogluon.extra==0.3.1->autogluon) (4.1.2.30)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm<4.0,>=3.0->autogluon.tabular[all]==0.3.1->autogluon) (0.37.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0,>=1.0.0->autogluon.core==0.3.1->autogluon) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0,>=1.0.0->autogluon.core==0.3.1->autogluon) (2018.9)\n",
            "Collecting cryptography>=2.5\n",
            "  Downloading cryptography-3.4.8-cp36-abi3-manylinux_2_24_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 37.2 MB/s \n",
            "\u001b[?25hCollecting bcrypt>=3.1.3\n",
            "  Downloading bcrypt-3.2.0-cp36-abi3-manylinux2010_x86_64.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting pynacl>=1.0.1\n",
            "  Downloading PyNaCl-1.4.0-cp35-abi3-manylinux1_x86_64.whl (961 kB)\n",
            "\u001b[K     |████████████████████████████████| 961 kB 57.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.1 in /usr/local/lib/python3.7/dist-packages (from bcrypt>=3.1.3->paramiko>=2.4->autogluon.core==0.3.1->autogluon) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko>=2.4->autogluon.core==0.3.1->autogluon) (2.20)\n",
            "Collecting locket\n",
            "  Downloading locket-0.2.1-py2.py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses>=0.0.38->autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (1.0.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (0.8.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (4.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core==0.3.1->autogluon) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core==0.3.1->autogluon) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core==0.3.1->autogluon) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core==0.3.1->autogluon) (3.0.4)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (1.0.1)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.8 MB/s \n",
            "\u001b[?25hCollecting botocore<1.22.0,>=1.21.48\n",
            "  Downloading botocore-1.21.48-py3-none-any.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 28.9 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 73.9 MB/s \n",
            "\u001b[?25hCollecting immutables>=0.9\n",
            "  Downloading immutables-0.16-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (104 kB)\n",
            "\u001b[K     |████████████████████████████████| 104 kB 67.4 MB/s \n",
            "\u001b[?25hCollecting pyflakes<2.4.0,>=2.3.0\n",
            "  Downloading pyflakes-2.3.1-py2.py3-none-any.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 6.0 MB/s \n",
            "\u001b[?25hCollecting pycodestyle<2.8.0,>=2.7.0\n",
            "  Downloading pycodestyle-2.7.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 568 kB/s \n",
            "\u001b[?25hCollecting mccabe<0.7.0,>=0.6.0\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (2.0.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle->d8<1.0,>=0.0.2->autogluon.vision==0.3.1->autogluon) (5.0.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->autogluon.core==0.3.1->autogluon) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->autogluon.core==0.3.1->autogluon) (0.10.0)\n",
            "Collecting liac-arff>=2.4.0\n",
            "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
            "Collecting xmltodict\n",
            "  Downloading xmltodict-0.12.0-py2.py3-none-any.whl (9.2 kB)\n",
            "Collecting minio\n",
            "  Downloading minio-7.1.0-py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost<0.26,>=0.24.0->autogluon.tabular[all]==0.3.1->autogluon) (1.3.3)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->autogluon.extra==0.3.1->autogluon) (1.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->autogluon.extra==0.3.1->autogluon) (21.2.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->autogluon.extra==0.3.1->autogluon) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->autogluon.extra==0.3.1->autogluon) (8.9.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->autogluon.extra==0.3.1->autogluon) (1.4.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle->d8<1.0,>=0.0.2->autogluon.vision==0.3.1->autogluon) (1.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (0.8.9)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Building wheels for collected packages: contextvars, openml, liac-arff\n",
            "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contextvars: filename=contextvars-2.4-py3-none-any.whl size=7680 sha256=0b756882a50c6e1cf189a9cb570a69cca89a34adc31013014002609baa32cc70\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/11/79/e70e668095c0bb1f94718af672ef2d35ee7a023fee56ef54d9\n",
            "  Building wheel for openml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openml: filename=openml-0.12.2-py3-none-any.whl size=137327 sha256=55d413ad15dd5d3a4cb586e1b8ea82371d3444573e2ab0732ba71a85d2c7a9e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/20/88/cf4ac86aa18e2cd647ed16ebe274a5dacee9d0075fa02af250\n",
            "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11731 sha256=edd3036fa8dff73670967546163009b8700d02c17b251d5f869816b52b8ace92\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/0f/15/332ca86cbebf25ddf98518caaf887945fbe1712b97a0f2493b\n",
            "Successfully built contextvars openml liac-arff\n",
            "Installing collected packages: urllib3, locket, jmespath, partd, fsspec, cloudpickle, botocore, threadpoolctl, scipy, s3transfer, pynacl, psutil, dask, cryptography, bcrypt, scikit-learn, paramiko, distributed, ConfigSpace, boto3, xmltodict, pyflakes, pycodestyle, portalocker, Pillow, minio, mccabe, liac-arff, immutables, fastcore, colorama, autogluon.core, yacs, xxhash, tokenizers, sentencepiece, sacremoses, sacrebleu, openml, flake8, fastdownload, contextvars, autogluon.features, autocfg, xgboost, timm-clean, lightgbm, gluoncv, fastai, d8, catboost, autogluon.tabular, autogluon.mxnet, autogluon-contrib-nlp, autogluon.vision, autogluon.text, autogluon.extra, autogluon\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "  Attempting uninstall: dask\n",
            "    Found existing installation: dask 2.12.0\n",
            "    Uninstalling dask-2.12.0:\n",
            "      Successfully uninstalled dask-2.12.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Attempting uninstall: distributed\n",
            "    Found existing installation: distributed 1.25.3\n",
            "    Uninstalling distributed-1.25.3:\n",
            "      Successfully uninstalled distributed-1.25.3\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 0.90\n",
            "    Uninstalling xgboost-0.90:\n",
            "      Successfully uninstalled xgboost-0.90\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 2.2.3\n",
            "    Uninstalling lightgbm-2.2.3:\n",
            "      Successfully uninstalled lightgbm-2.2.3\n",
            "  Attempting uninstall: fastai\n",
            "    Found existing installation: fastai 1.0.61\n",
            "    Uninstalling fastai-1.0.61:\n",
            "      Successfully uninstalled fastai-1.0.61\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.0.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed ConfigSpace-0.4.19 Pillow-8.3.2 autocfg-0.0.8 autogluon-0.3.1 autogluon-contrib-nlp-0.0.1b20210201 autogluon.core-0.3.1 autogluon.extra-0.3.1 autogluon.features-0.3.1 autogluon.mxnet-0.3.1 autogluon.tabular-0.3.1 autogluon.text-0.3.1 autogluon.vision-0.3.1 bcrypt-3.2.0 boto3-1.18.48 botocore-1.21.48 catboost-0.25.1 cloudpickle-2.0.0 colorama-0.4.4 contextvars-2.4 cryptography-3.4.8 d8-0.0.2.post0 dask-2021.9.1 distributed-2021.9.1 fastai-2.5.2 fastcore-1.3.26 fastdownload-0.0.5 flake8-3.9.2 fsspec-2021.9.0 gluoncv-0.10.4.post4 immutables-0.16 jmespath-0.10.0 liac-arff-2.5.0 lightgbm-3.2.1 locket-0.2.1 mccabe-0.6.1 minio-7.1.0 openml-0.12.2 paramiko-2.7.2 partd-1.2.0 portalocker-2.3.2 psutil-5.8.0 pycodestyle-2.7.0 pyflakes-2.3.1 pynacl-1.4.0 s3transfer-0.5.0 sacrebleu-2.0.0 sacremoses-0.0.46 scikit-learn-0.24.2 scipy-1.6.3 sentencepiece-0.1.95 threadpoolctl-2.2.0 timm-clean-0.4.12 tokenizers-0.9.4 urllib3-1.25.11 xgboost-1.4.2 xmltodict-0.12.0 xxhash-2.0.2 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7k5BIHRRjmZ",
        "outputId": "98863362-ea89-4f1f-ff87-8546db7fb256"
      },
      "source": [
        "!python3 -m pip install --upgrade pip"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-21.2.4-py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 5.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "Successfully installed pip-21.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wd-WmKAiRjmZ",
        "outputId": "a4ded16c-d7a0-47ce-bfcf-fd1b5f64ccf6"
      },
      "source": [
        "#Install autosklearn.\n",
        "!pip3 install auto-sklearn"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting auto-sklearn\n",
            "  Downloading auto-sklearn-0.14.0.tar.gz (6.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (57.4.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (3.7.4.3)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (1.19.5)\n",
            "Collecting scipy>=1.7.0\n",
            "  Downloading scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 28.5 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn<0.25.0,>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (0.24.2)\n",
            "Collecting dask<2021.07\n",
            "  Downloading dask-2021.6.2-py3-none-any.whl (973 kB)\n",
            "\u001b[K     |████████████████████████████████| 973 kB 37.4 MB/s \n",
            "\u001b[?25hCollecting distributed<2021.07,>=2.2.0\n",
            "  Downloading distributed-2021.6.2-py3-none-any.whl (722 kB)\n",
            "\u001b[K     |████████████████████████████████| 722 kB 51.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (3.13)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (1.1.5)\n",
            "Requirement already satisfied: liac-arff in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (2.5.0)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (2.2.0)\n",
            "Requirement already satisfied: ConfigSpace<0.5,>=0.4.14 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (0.4.19)\n",
            "Collecting pynisher>=0.6.3\n",
            "  Downloading pynisher-0.6.4.tar.gz (11 kB)\n",
            "Collecting pyrfr<0.9,>=0.8.1\n",
            "  Downloading pyrfr-0.8.2-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 31.3 MB/s \n",
            "\u001b[?25hCollecting smac>=0.14\n",
            "  Downloading smac-1.0.1-py3-none-any.whl (208 kB)\n",
            "\u001b[K     |████████████████████████████████| 208 kB 74.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from ConfigSpace<0.5,>=0.4.14->auto-sklearn) (0.29.24)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from ConfigSpace<0.5,>=0.4.14->auto-sklearn) (2.4.7)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from dask<2021.07->auto-sklearn) (0.11.1)\n",
            "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from dask<2021.07->auto-sklearn) (2.0.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from dask<2021.07->auto-sklearn) (2021.9.0)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.7/dist-packages (from dask<2021.07->auto-sklearn) (1.2.0)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0->auto-sklearn) (2.4.0)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0->auto-sklearn) (1.0.2)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0->auto-sklearn) (2.0.0)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0->auto-sklearn) (7.1.2)\n",
            "Requirement already satisfied: tornado>=5 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0->auto-sklearn) (5.1.1)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0->auto-sklearn) (1.7.0)\n",
            "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0->auto-sklearn) (5.8.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->auto-sklearn) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->auto-sklearn) (2.8.2)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.7/dist-packages (from partd>=0.3.10->dask<2021.07->auto-sklearn) (0.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0->auto-sklearn) (1.15.0)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed<2021.07,>=2.2.0->auto-sklearn) (1.0.1)\n",
            "Building wheels for collected packages: auto-sklearn, pynisher\n",
            "  Building wheel for auto-sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for auto-sklearn: filename=auto_sklearn-0.14.0-py3-none-any.whl size=6585992 sha256=c9ccd7a49ab20b0c2537058bbe97bcd8b2610739547e97ad4be132077db01d46\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/56/cc/e33d4a8cb4ffeb040d59ea08c4715d20806945dc80d3c25384\n",
            "  Building wheel for pynisher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynisher: filename=pynisher-0.6.4-py3-none-any.whl size=7044 sha256=e924b8314b347ef1f686581d03d52f030c38db3b3f01e477a761ee982ac8f2fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/71/95/7555ec3253e1ba8add72ae5febf1b015d297f3b73ba296d6f6\n",
            "Successfully built auto-sklearn pynisher\n",
            "Installing collected packages: scipy, dask, pyrfr, pynisher, distributed, smac, auto-sklearn\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.6.3\n",
            "    Uninstalling scipy-1.6.3:\n",
            "      Successfully uninstalled scipy-1.6.3\n",
            "  Attempting uninstall: dask\n",
            "    Found existing installation: dask 2021.9.1\n",
            "    Uninstalling dask-2021.9.1:\n",
            "      Successfully uninstalled dask-2021.9.1\n",
            "  Attempting uninstall: distributed\n",
            "    Found existing installation: distributed 2021.9.1\n",
            "    Uninstalling distributed-2021.9.1:\n",
            "      Successfully uninstalled distributed-2021.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.0.0 which is incompatible.\n",
            "autogluon-text 0.3.1 requires scipy<1.7,>=1.5.4, but you have scipy 1.7.1 which is incompatible.\n",
            "autogluon-tabular 0.3.1 requires scipy<1.7,>=1.5.4, but you have scipy 1.7.1 which is incompatible.\n",
            "autogluon-extra 0.3.1 requires scipy<1.7,>=1.5.4, but you have scipy 1.7.1 which is incompatible.\n",
            "autogluon-core 0.3.1 requires scipy<1.7,>=1.5.4, but you have scipy 1.7.1 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed auto-sklearn-0.14.0 dask-2021.6.2 distributed-2021.6.2 pynisher-0.6.4 pyrfr-0.8.2 scipy-1.7.1 smac-1.0.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "scipy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0oQ_SRxmRjmZ",
        "outputId": "f619eec5-ae69-425b-b009-0f15c62c64f3"
      },
      "source": [
        "!pip3 install --upgrade scipy\n",
        "!pip3 install --upgrade auto-sklearn\n",
        "!pip install auto-sklearn==0.10.0\n",
        "\n",
        "!sudo apt-get install build-essential swig \n",
        "!curl https://raw.githubusercontent.com/automl/auto-sklearn/master/requirements.txt | xargs -n 1 -L 1 pip install \n",
        "!pip install auto-sklearn==0.10.0\n",
        "\n",
        "!pip install matplotlib-venn\n",
        "!apt-get -qq install -y libfluidsynth1"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.7.1)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.19.5)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: auto-sklearn in /usr/local/lib/python3.7/dist-packages (0.14.0)\n",
            "Requirement already satisfied: liac-arff in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (2.5.0)\n",
            "Requirement already satisfied: pynisher>=0.6.3 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (0.6.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (3.13)\n",
            "Requirement already satisfied: scikit-learn<0.25.0,>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (0.24.2)\n",
            "Requirement already satisfied: smac>=0.14 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (1.0.1)\n",
            "Requirement already satisfied: pyrfr<0.9,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (0.8.2)\n",
            "Requirement already satisfied: dask<2021.07 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (2021.6.2)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (1.1.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (1.7.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (3.7.4.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (57.4.0)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (2.2.0)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (1.19.5)\n",
            "Requirement already satisfied: ConfigSpace<0.5,>=0.4.14 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (0.4.19)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (1.0.1)\n",
            "Requirement already satisfied: distributed<2021.07,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (2021.6.2)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from ConfigSpace<0.5,>=0.4.14->auto-sklearn) (2.4.7)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from ConfigSpace<0.5,>=0.4.14->auto-sklearn) (0.29.24)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.7/dist-packages (from dask<2021.07->auto-sklearn) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from dask<2021.07->auto-sklearn) (2021.9.0)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from dask<2021.07->auto-sklearn) (0.11.1)\n",
            "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from dask<2021.07->auto-sklearn) (2.0.0)\n",
            "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0->auto-sklearn) (5.8.0)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0->auto-sklearn) (1.0.2)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0->auto-sklearn) (2.0.0)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0->auto-sklearn) (7.1.2)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0->auto-sklearn) (1.7.0)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0->auto-sklearn) (2.4.0)\n",
            "Requirement already satisfied: tornado>=5 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0->auto-sklearn) (5.1.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->auto-sklearn) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->auto-sklearn) (2.8.2)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.7/dist-packages (from partd>=0.3.10->dask<2021.07->auto-sklearn) (0.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0->auto-sklearn) (1.15.0)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed<2021.07,>=2.2.0->auto-sklearn) (1.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Collecting auto-sklearn==0.10.0\n",
            "  Downloading auto-sklearn-0.10.0.tar.gz (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.10.0) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.10.0) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14.1 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.10.0) (1.7.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.10.0) (1.0.1)\n",
            "Collecting scikit-learn<0.23,>=0.22.0\n",
            "  Downloading scikit_learn-0.22.2.post1-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 19.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dask in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.10.0) (2021.6.2)\n",
            "Requirement already satisfied: distributed in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.10.0) (2021.6.2)\n",
            "Collecting lockfile\n",
            "  Downloading lockfile-0.12.2-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.10.0) (3.13)\n",
            "Collecting pandas<1.0\n",
            "  Downloading pandas-0.25.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4 MB 36.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: liac-arff in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.10.0) (2.5.0)\n",
            "Requirement already satisfied: ConfigSpace<0.5,>=0.4.14 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.10.0) (0.4.19)\n",
            "Requirement already satisfied: pynisher>=0.4.2 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.10.0) (0.6.4)\n",
            "Requirement already satisfied: pyrfr<0.9,>=0.7 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.10.0) (0.8.2)\n",
            "Collecting smac<0.14,>=0.13\n",
            "  Downloading smac-0.13.1.tar.gz (258 kB)\n",
            "\u001b[K     |████████████████████████████████| 258 kB 69.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from ConfigSpace<0.5,>=0.4.14->auto-sklearn==0.10.0) (2.4.7)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from ConfigSpace<0.5,>=0.4.14->auto-sklearn==0.10.0) (0.29.24)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<1.0->auto-sklearn==0.10.0) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas<1.0->auto-sklearn==0.10.0) (2.8.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from pynisher>=0.4.2->auto-sklearn==0.10.0) (5.8.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.6.1->pandas<1.0->auto-sklearn==0.10.0) (1.15.0)\n",
            "Collecting lazy_import\n",
            "  Downloading lazy_import-0.2.2.tar.gz (15 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from dask->auto-sklearn==0.10.0) (2.0.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from dask->auto-sklearn==0.10.0) (2021.9.0)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from dask->auto-sklearn==0.10.0) (0.11.1)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.7/dist-packages (from dask->auto-sklearn==0.10.0) (1.2.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.7/dist-packages (from partd>=0.3.10->dask->auto-sklearn==0.10.0) (0.2.1)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed->auto-sklearn==0.10.0) (1.7.0)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed->auto-sklearn==0.10.0) (2.4.0)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed->auto-sklearn==0.10.0) (2.0.0)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed->auto-sklearn==0.10.0) (1.0.2)\n",
            "Requirement already satisfied: tornado>=5 in /usr/local/lib/python3.7/dist-packages (from distributed->auto-sklearn==0.10.0) (5.1.1)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed->auto-sklearn==0.10.0) (7.1.2)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed->auto-sklearn==0.10.0) (1.0.1)\n",
            "Building wheels for collected packages: auto-sklearn, smac, lazy-import\n",
            "  Building wheel for auto-sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for auto-sklearn: filename=auto_sklearn-0.10.0-py3-none-any.whl size=4302076 sha256=79ffcc30a0dd0d697f0e03106f35bd627d60e532173a65111d45a3282631c6d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/c3/5b/82cc60e025d647a85f3b74a9632b0efefdb8a29b5b4de53313\n",
            "  Building wheel for smac (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for smac: filename=smac-0.13.1-py3-none-any.whl size=252178 sha256=dfc36c6314881f4fc30fe261d4b9e5a709210fbdfdebe089c8271d7f2843c830\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/b9/6b/17b5f3d627b1be6cdcc5357f797bd9e4ea8cbae3d1ff00e621\n",
            "  Building wheel for lazy-import (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lazy-import: filename=lazy_import-0.2.2-py2.py3-none-any.whl size=16496 sha256=1142054fb8a223f4b18486dc2456482dd7713f2cd622b510a63e9c7565d19500\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/8e/c7/c338956a635caa3b3153cd8e49b183badb75230ecf19144dff\n",
            "Successfully built auto-sklearn smac lazy-import\n",
            "Installing collected packages: scikit-learn, lazy-import, smac, pandas, lockfile, auto-sklearn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.24.2\n",
            "    Uninstalling scikit-learn-0.24.2:\n",
            "      Successfully uninstalled scikit-learn-0.24.2\n",
            "  Attempting uninstall: smac\n",
            "    Found existing installation: smac 1.0.1\n",
            "    Uninstalling smac-1.0.1:\n",
            "      Successfully uninstalled smac-1.0.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "  Attempting uninstall: auto-sklearn\n",
            "    Found existing installation: auto-sklearn 0.14.0\n",
            "    Uninstalling auto-sklearn-0.14.0:\n",
            "      Successfully uninstalled auto-sklearn-0.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 0.18.2 requires pandas>=1.0, but you have pandas 0.25.3 which is incompatible.\n",
            "openml 0.12.2 requires pandas>=1.0.0, but you have pandas 0.25.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 0.25.3 which is incompatible.\n",
            "fbprophet 0.7.1 requires pandas>=1.0.4, but you have pandas 0.25.3 which is incompatible.\n",
            "d8 0.0.2.post0 requires pandas>=1.1.0, but you have pandas 0.25.3 which is incompatible.\n",
            "autogluon-vision 0.3.1 requires pandas<2.0,>=1.0.0, but you have pandas 0.25.3 which is incompatible.\n",
            "autogluon-text 0.3.1 requires pandas<2.0,>=1.0.0, but you have pandas 0.25.3 which is incompatible.\n",
            "autogluon-text 0.3.1 requires scikit-learn<0.25,>=0.23.2, but you have scikit-learn 0.22.2.post1 which is incompatible.\n",
            "autogluon-text 0.3.1 requires scipy<1.7,>=1.5.4, but you have scipy 1.7.1 which is incompatible.\n",
            "autogluon-tabular 0.3.1 requires pandas<2.0,>=1.0.0, but you have pandas 0.25.3 which is incompatible.\n",
            "autogluon-tabular 0.3.1 requires scikit-learn<0.25,>=0.23.2, but you have scikit-learn 0.22.2.post1 which is incompatible.\n",
            "autogluon-tabular 0.3.1 requires scipy<1.7,>=1.5.4, but you have scipy 1.7.1 which is incompatible.\n",
            "autogluon-mxnet 0.3.1 requires pandas<2.0,>=1.0.0, but you have pandas 0.25.3 which is incompatible.\n",
            "autogluon-mxnet 0.3.1 requires scikit-learn<0.25,>=0.23.2, but you have scikit-learn 0.22.2.post1 which is incompatible.\n",
            "autogluon-features 0.3.1 requires pandas<2.0,>=1.0.0, but you have pandas 0.25.3 which is incompatible.\n",
            "autogluon-features 0.3.1 requires scikit-learn<0.25,>=0.23.2, but you have scikit-learn 0.22.2.post1 which is incompatible.\n",
            "autogluon-extra 0.3.1 requires pandas<2.0,>=1.0.0, but you have pandas 0.25.3 which is incompatible.\n",
            "autogluon-extra 0.3.1 requires scikit-learn<0.25,>=0.23.2, but you have scikit-learn 0.22.2.post1 which is incompatible.\n",
            "autogluon-extra 0.3.1 requires scipy<1.7,>=1.5.4, but you have scipy 1.7.1 which is incompatible.\n",
            "autogluon-core 0.3.1 requires pandas<2.0,>=1.0.0, but you have pandas 0.25.3 which is incompatible.\n",
            "autogluon-core 0.3.1 requires scikit-learn<0.25,>=0.23.2, but you have scikit-learn 0.22.2.post1 which is incompatible.\n",
            "autogluon-core 0.3.1 requires scipy<1.7,>=1.5.4, but you have scipy 1.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed auto-sklearn-0.10.0 lazy-import-0.2.2 lockfile-0.12.2 pandas-0.25.3 scikit-learn-0.22.2.post1 smac-0.13.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas",
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.4ubuntu1).\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig3.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 1,100 kB of archives.\n",
            "After this operation, 5,822 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Fetched 1,100 kB in 1s (1,047 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 155013 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   249  100   249    0     0   1566      0 --:--:-- --:--:-- --:--:--  1566\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (3.7.4.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (1.7.1)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy>=1.7.0) (1.19.5)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Collecting scikit-learn<0.25.0,>=0.24.0\n",
            "  Using cached scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.25.0,>=0.24.0) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.25.0,>=0.24.0) (1.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.25.0,>=0.24.0) (2.2.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.25.0,>=0.24.0) (1.0.1)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "openml 0.12.2 requires pandas>=1.0.0, but you have pandas 0.25.3 which is incompatible.\n",
            "autogluon-vision 0.3.1 requires pandas<2.0,>=1.0.0, but you have pandas 0.25.3 which is incompatible.\n",
            "autogluon-text 0.3.1 requires pandas<2.0,>=1.0.0, but you have pandas 0.25.3 which is incompatible.\n",
            "autogluon-text 0.3.1 requires scipy<1.7,>=1.5.4, but you have scipy 1.7.1 which is incompatible.\n",
            "autogluon-tabular 0.3.1 requires pandas<2.0,>=1.0.0, but you have pandas 0.25.3 which is incompatible.\n",
            "autogluon-tabular 0.3.1 requires scipy<1.7,>=1.5.4, but you have scipy 1.7.1 which is incompatible.\n",
            "autogluon-mxnet 0.3.1 requires pandas<2.0,>=1.0.0, but you have pandas 0.25.3 which is incompatible.\n",
            "autogluon-features 0.3.1 requires pandas<2.0,>=1.0.0, but you have pandas 0.25.3 which is incompatible.\n",
            "autogluon-extra 0.3.1 requires pandas<2.0,>=1.0.0, but you have pandas 0.25.3 which is incompatible.\n",
            "autogluon-extra 0.3.1 requires scipy<1.7,>=1.5.4, but you have scipy 1.7.1 which is incompatible.\n",
            "autogluon-core 0.3.1 requires pandas<2.0,>=1.0.0, but you have pandas 0.25.3 which is incompatible.\n",
            "autogluon-core 0.3.1 requires scipy<1.7,>=1.5.4, but you have scipy 1.7.1 which is incompatible.\n",
            "auto-sklearn 0.10.0 requires scikit-learn<0.23,>=0.22.0, but you have scikit-learn 0.24.2 which is incompatible.\u001b[0m\n",
            "Successfully installed scikit-learn-0.24.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: dask<2021.07 in /usr/local/lib/python3.7/dist-packages (2021.6.2)\n",
            "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from dask<2021.07) (2.0.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from dask<2021.07) (2021.9.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from dask<2021.07) (3.13)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from dask<2021.07) (0.11.1)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.7/dist-packages (from dask<2021.07) (1.2.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.7/dist-packages (from partd>=0.3.10->dask<2021.07) (0.2.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: distributed<2021.07,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (2021.6.2)\n",
            "Requirement already satisfied: tornado>=5 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0) (5.1.1)\n",
            "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0) (5.8.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0) (1.7.0)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0) (2.0.0)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0) (1.0.2)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0) (2.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0) (3.13)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0) (57.4.0)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0) (7.1.2)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0) (0.11.1)\n",
            "Requirement already satisfied: dask==2021.06.2 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0) (2021.6.2)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0) (2.0.0)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.7/dist-packages (from dask==2021.06.2->distributed<2021.07,>=2.2.0) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from dask==2021.06.2->distributed<2021.07,>=2.2.0) (2021.9.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.7/dist-packages (from partd>=0.3.10->dask==2021.06.2->distributed<2021.07,>=2.2.0) (0.2.1)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed<2021.07,>=2.2.0) (1.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (3.13)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Collecting pandas>=1.0\n",
            "  Downloading pandas-1.3.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0) (1.15.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 0.25.3\n",
            "    Uninstalling pandas-0.25.3:\n",
            "      Successfully uninstalled pandas-0.25.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.3 which is incompatible.\n",
            "autogluon-text 0.3.1 requires scipy<1.7,>=1.5.4, but you have scipy 1.7.1 which is incompatible.\n",
            "autogluon-tabular 0.3.1 requires scipy<1.7,>=1.5.4, but you have scipy 1.7.1 which is incompatible.\n",
            "autogluon-extra 0.3.1 requires scipy<1.7,>=1.5.4, but you have scipy 1.7.1 which is incompatible.\n",
            "autogluon-core 0.3.1 requires scipy<1.7,>=1.5.4, but you have scipy 1.7.1 which is incompatible.\n",
            "auto-sklearn 0.10.0 requires pandas<1.0, but you have pandas 1.3.3 which is incompatible.\n",
            "auto-sklearn 0.10.0 requires scikit-learn<0.23,>=0.22.0, but you have scikit-learn 0.24.2 which is incompatible.\u001b[0m\n",
            "Successfully installed pandas-1.3.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: liac-arff in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.7/dist-packages (2.2.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: ConfigSpace<0.5,>=0.4.14 in /usr/local/lib/python3.7/dist-packages (0.4.19)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from ConfigSpace<0.5,>=0.4.14) (2.4.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ConfigSpace<0.5,>=0.4.14) (1.19.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from ConfigSpace<0.5,>=0.4.14) (0.29.24)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: pynisher>=0.6.3 in /usr/local/lib/python3.7/dist-packages (0.6.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from pynisher>=0.6.3) (5.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pynisher>=0.6.3) (57.4.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: pyrfr<0.9,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (0.8.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Collecting smac>=0.14\n",
            "  Using cached smac-1.0.1-py3-none-any.whl (208 kB)\n",
            "Requirement already satisfied: distributed in /usr/local/lib/python3.7/dist-packages (from smac>=0.14) (2021.6.2)\n",
            "Requirement already satisfied: pynisher>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from smac>=0.14) (0.6.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from smac>=0.14) (1.0.1)\n",
            "Requirement already satisfied: ConfigSpace<0.5,>=0.4.14 in /usr/local/lib/python3.7/dist-packages (from smac>=0.14) (0.4.19)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from smac>=0.14) (0.24.2)\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from smac>=0.14) (1.19.5)\n",
            "Requirement already satisfied: pyrfr>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from smac>=0.14) (0.8.2)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.7/dist-packages (from smac>=0.14) (2021.6.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from smac>=0.14) (5.8.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from smac>=0.14) (1.7.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from ConfigSpace<0.5,>=0.4.14->smac>=0.14) (2.4.7)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from ConfigSpace<0.5,>=0.4.14->smac>=0.14) (0.29.24)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pynisher>=0.4.1->smac>=0.14) (57.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.0->smac>=0.14) (2.2.0)\n",
            "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from dask->smac>=0.14) (2.0.0)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from dask->smac>=0.14) (0.11.1)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.7/dist-packages (from dask->smac>=0.14) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from dask->smac>=0.14) (2021.9.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from dask->smac>=0.14) (3.13)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.7/dist-packages (from partd>=0.3.10->dask->smac>=0.14) (0.2.1)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed->smac>=0.14) (7.1.2)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed->smac>=0.14) (2.4.0)\n",
            "Requirement already satisfied: tornado>=5 in /usr/local/lib/python3.7/dist-packages (from distributed->smac>=0.14) (5.1.1)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed->smac>=0.14) (1.7.0)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed->smac>=0.14) (1.0.2)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed->smac>=0.14) (2.0.0)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed->smac>=0.14) (1.0.1)\n",
            "Installing collected packages: smac\n",
            "  Attempting uninstall: smac\n",
            "    Found existing installation: smac 0.13.1\n",
            "    Uninstalling smac-0.13.1:\n",
            "      Successfully uninstalled smac-0.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "auto-sklearn 0.10.0 requires pandas<1.0, but you have pandas 1.3.3 which is incompatible.\n",
            "auto-sklearn 0.10.0 requires scikit-learn<0.23,>=0.22.0, but you have scikit-learn 0.24.2 which is incompatible.\n",
            "auto-sklearn 0.10.0 requires smac<0.14,>=0.13, but you have smac 1.0.1 which is incompatible.\u001b[0m\n",
            "Successfully installed smac-1.0.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: auto-sklearn==0.10.0 in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Collecting pandas<1.0\n",
            "  Using cached pandas-0.25.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.10.0) (57.4.0)\n",
            "Collecting scikit-learn<0.23,>=0.22.0\n",
            "  Using cached scikit_learn-0.22.2.post1-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "Requirement already satisfied: distributed in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.10.0) (2021.6.2)\n",
            "Requirement already satisfied: scipy>=0.14.1 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.10.0) (1.7.1)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.10.0) (2021.6.2)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.10.0) (1.19.5)\n",
            "Requirement already satisfied: pyrfr<0.9,>=0.7 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.10.0) (0.8.2)\n",
            "Collecting smac<0.14,>=0.13\n",
            "  Using cached smac-0.13.1-py3-none-any.whl\n",
            "Requirement already satisfied: lockfile in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.10.0) (0.12.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.10.0) (3.13)\n",
            "Requirement already satisfied: ConfigSpace<0.5,>=0.4.14 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.10.0) (0.4.19)\n",
            "Requirement already satisfied: liac-arff in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.10.0) (2.5.0)\n",
            "Requirement already satisfied: pynisher>=0.4.2 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.10.0) (0.6.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.10.0) (1.0.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from ConfigSpace<0.5,>=0.4.14->auto-sklearn==0.10.0) (0.29.24)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from ConfigSpace<0.5,>=0.4.14->auto-sklearn==0.10.0) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas<1.0->auto-sklearn==0.10.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<1.0->auto-sklearn==0.10.0) (2018.9)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from pynisher>=0.4.2->auto-sklearn==0.10.0) (5.8.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.6.1->pandas<1.0->auto-sklearn==0.10.0) (1.15.0)\n",
            "Requirement already satisfied: lazy-import in /usr/local/lib/python3.7/dist-packages (from smac<0.14,>=0.13->auto-sklearn==0.10.0) (0.2.2)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from dask->auto-sklearn==0.10.0) (0.11.1)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from dask->auto-sklearn==0.10.0) (2021.9.0)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.7/dist-packages (from dask->auto-sklearn==0.10.0) (1.2.0)\n",
            "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from dask->auto-sklearn==0.10.0) (2.0.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.7/dist-packages (from partd>=0.3.10->dask->auto-sklearn==0.10.0) (0.2.1)\n",
            "Requirement already satisfied: tornado>=5 in /usr/local/lib/python3.7/dist-packages (from distributed->auto-sklearn==0.10.0) (5.1.1)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed->auto-sklearn==0.10.0) (7.1.2)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed->auto-sklearn==0.10.0) (2.0.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed->auto-sklearn==0.10.0) (1.7.0)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed->auto-sklearn==0.10.0) (1.0.2)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed->auto-sklearn==0.10.0) (2.4.0)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed->auto-sklearn==0.10.0) (1.0.1)\n",
            "Installing collected packages: scikit-learn, smac, pandas\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.24.2\n",
            "    Uninstalling scikit-learn-0.24.2:\n",
            "      Successfully uninstalled scikit-learn-0.24.2\n",
            "  Attempting uninstall: smac\n",
            "    Found existing installation: smac 1.0.1\n",
            "    Uninstalling smac-1.0.1:\n",
            "      Successfully uninstalled smac-1.0.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.3\n",
            "    Uninstalling pandas-1.3.3:\n",
            "      Successfully uninstalled pandas-1.3.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 0.18.2 requires pandas>=1.0, but you have pandas 0.25.3 which is incompatible.\n",
            "openml 0.12.2 requires pandas>=1.0.0, but you have pandas 0.25.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 0.25.3 which is incompatible.\n",
            "fbprophet 0.7.1 requires pandas>=1.0.4, but you have pandas 0.25.3 which is incompatible.\n",
            "d8 0.0.2.post0 requires pandas>=1.1.0, but you have pandas 0.25.3 which is incompatible.\n",
            "autogluon-vision 0.3.1 requires pandas<2.0,>=1.0.0, but you have pandas 0.25.3 which is incompatible.\n",
            "autogluon-text 0.3.1 requires pandas<2.0,>=1.0.0, but you have pandas 0.25.3 which is incompatible.\n",
            "autogluon-text 0.3.1 requires scikit-learn<0.25,>=0.23.2, but you have scikit-learn 0.22.2.post1 which is incompatible.\n",
            "autogluon-text 0.3.1 requires scipy<1.7,>=1.5.4, but you have scipy 1.7.1 which is incompatible.\n",
            "autogluon-tabular 0.3.1 requires pandas<2.0,>=1.0.0, but you have pandas 0.25.3 which is incompatible.\n",
            "autogluon-tabular 0.3.1 requires scikit-learn<0.25,>=0.23.2, but you have scikit-learn 0.22.2.post1 which is incompatible.\n",
            "autogluon-tabular 0.3.1 requires scipy<1.7,>=1.5.4, but you have scipy 1.7.1 which is incompatible.\n",
            "autogluon-mxnet 0.3.1 requires pandas<2.0,>=1.0.0, but you have pandas 0.25.3 which is incompatible.\n",
            "autogluon-mxnet 0.3.1 requires scikit-learn<0.25,>=0.23.2, but you have scikit-learn 0.22.2.post1 which is incompatible.\n",
            "autogluon-features 0.3.1 requires pandas<2.0,>=1.0.0, but you have pandas 0.25.3 which is incompatible.\n",
            "autogluon-features 0.3.1 requires scikit-learn<0.25,>=0.23.2, but you have scikit-learn 0.22.2.post1 which is incompatible.\n",
            "autogluon-extra 0.3.1 requires pandas<2.0,>=1.0.0, but you have pandas 0.25.3 which is incompatible.\n",
            "autogluon-extra 0.3.1 requires scikit-learn<0.25,>=0.23.2, but you have scikit-learn 0.22.2.post1 which is incompatible.\n",
            "autogluon-extra 0.3.1 requires scipy<1.7,>=1.5.4, but you have scipy 1.7.1 which is incompatible.\n",
            "autogluon-core 0.3.1 requires pandas<2.0,>=1.0.0, but you have pandas 0.25.3 which is incompatible.\n",
            "autogluon-core 0.3.1 requires scikit-learn<0.25,>=0.23.2, but you have scikit-learn 0.22.2.post1 which is incompatible.\n",
            "autogluon-core 0.3.1 requires scipy<1.7,>=1.5.4, but you have scipy 1.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed pandas-0.25.3 scikit-learn-0.22.2.post1 smac-0.13.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas",
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib-venn in /usr/local/lib/python3.7/dist-packages (0.11.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from matplotlib-venn) (1.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from matplotlib-venn) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from matplotlib-venn) (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->matplotlib-venn) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->matplotlib-venn) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->matplotlib-venn) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->matplotlib-venn) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->matplotlib-venn) (1.15.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Selecting previously unselected package libfluidsynth1:amd64.\n",
            "(Reading database ... 155804 files and directories currently installed.)\n",
            "Preparing to unpack .../libfluidsynth1_1.1.9-1_amd64.deb ...\n",
            "Unpacking libfluidsynth1:amd64 (1.1.9-1) ...\n",
            "Setting up libfluidsynth1:amd64 (1.1.9-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcan2r2yQNGJ",
        "outputId": "e1db1427-e4c9-4a6b-c0f8-19c67e412f63"
      },
      "source": [
        "!pip install geopandas\n",
        "!pip3 uninstall statsmodels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: fiona>=1.8 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.8.20)\n",
            "Requirement already satisfied: pyproj>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from geopandas) (3.2.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from geopandas) (0.25.3)\n",
            "Requirement already satisfied: shapely>=1.6 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.7.1)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (1.1.1)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (2.5.0)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (7.1.2)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (0.7.2)\n",
            "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (57.4.0)\n",
            "Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (21.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (2021.5.30)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->geopandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->geopandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->geopandas) (1.19.5)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Found existing installation: statsmodels 0.10.2\n",
            "Uninstalling statsmodels-0.10.2:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/statsmodels-0.10.2.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/statsmodels/*\n",
            "Proceed (Y/n)? "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkryONAjFuz7"
      },
      "source": [
        "**Install MLBox and H2O**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mfd23yfVJ61m"
      },
      "source": [
        "!pip install mlbox\n",
        "!pip install -f http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html h2o"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iENit0lChKYW"
      },
      "source": [
        "!pip install seaborn\n",
        "!pip install matplotlib\n",
        "!pip install --upgrade matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Odx88xt1Hfb2"
      },
      "source": [
        "Train and Test a Gradient Boosting Model (GBM) model:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpqtE1AXHEE6"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "url = \"https://www.kaggle.com/fatiimaezzahra/famous-iconic-women\"\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  #Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    kaggle_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = stats_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=11)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=100,\n",
        "                                    max_depth=4,\n",
        "                                    learn_rate=0.1)\n",
        "    #Specify the predictor set and response.\n",
        "    x = list(stats_train.columns)\n",
        "    x\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLeeQQJcMUw4"
      },
      "source": [
        "#Performing Weather forecast using H2O:\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "url=\"https://www.climate.gov/maps-data/datasets\"\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  #Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    weather_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = weather_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1313)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=100,\n",
        "                                    max_depth=4,\n",
        "                                    learn_rate=0.1)\n",
        "    #Specify the predictor set and response.\n",
        "    x = list(weather_data.columns)\n",
        "    x\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q39N2abkMvSc"
      },
      "source": [
        "#MLbox\n",
        "from mlbox.optimisation import Optimiser, Regressor\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "#Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "url=\"https://www.kaggle.com/mattiuzc/stock-exchange-data\"\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  #Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    stock_data[each_new_col] = stock_data[each_new_col].map(int)\n",
        "    X, y = stock_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1313)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=100,\n",
        "                                    max_depth=4,\n",
        "                                    learn_rate=0.1)\n",
        "    #Specify the predictor set and response.\n",
        "    x = list(stock_data.columns)\n",
        "    x\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)\n",
        "    \n",
        "    #Evaluate the pipeline.\n",
        "    opt = Optimiser()\n",
        "    params = {\"ne__numerical_strategy\" : 0, \"ce__strategy\" : \"label_encoding\", \"fs__threshold\" : 0.1, \"stck__base_estimators\" : [Regressor(strategy=\"RandomForest\"), Regressor(strategy=\"ExtraTrees\")], \"est__strategy\" : \"Linear\"}\n",
        "\n",
        "    df = {\"train\" : pd.DataFrame(dataset.data), \"target\" : pd.Series(dataset.target)}\n",
        "    opt.evaluate(params, df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2Ss2MoN1fJk"
      },
      "source": [
        "#MNIST dataset\n",
        "from mlbox.optimisation import Optimiser, Regressor\n",
        "import pandas as pd\n",
        "mnist_train_data=pd.read_csv(\"/content/sample_data/mnist_train_small.csv\")\n",
        "mnist_test_data=pd.read_csv(\"/content/sample_data/mnist_test.csv\")\n",
        "mnist_data = pd.merge(mnist_train_data, mnist_test_data)\n",
        "#Load the data.\n",
        "dataset = mnist_data\n",
        "#Evaluate the pipeline.\n",
        "opt = Optimiser()\n",
        "params = {\"ne__numerical_strategy\" : 0, \"ce__strategy\" : \"label_encoding\", \"fs__threshold\" : 0.1, \"stck__base_estimators\" : [Regressor(strategy=\"RandomForest\"), Regressor(strategy=\"ExtraTrees\")], \"est__strategy\" : \"Linear\"}\n",
        "df = {\"train\" : pd.DataFrame(mnist_train_data.iloc[:,:-1]), \"target\" : pd.Series(mnist_test_data.iloc[:,-1])}\n",
        "\n",
        "#Build a keras model.\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "model = keras.Sequential()\n",
        "#ReLU: Rectified Linear Unit.\n",
        "#Add a densely-connected layer with 64 units to the model.\n",
        "model.add(keras.layers.Dense(64, activation='relu'))\n",
        "#Add another.\n",
        "model.add(keras.layers.Dense(64, activation='relu'))\n",
        "#Add a softmax layer with 10 output units.\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "#Define a ConvModel.\n",
        "class ConvModel(tf.keras.Model):\n",
        "    def __init__(self, nfs, input_shape, output_shape, use_bn=False, use_dp=False):\n",
        "        super(ConvModel, self).__init__(name='mlp')\n",
        "        self.use_bn = use_bn\n",
        "        self.use_dp = use_dp\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        #Backbone layers\n",
        "        self.convs = [ConvLayer(nfs[0], s=1, input_shape=input_shape)]\n",
        "        self.convs += [ConvLayer(nf) for nf in nfs[1:]]\n",
        "        #Classification layers\n",
        "        self.convs.append(AveragePooling2D())\n",
        "        self.convs.append(Dense(output_shape, activation='softmax'))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        for layer in self.convs: inputs = layer(inputs)\n",
        "        return inputs\n",
        "#Compile the model.\n",
        "model.compile(loss='categorical crossentropy', metrics=['accuracy'], optimizer='rmsprop')\n",
        "model.build((None, 32, 32, 3))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "#Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "\n",
        "#Famous iconic women dataset\n",
        "url = \"https://www.kaggle.com/fatiimaezzahra/famous-iconic-women\"\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  #Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    kaggle_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = stats_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=11)\n",
        "    \n",
        "    history = model.fit(x_train, y_train,\n",
        "                    batch_size=64,\n",
        "                    epochs=1)\n",
        "\n",
        "    model.summary()\n",
        "    input_shape = (2, 3, 4)\n",
        "    x1 = tf.random.normal(input_shape)\n",
        "    x2 = tf.random.normal(input_shape)\n",
        "    y = tf.keras.layers.Add()([x1, x2])\n",
        "    print(y.shape)\n",
        "\n",
        "    tf.keras.layers.LSTM(3, activation='tanh', recurrent_activation='sigmoid',\n",
        "        use_bias=True, kernel_initializer='glorot_uniform',\n",
        "        recurrent_initializer='orthogonal',\n",
        "        bias_initializer='zeros', unit_forget_bias=True, dropout=0.0, recurrent_dropout=0.0,\n",
        "        return_sequences=False, return_state=False, go_backwards=False, stateful=False,\n",
        "        time_major=False, unroll=False)\n",
        "\n",
        "    #Define a ConvLayer.\n",
        "    class ConvLayer(Layer) :\n",
        "        def __init__(self, nf, ks=3, s=2, **kwargs):\n",
        "            self.nf = nf\n",
        "            self.grelu = GeneralReLU(leak=0.01)\n",
        "            self.conv = (Conv2D(filters     = nf,\n",
        "                                kernel_size = ks,\n",
        "                                strides     = s,\n",
        "                                padding     = \"same\",\n",
        "                                use_bias    = False,\n",
        "                                activation  = \"linear\"))\n",
        "            super(ConvLayer, self).__init__(**kwargs)\n",
        "\n",
        "        def rsub(self): return -self.grelu.sub\n",
        "        def set_sub(self, v): self.grelu.sub = -v\n",
        "        def conv_weights(self): return self.conv.weight[0]\n",
        "\n",
        "        def build(self, input_shape):\n",
        "            #No weight to train.\n",
        "            super(ConvLayer, self).build(input_shape)  #Be sure to call this at the end.\n",
        "\n",
        "        def compute_output_shape(self, input_shape):\n",
        "            output_shape = (input_shape[0],\n",
        "                            input_shape[1]/2,\n",
        "                            input_shape[2]/2,\n",
        "                            self.nf)\n",
        "            return output_shape\n",
        "\n",
        "        def call(self, x):\n",
        "            return self.grelu(self.conv(x))\n",
        "\n",
        "        def __repr__(self):\n",
        "            return f'ConvLayer(nf={self.nf}, activation={self.grelu})'\n",
        "\n",
        "    opt.evaluate(params, df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVw1xtRJeJle"
      },
      "source": [
        "#Outbrain Click Prediction dataset\n",
        "from mlbox.optimisation import Optimiser, Regressor\n",
        "import pandas as pd\n",
        "clicks_train_data=pd.read_csv(\"C:/Users/Administrator/OneDrive - Bitwise Solutions Private Limited/Documents/AutoML/OutbrainClickPrediction/clicks_train.csv\")\n",
        "clicks_test_data=pd.read_csv(\"C:/Users/Administrator/OneDrive - Bitwise Solutions Private Limited/Documents/AutoML/OutbrainClickPrediction/clicks_test.csv\")\n",
        "clicks_data = pd.merge(clicks_train_data, clicks_test_data)\n",
        "#Load the data.\n",
        "dataset = clicks_data\n",
        "#Evaluate the pipeline.\n",
        "opt = Optimiser()\n",
        "params = {\"ne__numerical_strategy\" : 0, \"ce__strategy\" : \"label_encoding\", \"fs__threshold\" : 0.1, \"stck__base_estimators\" : [Regressor(strategy=\"RandomForest\"), Regressor(strategy=\"ExtraTrees\")], \"est__strategy\" : \"Linear\"}\n",
        "df = {\"train\" : pd.DataFrame(clicks_train_data.iloc[:,:-1]), \"target\" : pd.Series(clicks_test_data.iloc[:,-1])}\n",
        "\n",
        "#Build a keras model.\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "model = keras.Sequential()\n",
        "#ReLU: Rectified Linear Unit.\n",
        "#Add a densely-connected layer with 64 units to the model.\n",
        "model.add(keras.layers.Dense(64, activation='relu'))\n",
        "#Add another.\n",
        "model.add(keras.layers.Dense(64, activation='relu'))\n",
        "#Add a softmax layer with 10 output units.\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "#Define a ConvModel.\n",
        "class ConvModel(tf.keras.Model):\n",
        "    def __init__(self, nfs, input_shape, output_shape, use_bn=False, use_dp=False):\n",
        "        super(ConvModel, self).__init__(name='mlp')\n",
        "        self.use_bn = use_bn\n",
        "        self.use_dp = use_dp\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        #Backbone layers\n",
        "        self.convs = [ConvLayer(nfs[0], s=1, input_shape=input_shape)]\n",
        "        self.convs += [ConvLayer(nf) for nf in nfs[1:]]\n",
        "        #Classification layers\n",
        "        self.convs.append(AveragePooling2D())\n",
        "        self.convs.append(Dense(output_shape, activation='softmax'))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        for layer in self.convs: inputs = layer(inputs)\n",
        "        return inputs\n",
        "#Compile the model.\n",
        "model.compile(loss='categorical crossentropy', metrics=['accuracy'], optimizer='rmsprop')\n",
        "model.build((None, 32, 32, 3))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "#Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "\n",
        "url = \"https://www.kaggle.com/c/outbrain-click-prediction/data\"\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  #Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    kaggle_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = stats_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=11)\n",
        "    \n",
        "    history = model.fit(x_train, y_train,\n",
        "                    batch_size=64,\n",
        "                    epochs=1)\n",
        "\n",
        "    model.summary()\n",
        "    input_shape = (2, 3, 4)\n",
        "    x1 = tf.random.normal(input_shape)\n",
        "    x2 = tf.random.normal(input_shape)\n",
        "    y = tf.keras.layers.Add()([x1, x2])\n",
        "    print(y.shape)\n",
        "\n",
        "    tf.keras.layers.LSTM(3, activation='tanh', recurrent_activation='sigmoid',\n",
        "        use_bias=True, kernel_initializer='glorot_uniform',\n",
        "        recurrent_initializer='orthogonal',\n",
        "        bias_initializer='zeros', unit_forget_bias=True, dropout=0.0, recurrent_dropout=0.0,\n",
        "        return_sequences=False, return_state=False, go_backwards=False, stateful=False,\n",
        "        time_major=False, unroll=False)\n",
        "\n",
        "    #Define a ConvLayer.\n",
        "    class ConvLayer(Layer) :\n",
        "        def __init__(self, nf, ks=3, s=2, **kwargs):\n",
        "            self.nf = nf\n",
        "            self.grelu = GeneralReLU(leak=0.01)\n",
        "            self.conv = (Conv2D(filters     = nf,\n",
        "                                kernel_size = ks,\n",
        "                                strides     = s,\n",
        "                                padding     = \"same\",\n",
        "                                use_bias    = False,\n",
        "                                activation  = \"linear\"))\n",
        "            super(ConvLayer, self).__init__(**kwargs)\n",
        "\n",
        "        def rsub(self): return -self.grelu.sub\n",
        "        def set_sub(self, v): self.grelu.sub = -v\n",
        "        def conv_weights(self): return self.conv.weight[0]\n",
        "\n",
        "        def build(self, input_shape):\n",
        "            #No weight to train.\n",
        "            super(ConvLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "        def compute_output_shape(self, input_shape):\n",
        "            output_shape = (input_shape[0],\n",
        "                            input_shape[1]/2,\n",
        "                            input_shape[2]/2,\n",
        "                            self.nf)\n",
        "            return output_shape\n",
        "\n",
        "        def call(self, x):\n",
        "            return self.grelu(self.conv(x))\n",
        "\n",
        "        def __repr__(self):\n",
        "            return f'ConvLayer(nf={self.nf}, activation={self.grelu})'\n",
        "\n",
        "    opt.evaluate(params, df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34C7E_3E19a4"
      },
      "source": [
        "Image Prediction with AutoGluon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQU0LjYk10Q6"
      },
      "source": [
        "#Image Prediction with AutoGluon\n",
        "#Import AutoGluon.\n",
        "%matplotlib inline\n",
        "import autogluon.core as ag\n",
        "from autogluon.vision import ImageDataset\n",
        "import pandas as pd\n",
        "#Celeb faces (celebA) dataset\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "#Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "url = \"https://www.kaggle.com/jessicali9530/celeba-dataset\"\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  #Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    olympics2021_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = stats_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1221)\n",
        "    csv_file_list=[\"list_attr_celeba.csv\", \"list_bbox_celeba.csv\", \"list_eval_partition.csv\", \"list_landmarks_align_celeba.csv\"]\n",
        "    for each_csv_file in csv_file_list:\n",
        "      csv_file = ag.utils.download(each_csv_file)\n",
        "      df = pd.read_csv(csv_file)\n",
        "      df.head()\n",
        "      df = ImageDataset.from_csv(csv_file)\n",
        "      df.head()\n",
        "    train_data, _, test_data = ImageDataset.from_folders(\"img_align_celeba.zip\", train='train', test='test')\n",
        "    print('train #', len(train_data), 'test #', len(test_data))\n",
        "    train_data.head()\n",
        "    #Load the splits with from_folder.\n",
        "    root = os.path.join(os.path.dirname(train_data.iloc[0]['image']), '..')\n",
        "    all_data = ImageDataset.from_folder(root)\n",
        "    all_data.head()\n",
        "    #Split the dataset.\n",
        "    train, val, test = all_data.random_split(val_size=0.1, test_size=0.1)\n",
        "    print('train #:', len(train), 'test #:', len(test))\n",
        "    #Convert a list of images to dataset.\n",
        "    celeba = ag.utils.download(\"img_align_celeba.zip\")\n",
        "    celeba = ag.utils.unzip(celeba)\n",
        "    image_list = [x for x in os.listdir(os.path.join(pets, 'images')) if x.endswith('jpg')]\n",
        "    new_data = ImageDataset.from_name_func(image_list, label_fn, root=os.path.join(os.getcwd(), celeba, 'images'))\n",
        "    new_data\n",
        "    #Visualize the images.\n",
        "    new_data.show_images()\n",
        "    #Image prediction\n",
        "    import autogluon.core as ag\n",
        "    from autogluon.vision import ImagePredictor, ImageDataset\n",
        "    train_dataset, _, test_dataset = ImageDataset.from_folders(\"img_align_celeba.zip\")\n",
        "    print(train_dataset)\n",
        "    #Fit a classifier.\n",
        "    predictor = ImagePredictor()\n",
        "    #Since the original dataset does not provide validation split, the `fit` function splits it randomly with 90/10 ratio.\n",
        "    predictor.fit(train_dataset, hyperparameters={'epochs': 2})\n",
        "    #The best Top-1 accuracy achieved on the validation set is:\n",
        "    fit_result = predictor.fit_summary()\n",
        "    print('Top-1 train acc: %.3f, val acc: %.3f' %(fit_result['train_acc'], fit_result['valid_acc']))\n",
        "    #Predict on a new image.\n",
        "    image_path = test_dataset.iloc[0]['image']\n",
        "    result = predictor.predict(image_path)\n",
        "    print(result)\n",
        "    bulk_result = predictor.predict(test_dataset)\n",
        "    print(bulk_result)\n",
        "    #Generate image features with a classifier.\n",
        "    image_path = test_dataset.iloc[0]['image']\n",
        "    feature = predictor.predict_feature(image_path)\n",
        "    print(feature)\n",
        "    #Validate and test top-1 accuracy.\n",
        "    test_acc = predictor.evaluate(test_dataset)\n",
        "    print('Top-1 test acc: %.3f' % test_acc['top1'])\n",
        "    #Save and load the classifiers.\n",
        "    filename = 'predictor.ag'\n",
        "    predictor.save(filename)\n",
        "    predictor_loaded = ImagePredictor.load(filename)\n",
        "    #Use predictor_loaded as usual.\n",
        "    result = predictor_loaded.predict(image_path)\n",
        "    print(result)\n",
        "    #Use AutoGluon to produce an ImagePredictor to classify images.\n",
        "    import autogluon.core as ag\n",
        "    from autogluon.vision import ImagePredictor, ImageDataset\n",
        "    train_data, _, test_data = ImageDataset.from_folders(\"img_align_celeba.zip\")\n",
        "    model = ag.Categorical('resnet18_v1b', 'mobilenetv3_small')\n",
        "    model_list = ImagePredictor.list_models()\n",
        "    #Specify the training hyper-parameters.\n",
        "    batch_size = 8\n",
        "    lr = ag.Categorical(1e-2, 1e-3)\n",
        "    #Bayesian Optimization\n",
        "    hyperparameters={'model': model, 'batch_size': batch_size, 'lr': lr, 'epochs': 2}\n",
        "    predictor = ImagePredictor()\n",
        "    predictor.fit(train_data, time_limit=60*10, hyperparameters=hyperparameters,\n",
        "                  hyperparameter_tune_kwargs={'searcher': 'bayesopt', 'num_trials': 2})\n",
        "    print('Top-1 val acc: %.3f' % predictor.fit_summary()['valid_acc'])\n",
        "    #Load the test dataset and evaluate.\n",
        "    results = predictor.evaluate(test_data)\n",
        "    print('Test acc on hold-out data:', results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0fSbsH-ZEb3"
      },
      "source": [
        "#Install.\n",
        "!pip install torch\n",
        "#Upgrade pytorch.\n",
        "!pip install --upgrade torch torchvision\n",
        "#Install H2O.\n",
        "!pip install h2o\n",
        "#Install AutoKeras.\n",
        "!pip install autokeras\n",
        "#Upgrade TensorFlow\n",
        "!pip install --ignore-installed --upgrade tensorflow\n",
        "#Install PyTorch.\n",
        "!pip install pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlGjIrj4xEfX"
      },
      "source": [
        "import pandas as pd \n",
        "#AutoGluon\n",
        "subsample_size = 2000  \n",
        "feature_columns = ['Product_Description', 'Product_Type']\n",
        "label = 'Sentiment'\n",
        "\n",
        "train_df = pd.read_csv('Participants_Data.zip', index_col=0).sample(2000, random_state=123)\n",
        "dev_df = pd.read_csv('Participants_Data.zip', index_col=0)\n",
        "test_df = pd.read_csv('Participants_Data.zip', index_col=0)\n",
        "\n",
        "train_df = train_df[feature_columns + [label]]\n",
        "dev_df = dev_df[feature_columns + [label]]\n",
        "test_df = test_df[feature_columns]\n",
        "print('Number of training samples:', len(train_df))\n",
        "print('Number of dev samples:', len(dev_df))\n",
        "print('Number of test samples:', len(test_df))\n",
        "train_df.head()\n",
        "dev_df.head()\n",
        "test_df.head()\n",
        "from autogluon.tabular import TabularPredictor\n",
        "predictor = TabularPredictor(label='Sentiment', path='ag_tabular_product_sentiment_multimodal')\n",
        "predictor.fit(train_df, hyperparameters='multimodal')\n",
        "predictor.leaderboard(dev_df)\n",
        "#Improve predictive performance by using stack ensembling.\n",
        "predictor.fit(train_df, hyperparameters='multimodal', num_bag_folds=5, num_stack_levels=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJQuBiSLHi2g"
      },
      "source": [
        "#NFL Big Data Bowl 2022 kaggle dataset\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from autokeras import StructuredDataClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import tensorflow as tf\n",
        "import autokeras as ak\n",
        "\n",
        "#Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "url = \"https://www.kaggle.com/c/nfl-big-data-bowl-2022/data\"\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  #Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    nfl_2022_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = stats_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1221)\n",
        "    csv_file_list=[\"PFFScoutingData.csv\", \"games.csv\", \"players.csv\", \"plays.csv\", \"tracking2018.csv\", \"tracking2019.csv\", \"tracking2020.csv\"]\n",
        "    for each_csv_file in csv_file_list:\n",
        "      csv_file = ag.utils.download(each_csv_file)\n",
        "      df = pd.read_csv(csv_file)\n",
        "      df.head()\n",
        "      df = ImageDataset.from_csv(csv_file)\n",
        "      df.head()\n",
        "\n",
        "#StructuredDataClassifier\n",
        "autokeras.StructuredDataClassifier(\n",
        "    column_names=None,\n",
        "    column_types=None,\n",
        "    num_classes=None,\n",
        "    multi_label=False,\n",
        "    loss=None,\n",
        "    metrics=None,\n",
        "    project_name=\"structured_data_classifier\",\n",
        "    max_trials=100,\n",
        "    directory=None,\n",
        "    objective=\"val_accuracy\",\n",
        "    tuner=None,\n",
        "    overwrite=False,\n",
        "    seed=None,\n",
        "    max_model_size=None,\n",
        "    **kwargs)\n",
        "\n",
        "#Fit.\n",
        "StructuredDataClassifier.fit(\n",
        "    x=None, y=None, epochs=None, callbacks=None, validation_split=0.2, validation_data=None, **kwargs)\n",
        "#Predict.\n",
        "StructuredDataClassifier.predict(x, **kwargs)\n",
        "#Evaluate.\n",
        "StructuredDataClassifier.evaluate(x, y=None, **kwargs)\n",
        "#Export the model using export_model.\n",
        "StructuredDataClassifier.export_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gUHW4MyxTTH"
      },
      "source": [
        "#MNIST\n",
        "from mlbox.optimisation import Optimiser, Regressor\n",
        "import pandas as pd\n",
        "mnist_train_data=pd.read_csv(\"/content/sample_data/mnist_train_small.csv\")\n",
        "mnist_test_data=pd.read_csv(\"/content/sample_data/mnist_test.csv\")\n",
        "mnist_data = pd.merge(mnist_train_data, mnist_test_data)\n",
        "#Load the data.\n",
        "dataset = mnist_data\n",
        "#Evaluate the pipeline.\n",
        "opt = Optimiser()\n",
        "params = {\"ne__numerical_strategy\" : 0, \"ce__strategy\" : \"label_encoding\", \"fs__threshold\" : 0.1, \"stck__base_estimators\" : [Regressor(strategy=\"RandomForest\"), Regressor(strategy=\"ExtraTrees\")], \"est__strategy\" : \"Linear\"}\n",
        "df = {\"train\" : pd.DataFrame(mnist_train_data.iloc[:,:-1]), \"target\" : pd.Series(mnist_test_data.iloc[:,-1])}\n",
        "\n",
        "#Build a keras model.\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "model = keras.Sequential()\n",
        "#ReLU: Rectified Linear Unit.\n",
        "#Adds a densely-connected layer with 64 units to the model.\n",
        "model.add(keras.layers.Dense(64, activation='relu'))\n",
        "#Add another.\n",
        "model.add(keras.layers.Dense(64, activation='relu'))\n",
        "#Add a softmax layer with 10 output units.\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "#Define a ConvModel.\n",
        "class ConvModel(tf.keras.Model):\n",
        "    def __init__(self, nfs, input_shape, output_shape, use_bn=False, use_dp=False):\n",
        "        super(ConvModel, self).__init__(name='mlp')\n",
        "        self.use_bn = use_bn\n",
        "        self.use_dp = use_dp\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        #Backbone layers\n",
        "        self.convs = [ConvLayer(nfs[0], s=1, input_shape=input_shape)]\n",
        "        self.convs += [ConvLayer(nf) for nf in nfs[1:]]\n",
        "        #Classification layers.\n",
        "        self.convs.append(AveragePooling2D())\n",
        "        self.convs.append(Dense(output_shape, activation='softmax'))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        for layer in self.convs: inputs = layer(inputs)\n",
        "        return inputs\n",
        "#Compile the model.\n",
        "model.compile(loss='categorical crossentropy', metrics=['accuracy'], optimizer='rmsprop')\n",
        "model.build((None, 32, 32, 3))\n",
        "model.summary()\n",
        "\n",
        "#Olympics 2021 dataset\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "#Dataset's URL:\n",
        "url = \"https://www.kaggle.com/arjunprasadsarkhel/2021-olympics-in-tokyo\"\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  #Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    olympics2021_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = stats_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=11)\n",
        "    \n",
        "    history = model.fit(x_train, y_train,\n",
        "                    batch_size=64,\n",
        "                    epochs=1)\n",
        "\n",
        "    model.summary()\n",
        "    input_shape = (2, 3, 4)\n",
        "    x1 = tf.random.normal(input_shape)\n",
        "    x2 = tf.random.normal(input_shape)\n",
        "    y = tf.keras.layers.Add()([x1, x2])\n",
        "    print(y.shape)\n",
        "\n",
        "    tf.keras.layers.LSTM(3, activation='tanh', recurrent_activation='sigmoid',\n",
        "        use_bias=True, kernel_initializer='glorot_uniform',\n",
        "        recurrent_initializer='orthogonal',\n",
        "        bias_initializer='zeros', unit_forget_bias=True, dropout=0.0, recurrent_dropout=0.0,\n",
        "        return_sequences=False, return_state=False, go_backwards=False, stateful=False,\n",
        "        time_major=False, unroll=False)\n",
        "\n",
        "    #Define a ConvLayer.\n",
        "    class ConvLayer(Layer) :\n",
        "        def __init__(self, nf, ks=3, s=2, **kwargs):\n",
        "            self.nf = nf\n",
        "            self.grelu = GeneralReLU(leak=0.01)\n",
        "            self.conv = (Conv2D(filters     = nf,\n",
        "                                kernel_size = ks,\n",
        "                                strides     = s,\n",
        "                                padding     = \"same\",\n",
        "                                use_bias    = False,\n",
        "                                activation  = \"linear\"))\n",
        "            super(ConvLayer, self).__init__(**kwargs)\n",
        "\n",
        "        def rsub(self): return -self.grelu.sub\n",
        "        def set_sub(self, v): self.grelu.sub = -v\n",
        "        def conv_weights(self): return self.conv.weight[0]\n",
        "\n",
        "        def build(self, input_shape):\n",
        "            #No weight to train.\n",
        "            super(ConvLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "        def compute_output_shape(self, input_shape):\n",
        "            output_shape = (input_shape[0],\n",
        "                            input_shape[1]/2,\n",
        "                            input_shape[2]/2,\n",
        "                            self.nf)\n",
        "            return output_shape\n",
        "\n",
        "        def call(self, x):\n",
        "            return self.grelu(self.conv(x))\n",
        "\n",
        "        def __repr__(self):\n",
        "            return f'ConvLayer(nf={self.nf}, activation={self.grelu})'\n",
        "\n",
        "    opt.evaluate(params, df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_YoMW88Lrz0"
      },
      "source": [
        "import pandas as pd\n",
        "dataset_dict={\"New York City Airport Activity\": \"https://www.kaggle.com/sveneschlbeck/new-york-city-airport-activity?select=nyc-flights.csv\"}\n",
        "trained_model = automl(**dataset_dict)\n",
        "print(trained_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKqHYODQyI4b"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "#Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "url = \"https://www.kaggle.com/andrewmvd/heart-failure-clinical-data\"\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  #Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    olympics2021_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = stats_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1001)\n",
        "    \n",
        "    history = model.fit(x_train, y_train,\n",
        "                    batch_size=64,\n",
        "                    epochs=1000)\n",
        "\n",
        "    model.summary()\n",
        "    input_shape = (2, 3, 4)\n",
        "    x1 = tf.random.normal(input_shape)\n",
        "    x2 = tf.random.normal(input_shape)\n",
        "    y = tf.keras.layers.Add()([x1, x2])\n",
        "    print(y.shape)\n",
        "\n",
        "    tf.keras.layers.LSTM(3, activation='tanh', recurrent_activation='sigmoid',\n",
        "        use_bias=True, kernel_initializer='glorot_uniform',\n",
        "        recurrent_initializer='orthogonal',\n",
        "        bias_initializer='zeros', unit_forget_bias=True, dropout=0.0, recurrent_dropout=0.0,\n",
        "        return_sequences=False, return_state=False, go_backwards=False, stateful=False,\n",
        "        time_major=False, unroll=False)\n",
        "\n",
        "    #Define a ConvLayer.\n",
        "    class ConvLayer(Layer) :\n",
        "        def __init__(self, nf, ks=3, s=2, **kwargs):\n",
        "            self.nf = nf\n",
        "            self.grelu = GeneralReLU(leak=0.01)\n",
        "            self.conv = (Conv2D(filters     = nf,\n",
        "                                kernel_size = ks,\n",
        "                                strides     = s,\n",
        "                                padding     = \"same\",\n",
        "                                use_bias    = False,\n",
        "                                activation  = \"linear\"))\n",
        "            super(ConvLayer, self).__init__(**kwargs)\n",
        "\n",
        "        def rsub(self): return -self.grelu.sub\n",
        "        def set_sub(self, v): self.grelu.sub = -v\n",
        "        def conv_weights(self): return self.conv.weight[0]\n",
        "\n",
        "        def build(self, input_shape):\n",
        "            #No weight to train.\n",
        "            super(ConvLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "        def compute_output_shape(self, input_shape):\n",
        "            output_shape = (input_shape[0],\n",
        "                            input_shape[1]/2,\n",
        "                            input_shape[2]/2,\n",
        "                            self.nf)\n",
        "            return output_shape\n",
        "\n",
        "        def call(self, x):\n",
        "            return self.grelu(self.conv(x))\n",
        "\n",
        "        def __repr__(self):\n",
        "            return f'ConvLayer(nf={self.nf}, activation={self.grelu})'\n",
        "\n",
        "    opt.evaluate(params, df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mqdaw3DliOu0"
      },
      "source": [
        "#Explore California housing dataset using MLBox.\n",
        "from mlbox.optimisation import Optimiser, Regressor\n",
        "import pandas as pd\n",
        "cal_house_train_data=pd.read_csv(\"/content/sample_data/california_housing_train.csv\")\n",
        "cal_house_test_data=pd.read_csv(\"/content/sample_data/california_housing_test.csv\")\n",
        "cal_house_data = pd.merge(cal_house_train_data, cal_house_test_data)\n",
        "#Load the data.\n",
        "dataset = cal_house_data\n",
        "#Evaluate the pipeline.\n",
        "opt = Optimiser()\n",
        "params = {\"ne__numerical_strategy\" : 0, \"ce__strategy\" : \"label_encoding\", \"fs__threshold\" : 0.1, \"stck__base_estimators\" : [Regressor(strategy=\"RandomForest\"), Regressor(strategy=\"ExtraTrees\")], \"est__strategy\" : \"Linear\"}\n",
        "df = {\"train\" : pd.DataFrame(cal_house_train_data.iloc[:,:-1]), \"target\" : pd.Series(cal_house_test_data.iloc[:,-1])}\n",
        "opt.evaluate(params, df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2Aj53iqLBY0"
      },
      "source": [
        "#Explore Amazon dataset using MLBox.\n",
        "from mlbox.optimisation import Optimiser, Regressor\n",
        "import pandas as pd\n",
        "amazon_train_data=pd.read_csv(\"C:\\Users\\Administrator\\OneDrive - Bitwise Solutions Private Limited\\Documents\\AutoML\\amazon 2\\train.arff\")\n",
        "amazon_test_data=pd.read_csv(\"C:\\Users\\Administrator\\OneDrive - Bitwise Solutions Private Limited\\Documents\\AutoML\\amazon 2\\test.arff\")\n",
        "amazon_data = pd.merge(amazon_train_data, amazon_test_data)\n",
        "#Load the data.\n",
        "dataset = amazon_data\n",
        "#Evaluate the pipeline.\n",
        "opt = Optimiser()\n",
        "params = {\"ne__numerical_strategy\" : 0, \"ce__strategy\" : \"label_encoding\", \"fs__threshold\" : 0.1, \"stck__base_estimators\" : [Regressor(strategy=\"RandomForest\"), Regressor(strategy=\"ExtraTrees\")], \"est__strategy\" : \"Linear\"}\n",
        "df = {\"train\" : pd.DataFrame(amazon_train_data.iloc[:,:-1]), \"target\" : pd.Series(amazon_test_data.iloc[:,-1])}\n",
        "opt.evaluate(params, df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VyS7p7yxFCI"
      },
      "source": [
        "#Explore the \"Covid-19 in India\" dataset.\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import geopandas as gpd\n",
        "from prettytable import PrettyTable\n",
        "url = \"https://www.kaggle.com/sudalairajkumar/covid19-in-india\"\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "    stat = extract_contents(row.find_all('td')) \n",
        "#Notice that the data that we require is now a list of length 5.\n",
        "    if len(stat) == 5:\n",
        "        stats.append(stat)\n",
        "#Now convert the data into a pandas dataframe for further processing.\n",
        "new_cols = [\"Sr.No\", \"States/UT\",\"Confirmed\",\"Recovered\",\"Deceased\"]\n",
        "state_data = pd.DataFrame(data = stats, columns = new_cols)\n",
        "state_data.head()\n",
        "#Scraped data columns are of ‘string’ datatype.\n",
        "#Convert them into ‘int' datatype.\n",
        "state_data[\"Confirmed\"] = state_data[\"Confirmed\"].map(int)\n",
        "state_data[\"Recovered\"] = state_data[\"Recovered\"].map(int)\n",
        "state_data[\"Deceased\"] = state_data[\"Deceased\"].map(int)\n",
        "#Present the data using Pretty table.\n",
        "table = PrettyTable()\n",
        "table.field_names = (new_cols)\n",
        "for i in stats:\n",
        "    table.add_row(i)\n",
        "table.add_row(['','Total', \n",
        "               sum(state_data[\"Confirmed\"]), \n",
        "               sum(state_data[\"Recovered\"]),\n",
        "               sum(state_data[\"Deceased\"])])\n",
        "print(table)\n",
        "#Utilize Barplot to show total confirmed cases Statewise.\n",
        "sns.set_style(\"ticks\")\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.barh(state_data[\"States/UT\"], state_data[\"Confirmed\"].map(int),\n",
        "         align = 'center', color = 'lightblue', edgecolor = 'blue')\n",
        "plt.xlabel('No. of Confirmed cases', fontsize = 18)\n",
        "plt.ylabel('States/UT', fontsize = 18)\n",
        "plt.gca().invert_yaxis() # This is to maintain the order in which the states appear\n",
        "plt.xticks(fontsize = 14) \n",
        "plt.yticks(fontsize = 14)\n",
        "plt.title('Total Confirmed Cases Statewise', fontsize = 20)\n",
        "for index, value in enumerate(state_data[\"Confirmed\"]):\n",
        "    plt.text(value, index, str(value), fontsize = 12, verticalalignment = 'center')\n",
        "plt.show()  \n",
        "#Utilize donut chart representing nationwide total confirmed, cured and deceased cases.\n",
        "group_size = [sum(state_data['Confirmed']), \n",
        "              sum(state_data['Recovered']), \n",
        "              sum(state_data['Deceased'])]\n",
        "group_labels = ['Confirmed\\n' + str(sum(state_data['Confirmed'])), \n",
        "                'Recovered\\n' + str(sum(state_data['Recovered'])), \n",
        "                'Deceased\\n'  + str(sum(state_data['Deceased']))]\n",
        "custom_colors = ['skyblue','yellowgreen','tomato']\n",
        "plt.figure(figsize = (5,5))\n",
        "plt.pie(group_size, labels = group_labels, colors = custom_colors)\n",
        "central_circle = plt.Circle((0,0), 0.5, color = 'white')\n",
        "fig = plt.gcf()\n",
        "fig.gca().add_artist(central_circle)\n",
        "plt.rc('font', size = 12) \n",
        "plt.title('Nationwide total Confirmed, Recovered and Deceased Cases', fontsize = 16)\n",
        "plt.show()\n",
        "\n",
        "import fiona\n",
        "#Read the shape file of map of India in GeoDataFrame.\n",
        "map_data = gpd.read_file(\"Indian_States.shp\")\n",
        "map_data.rename(columns = {\"st_nm\":\"States/UT\"}, inplace = True)\n",
        "map_data.head()\n",
        "map_data[\"States/UT\"] = map_data[\"States/UT\"].str.replace(\"&\",\"and\")\n",
        "map_data[\"States/UT\"].replace(\"Arunanchal Pradesh\",\n",
        "                              \"Arunachal Pradesh\", inplace = True)\n",
        "map_data[\"States/UT\"].replace(\"Telangana\", \n",
        "                              \"Telengana\", inplace = True)\n",
        "map_data[\"States/UT\"].replace(\"NCT of Delhi\", \n",
        "                              \"Delhi\", inplace = True)\n",
        "map_data[\"States/UT\"].replace(\"Andaman and Nicobar Island\", \n",
        "                              \"Andaman and Nicobar Islands\", \n",
        "                               inplace = True)\n",
        "merged_data = pd.merge(map_data, state_data, \n",
        "                       how = \"left\", on = \"States/UT\")\n",
        "merged_data.fillna(0, inplace = True)\n",
        "merged_data.drop(\"Sr.No\", axis = 1, inplace = True)\n",
        "merged_data.head()\n",
        "\n",
        "#MLbox:\n",
        "from mlbox.optimisation import Optimiser\n",
        "#Evaluate the pipeline.\n",
        "opt = Optimiser()\n",
        "params = {\"ne__numerical_strategy\" : 0, \"ce__strategy\" : \"label_encoding\", \"fs__threshold\" : 0.1, \"stck__base_estimators\" : [Regressor(strategy=\"RandomForest\"), Regressor(strategy=\"ExtraTrees\")], \"est__strategy\" : \"Linear\"}\n",
        "df = {\"train\" : pd.DataFrame(dataset.data), \n",
        "      \"target\" : pd.Series(dataset.target)}\n",
        "opt.evaluate(params, df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNnlTba0t6GR"
      },
      "source": [
        "#Fashion MNIST dataset\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "#Explore the data.\n",
        "train_images.shape\n",
        "#Obtain the length of the train labels.\n",
        "len(train_labels)\n",
        "\n",
        "train_labels\n",
        "#Get the shape of test images.\n",
        "test_images.shape\n",
        "#Obtain the length of the test labels.\n",
        "len(test_labels)\n",
        "\n",
        "#Preprocess the data.\n",
        "plt.figure()\n",
        "plt.imshow(train_images[0])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "train_images = train_images / 255.0\n",
        "\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[train_labels[i]])\n",
        "plt.show()\n",
        "\n",
        "#Build the model. Set up the layers.\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "#Compile the model.\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Train the model. Feed the model.\n",
        "model.fit(train_images, train_labels, epochs=1000)\n",
        "\n",
        "#Evaluate the accuracy of the model.\n",
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "print('\\nTest accuracy:', test_acc)\n",
        "\n",
        "#Make the predictions.\n",
        "probability_model = tf.keras.Sequential([model, \n",
        "                                         tf.keras.layers.Softmax()])\n",
        "#Predict.\n",
        "predictions = probability_model.predict(test_images)\n",
        "predictions[0]\n",
        "np.argmax(predictions[0])\n",
        "test_labels[0]\n",
        "\n",
        "#Look at the full set of 10 class predictions.\n",
        "def plot_image(i, predictions_array, true_label, img):\n",
        "  true_label, img = true_label[i], img[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "\n",
        "  plt.imshow(img, cmap=plt.cm.binary)\n",
        "\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  if predicted_label == true_label:\n",
        "    color = 'blue'\n",
        "  else:\n",
        "    color = 'red'\n",
        "\n",
        "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
        "                                100*np.max(predictions_array),\n",
        "                                class_names[true_label]),\n",
        "                                color=color)\n",
        "\n",
        "def plot_value_array(i, predictions_array, true_label):\n",
        "  true_label = true_label[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks(range(10))\n",
        "  plt.yticks([])\n",
        "  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
        "  plt.ylim([0, 1])\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "\n",
        "  thisplot[predicted_label].set_color('red')\n",
        "  thisplot[true_label].set_color('blue')\n",
        "\n",
        "#Verify the predictions.\n",
        "i = 0\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(i, predictions[i], test_labels, test_images)\n",
        "plt.subplot(1,2,2)\n",
        "plot_value_array(i, predictions[i],  test_labels)\n",
        "plt.show()\n",
        "\n",
        "i = 12\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(i, predictions[i], test_labels, test_images)\n",
        "plt.subplot(1,2,2)\n",
        "plot_value_array(i, predictions[i],  test_labels)\n",
        "plt.show()\n",
        "\n",
        "#Plot the first X test images, their predicted labels, and the true labels.\n",
        "#Color correct predictions in blue and incorrect predictions in red.\n",
        "num_rows = 5\n",
        "num_cols = 3\n",
        "num_images = num_rows*num_cols\n",
        "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
        "for i in range(num_images):\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
        "  plot_image(i, predictions[i], test_labels, test_images)\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
        "  plot_value_array(i, predictions[i], test_labels)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#Use the trained model.\n",
        "#Grab an image from the test dataset.\n",
        "img = test_images[1]\n",
        "print(img.shape)\n",
        "#Add the image to a batch where it's the only member.\n",
        "img = (np.expand_dims(img,0))\n",
        "print(img.shape)\n",
        "#Now predict the correct label for this image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la10NR9BPrDA"
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import geopandas as gpd\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "#Specify URL for Web Scraping.\n",
        "url = \"https://www.mygov.in/corona-data/covid19-statewise-status/\"\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "    stat = extract_contents(row.find_all('td')) \n",
        "#Notice that the data that we require is now a list of length 5.\n",
        "    if len(stat) == 5:\n",
        "        stats.append(stat)\n",
        "#Now convert the data into a pandas dataframe for further processing.\n",
        "new_cols = [\"Sr.No\", \"States/UT\",\"Confirmed\",\"Recovered\",\"Deceased\"]\n",
        "state_data = pd.DataFrame(data = stats, columns = new_cols)\n",
        "state_data.head()\n",
        "#Scraped data columns are of ‘string’ datatype.\n",
        "#Convert them into ‘int' datatype.\n",
        "state_data[\"Confirmed\"] = state_data[\"Confirmed\"].map(int)\n",
        "state_data[\"Recovered\"] = state_data[\"Recovered\"].map(int)\n",
        "state_data[\"Deceased\"] = state_data[\"Deceased\"].map(int)\n",
        "\n",
        "#Present the data using Pretty table.\n",
        "table = PrettyTable()\n",
        "table.field_names = (new_cols)\n",
        "for i in stats:\n",
        "    table.add_row(i)\n",
        "table.add_row(['','Total', \n",
        "               sum(state_data[\"Confirmed\"]), \n",
        "               sum(state_data[\"Recovered\"]),\n",
        "               sum(state_data[\"Deceased\"])])\n",
        "print(table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1GERsCJXPOq"
      },
      "source": [
        "from mlbox.preprocessing import Reader\n",
        "from mlbox.preprocessing import Drift_thresholder\n",
        "from mlbox.optimisation import Optimiser\n",
        "from mlbox.prediction import Predictor\n",
        "\n",
        "#Paths to the train set and the test set.\n",
        "url = \"https://www.kaggle.com/shivamb/netflix-shows\"\n",
        "#Name of the feature to predict.\n",
        "#This columns should only be present in the train set.\n",
        "target_name = \"rating\"\n",
        "\n",
        "#Reading and cleaning all files\n",
        "#Declare a reader for csv files\n",
        "rd = Reader(sep=',')\n",
        "#Return a dictionary containing three entries\n",
        "# dict[\"train\"] contains training samples withtout target columns\n",
        "# dict[\"test\"] contains testing elements withtout target columns\n",
        "# dict[\"target\"] contains target columns for training samples.\n",
        "data = rd.train_test_split(\"https://www.kaggle.com/shivamb/netflix-shows\", target_name)\n",
        "\n",
        "dft = Drift_thresholder()\n",
        "data = dft.fit_transform(data)\n",
        "\n",
        "#Tuning\n",
        "# Declare an optimiser. Scoring possibilities for classification lie in :\n",
        "# {\"accuracy\", \"roc_auc\", \"f1\", \"neg_log_loss\", \"precision\", \"recall\"}\n",
        "opt = Optimiser(scoring='accuracy', n_folds=3)\n",
        "opt.evaluate(None, data)\n",
        "\n",
        "# Space of hyperparameters\n",
        "# The keys must respect the following syntax : \"enc__param\".\n",
        "#   \"enc\" = \"ne\" for na encoder\n",
        "#   \"enc\" = \"ce\" for categorical encoder\n",
        "#   \"enc\" = \"fs\" for feature selector [OPTIONAL]\n",
        "#   \"enc\" = \"stck\"+str(i) to add layer n°i of meta-features [OPTIONAL]\n",
        "#   \"enc\" = \"est\" for the final estimator\n",
        "#   \"param\" : a correct associated parameter for each step.\n",
        "#   Ex: \"max_depth\" for \"enc\"=\"est\", ...\n",
        "# The values must respect the syntax: {\"search\":strategy,\"space\":list}\n",
        "#   \"strategy\" = \"choice\" or \"uniform\". Default = \"choice\"\n",
        "#   list : a list of values to be tested if strategy=\"choice\".\n",
        "#   Else, list = [value_min, value_max].\n",
        "# Available strategies for ne_numerical_strategy are either an integer, a float\n",
        "#   or in {'mean', 'median', \"most_frequent\"}\n",
        "# Available strategies for ce_strategy are:\n",
        "#   {\"label_encoding\", \"dummification\", \"random_projection\", entity_embedding\"}\n",
        "space = {'ne__numerical_strategy': {\"search\": \"choice\", \"space\": [0]},\n",
        "         'ce__strategy': {\"search\": \"choice\",\n",
        "                          \"space\": [\"label_encoding\",\n",
        "                                    \"random_projection\",\n",
        "                                    \"entity_embedding\"]},\n",
        "         'fs__threshold': {\"search\": \"uniform\",\n",
        "                           \"space\": [0.01, 0.3]},\n",
        "         'est__max_depth': {\"search\": \"choice\",\n",
        "                            \"space\": [3, 4, 5, 6, 7]}\n",
        "\n",
        "         }\n",
        "\n",
        "# Optimises hyper-parameters of the whole Pipeline with a given scoring\n",
        "# function. Algorithm used to optimize : Tree Parzen Estimator.\n",
        "#\n",
        "# IMPORTANT : Try to avoid dependent parameters and to set one feature\n",
        "# selection strategy and one estimator strategy at a time.\n",
        "best = opt.optimise(space, data, 15)\n",
        "\n",
        "# Make prediction and save the results in save folder.\n",
        "prd = Predictor()\n",
        "prd.fit_predict(best, data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7rsYiwzMX5J"
      },
      "source": [
        "kaggle competitions download -c petfinder-adoption-predictionkaggle competitions download -c petfinder-adoption-url = \n",
        "u$ brew install jenv$!val scoreFn = new OpWorkflowRunnerLocal(workflow).scoreFunction(opParams)\n",
        "val scoreFn = new OpWorkflowRunnerLocal(workflow).scoreFunction(opParams)\n",
        "valInstall GraphViz & PyDot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNrnVvzHMVW0"
      },
      "source": [
        "#Install Graphviz and pydot.\n",
        "!apt-get -qq install -y graphviz && pip install pydot\n",
        "import pydot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZQD0xcaMwkx"
      },
      "source": [
        "#Install Cartopy.\n",
        "!pip install cartopy\n",
        "import cartopy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MExhf7pqfDEN"
      },
      "source": [
        "**TPOT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3p4bUw3ey9p"
      },
      "source": [
        "Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science\n",
        "\n",
        "[link text](https://)Randal S. Olson, Nathan Bartley, Ryan J. Urbanowicz, and Jason H. Moore (2016). Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science. Proceedings of GECCO 2016, pages 485-492.\n",
        "\n",
        "> Developed by Randal S. Olson and others at the University of Pennsylvania.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8P9iEpZkB9"
      },
      "source": [
        "**Install TPOT:**\n",
        "\n",
        "> \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IaLiNKmjJDa"
      },
      "source": [
        "!pip install tpot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLSTuPEEZsxE"
      },
      "source": [
        "**Classification using TPOT:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xv2HV0QzgQH"
      },
      "source": [
        "Wine dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm9rV-104ZFa"
      },
      "source": [
        "#TPOT\n",
        "from tpot import TPOTClassifier\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "wine=load_wine()\n",
        "#Perform a train test split.\n",
        "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, train_size=0.75, test_size=0.25)\n",
        "#TPOT classifier\n",
        "tpot=TPOTClassifier(generations=99, population_size=99, mutation_rate=0.7, crossover_rate=0.3, random_state=11, cv=5, subsample=0.98, verbosity=2, n_jobs=-2, max_eval_time_mins=0.00000001, config_dict='TPOT light', memory='áuto', log_file='tpot_digits_logs')\n",
        "tpot.fit(X_train, y_train)\n",
        "print(tpot.score(X_test, y_test))\n",
        "tpot.export('tpot_wine_pipeline.py')\n",
        "\n",
        "#TPOT to only use a PyTorch-based logistic regression classifier as its main estimator is:\n",
        "tpot_config = {\n",
        "    'tpot.nn.PytorchLRClassifier': {\n",
        "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.]\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrAXNfKAzm2t"
      },
      "source": [
        "Digits dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkuN5Fcj8QZa"
      },
      "source": [
        "#TPOT\n",
        "from tpot import TPOTClassifier\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "wine = load_digits()\n",
        "#Perform a train test split.\n",
        "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, train_size=0.75, test_size=0.25)\n",
        "#TPOT classifier\n",
        "tpot=TPOTClassifier(generations=99, population_size=99, mutation_rate=0.7, crossover_rate=0.3, random_state=1110, cv=5, subsample=0.98, verbosity=2, n_jobs=-2, max_eval_time_mins=0.00000001, config_dict='TPOT light', memory='áuto', log_file='tpot_digits_logs')\n",
        "tpot.fit(X_train, y_train)\n",
        "print(tpot.score(X_test, y_test))\n",
        "tpot.export(\"tpot_digits_pipeline.py\")\n",
        "\n",
        "#TPOT to only use a PyTorch-based logistic regression classifier as its main estimator is:\n",
        "tpot_config = {\n",
        "    'tpot.nn.PytorchLRClassifier': {\n",
        "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.]\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNYMwpKlz7tn"
      },
      "source": [
        "Diabetes dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWnDD-cvQF0R"
      },
      "source": [
        "#TPOT\n",
        "from tpot import TPOTClassifier\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "diabetes = load_diabetes()\n",
        "#Perform a train test split.\n",
        "X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, train_size=0.75, test_size=0.25)\n",
        "#TPOT classifier\n",
        "tpot=TPOTClassifier(generations=99, population_size=99, mutation_rate=0.7, crossover_rate=0.3, random_state=131, cv=5, subsample=0.999999999, verbosity=2, n_jobs=-2, max_eval_time_mins=0.00000001, config_dict='TPOT light', memory='áuto', log_file='tpot_diabetes_logs')\n",
        "tpot.fit(X_train, y_train)\n",
        "print(tpot.score(X_test, y_test))\n",
        "tpot.export('tpot_diabetes_pipeline.py')\n",
        "\n",
        "#TPOT to only use a PyTorch-based logistic regression classifier as its main estimator is:\n",
        "tpot_config = {\n",
        "    'tpot.nn.PytorchLRClassifier': {\n",
        "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.]\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUf7VPYB0jBM"
      },
      "source": [
        "Iris dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlBQKlDZ0MLk"
      },
      "source": [
        "#TPOT\n",
        "from tpot import TPOTClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "diabetes = load_iris()\n",
        "#Perform a train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, train_size=0.75, test_size=0.25)\n",
        "#TPOT classifier\n",
        "tpot=TPOTClassifier(generations=99, population_size=99, mutation_rate=0.7, crossover_rate=0.3, random_state=131, cv=5, subsample=0.98, verbosity=2, n_jobs=-2, max_eval_time_mins=0.000001, config_dict='TPOT light', memory='áuto', log_file='tpot_iris_logs')\n",
        "tpot.fit(X_train, y_train)\n",
        "print(tpot.score(X_test, y_test))\n",
        "tpot.export('tpot_iris_pipeline.py')\n",
        "\n",
        "#TPOT to only use a PyTorch-based logistic regression classifier as its main estimator is:\n",
        "tpot_config = {\n",
        "    'tpot.nn.PytorchLRClassifier': {\n",
        "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.]\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLeZZP-ufcHO"
      },
      "source": [
        "\n",
        "\n",
        "Latest India Covid-19 status dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSxQxs7RegMX"
      },
      "source": [
        "#TPOT\n",
        "from tpot import TPOTClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Import\n",
        "import sklearn.model_selection\n",
        "import sklearn.metrics\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import geopandas as gpd\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "#Specify the URL for web-scraping.\n",
        "url = \"https://www.kaggle.com/sudalairajkumar/covid19-in-india\"\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "    stat = extract_contents(row.find_all('td')) \n",
        "#Notice that the data that we require is now a list of length 5.\n",
        "    if len(stat) == 5:\n",
        "        stats.append(stat)\n",
        "#Now convert the data into a pandas dataframe for further processing.\n",
        "new_cols = [\"Sr.No\", \"States/UT\",\"Confirmed\",\"Recovered\",\"Deceased\"]\n",
        "state_data = pd.DataFrame(data = stats, columns = new_cols)\n",
        "state_data.head()\n",
        "#Scraped data columns are of ‘string’ datatype.\n",
        "#Convert them into ‘int' datatype.\n",
        "state_data[\"Confirmed\"] = state_data[\"Confirmed\"].map(int)\n",
        "state_data[\"Recovered\"] = state_data[\"Recovered\"].map(int)\n",
        "state_data[\"Deceased\"] = state_data[\"Deceased\"].map(int)\n",
        "India_covid_status = state_data\n",
        "#Perform a train test split.\n",
        "X_train, X_test, y_train, y_test = train_test_split(India_covid_status.data, India_covid_status.target, train_size=0.75, test_size=0.25)\n",
        "#TPOT classifier\n",
        "tpot=TPOTClassifier(generations=99, population_size=99, mutation_rate=0.7, crossover_rate=0.3, random_state=131, cv=5, subsample=0.98, verbosity=2, n_jobs=-2, max_eval_time_mins=0.00000001, config_dict='TPOT light', memory='áuto', log_file='tpot_India_covid_status_logs')\n",
        "tpot.fit(X_train, y_train)\n",
        "print(tpot.score(X_test, y_test))\n",
        "tpot.export('tpot_India_covid_status_pipeline.py')\n",
        "\n",
        "#TPOT to only use a PyTorch-based logistic regression classifier as its main estimator is:\n",
        "tpot_config = {\n",
        "    'tpot.nn.PytorchLRClassifier': {\n",
        "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.]\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Set962s_xR9D"
      },
      "source": [
        "California housing dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU_4lc8jxDfa"
      },
      "source": [
        "train_data = /content/sample_data/california_housing_train.csv;\n",
        "test_data = /content/sample_data/california_housing_test.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XW55pybyY9F"
      },
      "source": [
        "import pandas as pd\n",
        "cal_house_train_data=pd.read_csv(\"/content/sample_data/california_housing_train.csv\")\n",
        "cal_house_test_data=pd.read_csv(\"/content/sample_data/california_housing_test.csv\")\n",
        "\n",
        "cal_house_data = pd.merge(cal_house_train_data, cal_house_test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVedZEfDxzM0"
      },
      "source": [
        "#Regression\n",
        "from tpot import TPOTRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Perform a train test split.\n",
        "X_train, X_test, y_train, y_test = train_test_split(cal_house_data.data, cal_house_data.target, train_size=0.75, test_size=0.25)\n",
        "tpot_reg=TPOTRegressor(generations=99, population_size=99, mutation_rate=0.75, crossover_rate=0.25, cv=5, subsample=0.95, verbosity=2, n_jobs=-2, scoring='r2', random_state=21, max_eval_time_mins=0.5, config_dict='TPOT light', memory='áuto', log_file='tpot_cal_house_data_logs')\n",
        "tpot_reg.fit(X_train, y_train)\n",
        "print(tpot_reg.score(X_test, y_test))\n",
        "tpot_reg.export('tpot_california_house_prices_pipeline.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgI9fgEGEGol"
      },
      "source": [
        "Latest India Covid-19 statewise status dataset:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHvHbSLbgJ8l"
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import requests \n",
        "from bs4 import BeautifulSoup \n",
        "import geopandas as gpd\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "# Specify the URL for the offical ministry of health website.\n",
        "url = \"https://www.mygov.in/corona-data/covid19-statewise-status/\" \n",
        "\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace(\"\\n\", \"\") for x in row] \n",
        "\n",
        "stats = [] # Initialize stats.\n",
        "all_rows = soup.find_all(\"tr\") # Find all the table rows.\n",
        "\n",
        "for row in all_rows: \n",
        "    stat = extract_contents(row.find_all(\"td\")) # Find all data cells.\n",
        "    # Notice that the data that we require is now a list of length 5.\n",
        "    if len(stat) == 5: \n",
        "        stats.append(stat)\n",
        "\n",
        "#Now convert the data into a pandas dataframe for further processing.\n",
        "new_cols = [\"Sr.No\", \"States/UT\",\"Confirmed\",\"Recovered\",\"Deceased\"]\n",
        "state_data = pd.DataFrame(data = stats, columns = new_cols)\n",
        "\n",
        "#Converting the 'string' data to 'int'.\n",
        "state_data[\"Confirmed\"] = state_data[\"Confirmed\"].map(int)\n",
        "state_data[\"Recovered\"] = state_data[\"Recovered\"].map(int)\n",
        "state_data[\"Deceased\"]  = state_data[\"Deceased\"].map(int)\n",
        "\n",
        "# Pretty table representation\n",
        "table = PrettyTable()\n",
        "table.field_names = (new_cols)\n",
        "for i in stats:\n",
        "    table.add_row(i)\n",
        "table.add_row([\"\",\"Total\", \n",
        "               sum(state_data[\"Confirmed\"]), \n",
        "               sum(state_data[\"Recovered\"]), \n",
        "               sum(state_data[\"Deceased\"])])\n",
        "print(table)\n",
        "\n",
        "#Use barplot to show total confirmed cases Statewise. \n",
        "sns.set_style(\"ticks\")\n",
        "plt.figure(figsize = (15,10))\n",
        "plt.barh(state_data[\"States/UT\"], state_data[\"Confirmed\"].map(int),\n",
        "         align = \"center\", color = \"lightblue\", edgecolor = \"blue\")\n",
        "plt.xlabel(\"No. of Confirmed cases\", fontsize = 18)\n",
        "plt.ylabel(\"States/UT\", fontsize = 18)\n",
        "plt.gca().invert_yaxis() # This is to maintain the order in which the states appear.\n",
        "plt.xticks(fontsize = 14) \n",
        "plt.yticks(fontsize = 14)\n",
        "plt.title(\"Total Confirmed Cases Statewise\", fontsize = 20)\n",
        "\n",
        "for index, value in enumerate(state_data[\"Confirmed\"]):\n",
        "    plt.text(value, index, str(value), fontsize = 12, verticalalignment = \"center\")\n",
        "plt.show()  \n",
        "\n",
        "#Use donut chart representing nationwide total confirmed, cured and deceased cases.\n",
        "group_size = [sum(state_data[\"Confirmed\"]), \n",
        "              sum(state_data[\"Recovered\"]), \n",
        "              sum(state_data[\"Deceased\"])]\n",
        "\n",
        "group_labels = [\"Confirmed\\n\" + str(sum(state_data[\"Confirmed\"])), \n",
        "                \"Recovered\\n\" + str(sum(state_data[\"Recovered\"])), \n",
        "                \"Deceased\\n\"  + str(sum(state_data[\"Deceased\"]))]\n",
        "custom_colors = [\"skyblue\", \"yellowgreen\", \"tomato\"]\n",
        "\n",
        "plt.figure(figsize = (5,5))\n",
        "plt.pie(group_size, labels = group_labels, colors = custom_colors)\n",
        "central_circle = plt.Circle((0,0), 0.5, color = \"white\")\n",
        "fig = plt.gcf()\n",
        "fig.gca().add_artist(central_circle)\n",
        "plt.rc(\"font\", size = 12) \n",
        "plt.title(\"Nationwide total Confirmed, Recovered and Deceased Cases\", fontsize = 16)\n",
        "plt.show()\n",
        "\n",
        "# Read the state wise shapefile of India in a GeoDataFrame and preview it.\n",
        "map_data = gpd.read_file(\"Indian_States.shp\")\n",
        "map_data.rename(columns = {\"st_nm\":\"States/UT\"}, inplace = True)\n",
        "map_data.head()\n",
        "\n",
        "# Correct the name of states in the map dataframe. \n",
        "map_data[\"States/UT\"] = map_data[\"States/UT\"].str.replace('&', 'and')\n",
        "map_data[\"States/UT\"].replace(\"Arunanchal Pradesh\", \"Arunachal Pradesh\", inplace = True)\n",
        "map_data[\"States/UT\"].replace(\"Telangana\", \"Telengana\", inplace = True)\n",
        "map_data[\"States/UT\"].replace(\"NCT of Delhi\", \"Delhi\", inplace = True)\n",
        "\n",
        "# Merge both the dataframes - state_data and map_data.\n",
        "merged_data = pd.merge(map_data, state_data, how = \"left\", on = \"States/UT\")\n",
        "merged_data.fillna(0, inplace = True)\n",
        "merged_data.drop(\"Sr.No\", axis = 1, inplace = True)\n",
        "merged_data.head()\n",
        "\n",
        "# Create figure and axes for Matplotlib and set the title.\n",
        "fig, ax = plt.subplots(1, figsize=(20, 12))\n",
        "ax.axis('off')\n",
        "ax.set_title('Covid-19 Statewise Data - Confirmed Cases', fontdict = {'fontsize': '25', 'fontweight' : '3'})\n",
        "# Plot the figure.\n",
        "merged_data.plot(column = 'Confirmed', cmap='YlOrRd', linewidth=0.8, ax=ax, edgecolor='0.8', legend = True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAdt8pamLOBk"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from tpot.export_utils import set_param_recursive\n",
        "#TPOT\n",
        "from tpot import TPOTClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Perform the data and metric imports\n",
        "import sklearn.model_selection\n",
        "import sklearn.metrics\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import geopandas as gpd\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "#Specify the URL for Web Scraping.\n",
        "url = \"https://www.mygov.in/corona-data/covid19-statewise-status/\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "    stat = extract_contents(row.find_all('td')) \n",
        "# Notice that the data that we require is now a list of length 5.\n",
        "    if len(stat) == 5:\n",
        "        stats.append(stat)\n",
        "#Now convert the data into a pandas dataframe for further processing.\n",
        "new_cols = [\"Sr.No\", \"States/UT\",\"Confirmed\",\"Recovered\",\"Deceased\"]\n",
        "state_data = pd.DataFrame(data = stats, columns = new_cols)\n",
        "state_data.head()\n",
        "#Scraped data columns are of ‘string’ datatype.\n",
        "#Convert them into ‘int' datatype.\n",
        "state_data[\"Confirmed\"] = state_data[\"Confirmed\"].map(int)\n",
        "state_data[\"Recovered\"] = state_data[\"Recovered\"].map(int)\n",
        "state_data[\"Deceased\"] = state_data[\"Deceased\"].map(int)\n",
        "\n",
        "import csv\n",
        "data = state_data\n",
        "data = csv.reader(data)  \n",
        "print(data)\n",
        "# NOTE: Make sure that the outcome column is labeled 'target' in the data file.\n",
        "tpot_data = pd.read_csv(data, sep=',', dtype=np.float64)\n",
        "features = tpot_data.drop('target', axis=1)\n",
        "training_features, testing_features, training_target, testing_target = \\\n",
        "            train_test_split(features, tpot_data['target'], random_state=42)\n",
        "\n",
        "# Average CV score on the training set was: 0.9826086956521738\n",
        "exported_pipeline = make_pipeline(\n",
        "    Normalizer(norm=\"l2\"),\n",
        "    KNeighborsClassifier(n_neighbors=5, p=2, weights=\"distance\")\n",
        ")\n",
        "# Fix random state for all the steps in exported pipeline.\n",
        "set_param_recursive(exported_pipeline.steps, 'random_state', 42)\n",
        "\n",
        "exported_pipeline.fit(training_features, training_target)\n",
        "results = exported_pipeline.predict(testing_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9oS84ayLZES"
      },
      "source": [
        "**TPOT Regressor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fo8Zbc_oE20r"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from tpot import TPOTRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from tpot.export_utils import set_param_recursive\n",
        "#TPOT\n",
        "from tpot import TPOTRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "# data and metric imports\n",
        "import sklearn.model_selection\n",
        "import sklearn.metrics\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import geopandas as gpd\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "#Specify the URL for Web Scraping.\n",
        "url = \"https://www.mygov.in/corona-data/covid19-statewise-status/\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "    stat = extract_contents(row.find_all('td')) \n",
        "# Notice that the data that we require is now a list of length 5.\n",
        "    if len(stat) == 5:\n",
        "        stats.append(stat)\n",
        "#Now convert the data into a pandas dataframe for further processing.\n",
        "new_cols = [\"Sr.No\", \"States/UT\",\"Confirmed\",\"Recovered\",\"Deceased\"]\n",
        "state_data = pd.DataFrame(data = stats, columns = new_cols)\n",
        "state_data.head()\n",
        "#Scraped data columns are of ‘string’ datatype.\n",
        "#Convert them into ‘int' datatype.\n",
        "state_data[\"Confirmed\"] = state_data[\"Confirmed\"].map(int)\n",
        "state_data[\"Recovered\"] = state_data[\"Recovered\"].map(int)\n",
        "state_data[\"Deceased\"] = state_data[\"Deceased\"].map(int)\n",
        "#Perform a train test split.\n",
        "X_train, X_test, y_train, y_test = train_test_split(state_data.data, state_data.target, train_size=0.75, test_size=0.25)\n",
        "tpot_reg=TPOTRegressor(generations=99, population_size=99, mutation_rate=0.75, crossover_rate=0.25, cv=7, subsample=0.95, verbosity=2, n_jobs=-2, scoring='r2', random_state=21, max_eval_time_mins=0.5, config_dict='TPOT light', memory='áuto', log_file='tpot_state_data_logs')\n",
        "tpot_reg.fit(X_train, y_train)\n",
        "print(tpot_reg.score(X_test, y_test))\n",
        "tpot_reg.export(\"tpot_state_data_pipeline.py\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dro038ri-Cri"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from tpot import TPOTRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "house_data = load_boston()\n",
        "#Perform a train test split.\n",
        "X_train, X_test, y_train, y_test = train_test_split(house_data.data, house_data.target, train_size=0.75, test_size=0.25)\n",
        "tpot_reg=TPOTRegressor(generations=99, population_size=99, mutation_rate=0.75, crossover_rate=0.25, cv=5, subsample=0.95, verbosity=2, n_jobs=-2, scoring='r2', random_state=21, max_eval_time_mins=0.5, config_dict='TPOT light', memory='áuto', log_file='tpot_boston_data_logs')\n",
        "tpot_reg.fit(X_train, y_train)\n",
        "print(tpot_reg.score(X_test, y_test))\n",
        "tpot_reg.export('tpot_reg_house_prices_pipeline.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C6h5Ce86kLo"
      },
      "source": [
        "**Neural network classifier using TPOT-NN:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-BrkvyMO3TP"
      },
      "source": [
        "#Perform weather forecast using H2O.\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "url = \"https://www.climate.gov/maps-data/datasets\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  # Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    weather_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = weather_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1313)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=100,\n",
        "                                    max_depth=4,\n",
        "                                    learn_rate=0.1)\n",
        "    #Specify the predictor set and response.\n",
        "    x = list(weather_data.columns)\n",
        "    x\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)\n",
        "    #TPOT-NN\n",
        "    from tpot import TPOTClassifier\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)\n",
        "\n",
        "    clf = TPOTClassifier(config_dict='TPOT NN', template='Selector-Transformer-PytorchLRClassifier',\n",
        "                        verbosity=2, population_size=10, generations=10)\n",
        "    clf.fit(X_train, y_train)\n",
        "    print(clf.score(X_test, y_test))\n",
        "    clf.export('tpot_nn_weather_data_pipeline.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp6ve4m3BP0r"
      },
      "source": [
        "#RSNA dataset\n",
        "#Perform weather forecast using H2O.\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "#Specify the URL.\n",
        "url = \"https://www.kaggle.com/c/rsna-miccai-brain-tumor-radiogenomic-classification/data\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  # Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    rsna_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = rsna_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=01230)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=100,\n",
        "                                    max_depth=4,\n",
        "                                    learn_rate=0.1)\n",
        "    \n",
        "    #Specify the predictor set and response.\n",
        "    x = list(weather_data.columns)\n",
        "    x\n",
        "\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)\n",
        "\n",
        "    #TPOT-NN\n",
        "    from tpot import TPOTClassifier\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3)\n",
        "\n",
        "    clf = TPOTClassifier(config_dict='TPOT NN', template='Selector-Transformer-PytorchLRClassifier',\n",
        "                        verbosity=2, population_size=100, generations=100)\n",
        "    clf.fit(X_train, y_train)\n",
        "    print(clf.score(X_test, y_test))\n",
        "    clf.export('tpot_nn_rsna_data_pipeline.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fnft2FLoQjg2"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "url = \"https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/data\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  # Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    chaii_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = chaii_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=13131)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=1000,\n",
        "                                    max_depth=4,\n",
        "                                    learn_rate=0.1)\n",
        "    #Specify the predictor set and response.\n",
        "    x = list(chaii_data.columns)\n",
        "    x\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)\n",
        "    #TPOT-NN\n",
        "    from tpot import TPOTClassifier\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)\n",
        "\n",
        "    clf = TPOTClassifier(config_dict='TPOT NN', template='Selector-Transformer-PytorchLRClassifier',\n",
        "                        verbosity=2, population_size=10, generations=10)\n",
        "    clf.fit(X_train, y_train)\n",
        "    print(clf.score(X_test, y_test))\n",
        "    clf.export('tpot_nn_weather_data_pipeline.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYpyPTRtmn3r"
      },
      "source": [
        "Music dataset using TPOT-NN:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP8WrP1bmnO_"
      },
      "source": [
        "#Musicnet dataset\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "url = \"https://www.kaggle.com/imsparsh/musicnet-dataset?select=musicnet_metadata.csv\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  # Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    music_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = music_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1313)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=100,\n",
        "                                    max_depth=4,\n",
        "                                    learn_rate=0.1)\n",
        "    #Specify the predictor set and response.\n",
        "    x = list(music_data.columns)\n",
        "    x\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)\n",
        "    #TPOT-NN\n",
        "    from tpot import TPOTClassifier\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)\n",
        "\n",
        "    clf = TPOTClassifier(config_dict='TPOT NN', template='Selector-Transformer-PytorchLRClassifier',\n",
        "                        verbosity=2, population_size=10, generations=10)\n",
        "    clf.fit(X_train, y_train)\n",
        "    print(clf.score(X_test, y_test))\n",
        "    clf.export('tpot_nn_music_data_pipeline.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CcaiEGUfKzR"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "url = \"http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets/\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  # Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    autoweka_datasets[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = autoweka_datasets\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1313)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=100,\n",
        "                                    max_depth=4,\n",
        "                                    learn_rate=0.1)\n",
        "    \n",
        "    #Specify the predictor set and response.\n",
        "    x = list(autoweka_datasets.columns)\n",
        "    x\n",
        "\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)\n",
        "\n",
        "    #TPOT-NN\n",
        "    from tpot import TPOTClassifier\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)\n",
        "\n",
        "    clf = TPOTClassifier(config_dict='TPOT NN', template='Selector-Transformer-PytorchLRClassifier',\n",
        "                        verbosity=2, population_size=10, generations=10)\n",
        "    clf.fit(X_train, y_train)\n",
        "    print(clf.score(X_test, y_test))\n",
        "    clf.export('tpot_nn_autoweka_datasets_pipeline.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HivCgAdUDajz"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "url = \"https://www.climate.gov/maps-data/datasets\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  # Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    covid19_india_data[each_new_col] = covid19_india_data[each_new_col].map(int)\n",
        "    X, y = covid19_india_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1313)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=100,\n",
        "                                    max_depth=4,\n",
        "                                    learn_rate=0.1)\n",
        "    \n",
        "    #Specify the predictor set and response.\n",
        "    x = list(covid19_india_data.columns)\n",
        "    x\n",
        "\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)\n",
        "    \n",
        "    #TPOT-NN\n",
        "    from tpot import TPOTClassifier\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3)\n",
        "\n",
        "    clf = TPOTClassifier(config_dict='TPOT NN', template='Selector-Transformer-PytorchLRClassifier',\n",
        "                        verbosity=2, population_size=10, generations=10)\n",
        "    clf.fit(X_train, y_train)\n",
        "    print(clf.score(X_test, y_test))\n",
        "    clf.export('tpot_nn_covid19_india_data_pipeline.py')\n",
        "    fig, ax = plt.subplots(1, figsize=(20, 12))\n",
        "    ax.axis(‘off’)\n",
        "    ax.set_title(‘Covid-19 Statewise Data — Confirmed Cases’, \n",
        "                fontdict =  {‘fontsize’: ‘25’, ‘fontweight’ : ‘3’})\n",
        "    merged_data.plot(column = ‘Confirmed’, cmap=’YlOrRd’, \n",
        "                    linewidth=0.8, ax=ax, edgecolor=’0.8', \n",
        "                    legend = True)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCCEReAyNO6a"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "url = \"https://docs.gitlab.com/ee/development/value_stream_analytics.html#data-collector\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  # Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    covid19_india_data[each_new_col] = covid19_india_data[each_new_col].map(int)\n",
        "    X, y = covid19_india_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1313)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=100,\n",
        "                                    max_depth=4,\n",
        "                                    learn_rate=0.1)\n",
        "    #Specify the predictor set and response.\n",
        "    x = list(covid19_india_data.columns)\n",
        "    x\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)\n",
        "    perf = model.model_performance(test)\n",
        "    print(perf.__class__)\n",
        "    #Area Under the ROC Curve (AUC)\n",
        "    perf.auc()\n",
        "    perf.mse()\n",
        "    #Cross-validated Performance\n",
        "    cvmodel = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                       ntrees=1000,\n",
        "                                       max_depth=4,\n",
        "                                       learn_rate=0.1,\n",
        "                                       nfolds=5)\n",
        "    cvmodel.train(x=x, y=y, training_frame=data)\n",
        "    print(cvmodel.auc(train=True))\n",
        "    print(cvmodel.auc(xval=True))\n",
        "    #Grid Search\n",
        "    #ntrees: Number of trees\n",
        "    #max_depth: Maximum depth of a tree\n",
        "    #learn_rate: Learning rate in the GBM\n",
        "    ntrees_opt = [5,50,100]\n",
        "    max_depth_opt = [2,3,5]\n",
        "    learn_rate_opt = [0.1,0.2]\n",
        "    hyper_params = {'ntrees': ntrees_opt, \n",
        "                    'max_depth': max_depth_opt,\n",
        "                    'learn_rate': learn_rate_opt}\n",
        "\n",
        "    #Define an \"H2OGridSearch\" object by specifying the algorithm (GBM) and the hyper parameters.\n",
        "    from h2o.grid.grid_search import H2OGridSearch\n",
        "    gs = H2OGridSearch(H2OGradientBoostingEstimator, hyper_params = hyper_params)\n",
        "    gs.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(gs)\n",
        "\n",
        "    # Print out the AUC for all of the models.\n",
        "    auc_table = gs.sort_by('auc(valid=True)',increasing=False)\n",
        "    print(auc_table)\n",
        "    #Get the best model in terms of AUC.\n",
        "    best_model = h2o.get_model(auc_table['Model Id'][0])\n",
        "    best_model.auc() \n",
        "    #Generate predictions on the test set using the \"best\" model, and evaluate the test set AUC.\n",
        "    best_perf = best_model.model_performance(test)\n",
        "    best_perf.auc()  \n",
        "\n",
        "    #TPOT-NN\n",
        "    from tpot import TPOTClassifier\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3)\n",
        "\n",
        "    clf = TPOTClassifier(config_dict='TPOT NN', template='Selector-Transformer-PytorchLRClassifier',\n",
        "                        verbosity=2, population_size=10, generations=10)\n",
        "    clf.fit(X_train, y_train)\n",
        "    print(clf.score(X_test, y_test))\n",
        "    clf.export('tpot_nn_data_pipeline.py')\n",
        "    fig, ax = plt.subplots(1, figsize=(20, 12))\n",
        "    ax.axis(‘off’)\n",
        "    ax.set_title(‘Covid-19 Statewise Data — Confirmed Cases’, \n",
        "                fontdict =  {‘fontsize’: ‘25’, ‘fontweight’ : ‘3’})\n",
        "    merged_data.plot(column = ‘Confirmed’, cmap=’YlOrRd’, \n",
        "                    linewidth=0.8, ax=ax, edgecolor=’0.8', \n",
        "                    legend = True)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYwjNEhD4dTC"
      },
      "source": [
        "#Kaggle dataset\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "url = \"https://www.kaggle.com/kiva/data-science-for-good-kiva-crowdfunding\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  # Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    kaggle_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = kaggle_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=131311)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=100,\n",
        "                                    max_depth=4,\n",
        "                                    learn_rate=0.1)\n",
        "    \n",
        "    #Specify the predictor set and response.\n",
        "    x = list(kaggle_data.columns)\n",
        "    x\n",
        "\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)\n",
        "    \n",
        "    #TPOT-NN\n",
        "    from tpot import TPOTClassifier\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)\n",
        "\n",
        "    clf = TPOTClassifier(config_dict='TPOT NN', template='Selector-Transformer-PytorchLRClassifier',\n",
        "                        verbosity=2, population_size=100, generations=100)\n",
        "    clf.fit(X_train, y_train)\n",
        "    print(clf.score(X_test, y_test))\n",
        "    clf.export('tpot_nn_kaggle_dataset_pipeline.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke-mcGrZ6q6F"
      },
      "source": [
        "from tpot import TPOTClassifier\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = make_blobs(n_samples=100, centers=2, n_features=3, random_state=424)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)\n",
        "\n",
        "clf = TPOTClassifier(config_dict='TPOT NN', template='Selector-Transformer-PytorchLRClassifier',\n",
        "                     verbosity=2, population_size=10, generations=10)\n",
        "clf.fit(X_train, y_train)\n",
        "print(clf.score(X_test, y_test))\n",
        "clf.export('tpot_nn_data_pipeline.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY2Oll9CVZdM"
      },
      "source": [
        "**NOTE:** Turns out TPOT cannot solve multi label regression problems at this time as below;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-L7uiRHGZPU"
      },
      "source": [
        "#Latest India Covid-19 statewise status\n",
        "from tpot import TPOTClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from tpot.export_utils import set_param_recursive\n",
        "#TPOT\n",
        "from tpot import TPOTClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Perform the data and metric imports.\n",
        "import sklearn.model_selection\n",
        "import sklearn.metrics\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import geopandas as gpd\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "#Specify the URL for the Web Scraping.\n",
        "url = \"https://www.mygov.in/corona-data/covid19-statewise-status/\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "    stat = extract_contents(row.find_all('td')) \n",
        "# Notice that the data that we require is now a list of length 5.\n",
        "    if len(stat) == 5:\n",
        "        stats.append(stat)\n",
        "\n",
        "#now convert the data into a pandas dataframe for further processing\n",
        "new_cols = [\"Sr.No\", \"States/UT\",\"Confirmed\",\"Recovered\",\"Deceased\"]\n",
        "state_data = pd.DataFrame(data = stats, columns = new_cols)\n",
        "state_data.head()\n",
        "\n",
        "#Scraped data columns are of ‘string’ datatype.\n",
        "#Convert them into ‘int' datatype.\n",
        "state_data[\"Confirmed\"] = state_data[\"Confirmed\"].map(int)\n",
        "state_data[\"Recovered\"] = state_data[\"Recovered\"].map(int)\n",
        "state_data[\"Deceased\"] = state_data[\"Deceased\"].map(int)\n",
        "\n",
        "def getNumbers():\n",
        "    return 'one', 'two'\n",
        "one, two = getNumbers()\n",
        "\n",
        "X, y = getNumbers()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)\n",
        "#TPOT-NN\n",
        "clf = TPOTClassifier(config_dict='TPOT NN', template='Selector-Transformer-PytorchLRClassifier',\n",
        "                     verbosity=2, population_size=10, generations=10)\n",
        "\n",
        "#NOTE: Turns out TPOT cannot solve multi label regression problems at this time\n",
        "'''clf.fit(X_train, y_train)\n",
        "print(clf.score(X_test, y_test))\n",
        "clf.export('tpot_nn_state_data_pipeline.py')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnlw3IMO5H2-"
      },
      "source": [
        "#Import scikit-learn.\n",
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an_j8xecqKV9"
      },
      "source": [
        "**Auto-Sklearn**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3ICgx56qGuW"
      },
      "source": [
        "arXiv:2007.04074v2 [cs.LG]\n",
        "\n",
        "arXiv:2007.04074 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH0EPLazrNJ1"
      },
      "source": [
        "@article{ASKL2,\n",
        "   title = {Auto-Sklearn 2.0},\n",
        "   author = {Feurer, Matthias and Eggensperger, Katharina and\n",
        "             Falkner, Stefan and Lindauer, Marius and Hutter, Frank},\n",
        "   booktitle = {Advances in Neural Information Processing Systems 28},\n",
        "   year = {2020},\n",
        "   journal = {arXiv:2007.04074 [cs.LG]},\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEs0CID5raUs"
      },
      "source": [
        "**Install Auto-Sklearn**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ublFe3RO8Czz"
      },
      "source": [
        "!python3 -m pip install --upgrade pip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soMMfoYqqEyt"
      },
      "source": [
        "!pip3 install --upgrade pandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNDti9ukq86I"
      },
      "source": [
        "!pip3 install auto-sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UqEQ4iH6Uv-"
      },
      "source": [
        "!pip3 install --upgrade scipy\n",
        "!pip3 install --upgrade auto-sklearn\n",
        "!pip install auto-sklearn==0.10.0\n",
        "!pip install --upgrade pip\n",
        "!sudo apt-get install build-essential swig \n",
        "!curl https://raw.githubusercontent.com/automl/auto-sklearn/master/requirements.txt | xargs -n 1 -L 1 pip install \n",
        "!pip install auto-sklearn==0.10.0\n",
        "!pip install matplotlib-venn\n",
        "!apt-get -qq install -y libfluidsynth1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XQy4lG_EFru"
      },
      "source": [
        "Install 7zip reader libarchive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R9pjAB_EFEy"
      },
      "source": [
        "!apt-get -qq install -y libarchive-dev && pip install -U libarchive\n",
        "import libarchive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ1v1osNrwXM"
      },
      "source": [
        "import autosklearn.classification\n",
        "cls = autosklearn.classification.AutoSklearnClassifier()\n",
        "import sklearn.model_selection\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "X, y = sklearn.datasets.load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "        sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
        "\n",
        "cls.fit(X_train, y_train)\n",
        "predictions = cls.predict(X_test)\n",
        "\n",
        "import sklearn.model_selection\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "if __name__ == \"__main__\":\n",
        "    X, y = sklearn.datasets.load_iris(return_X_y=True)\n",
        "    X_train, X_test, y_train, y_test = \\\n",
        "            sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
        "    automl = autosklearn.classification.AutoSklearnClassifier()\n",
        "    #Fit.\n",
        "    automl.fit(X_train, y_train)\n",
        "    #Predict.\n",
        "    y_hat = automl.predict(X_test)\n",
        "    #Print the accuracy score.\n",
        "    print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_hat))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5YgOYMyJVSc"
      },
      "source": [
        "import autosklearn.classification\n",
        "cls = autosklearn.classification.AutoSklearnClassifier()\n",
        "import sklearn.model_selection\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "X, y = sklearn.datasets.load_digits(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "        sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
        "\n",
        "cls.fit(X_train, y_train)\n",
        "predictions = cls.predict(X_test)\n",
        "\n",
        "import sklearn.model_selection\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "if __name__ == \"__main__\":\n",
        "    X, y = sklearn.datasets.load_digits(return_X_y=True)\n",
        "    X_train, X_test, y_train, y_test = \\\n",
        "            sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
        "    automl = autosklearn.classification.AutoSklearnClassifier()\n",
        "    #Fit.\n",
        "    automl.fit(X_train, y_train)\n",
        "    #Predict.\n",
        "    y_hat = automl.predict(X_test)\n",
        "    #Print the accuracy score.\n",
        "    print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_hat))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjD7mEOpOui3"
      },
      "source": [
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(\"https://www.kaggle.com/kiva/data-science-for-good-kiva-crowdfunding\").content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "    stat = extract_contents(row.find_all('td')) \n",
        "\n",
        "    # Notice that the data that we require is now a list of length 5.\n",
        "    if len(stat) == 5:\n",
        "        stats.append(stat)\n",
        "    #Now convert the data into a pandas dataframe for further processing.\n",
        "    new_cols=[]\n",
        "    ozone_data = pd.DataFrame(data = stats, columns = new_cols)\n",
        "    ozone_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype.\n",
        "    #Convert them into ‘int' datatype.\n",
        "    ozone_data[new_cols] = ozone_data[new_cols].map(int)\n",
        "    X, y = ozone_data\n",
        "    X_train, X_test, y_train, y_test = \\\n",
        "        sklearn.model_selection.train_test_split(X, y, random_state=110011)\n",
        "\n",
        "    #Auto-PyTorch\n",
        "    autoPyTorch = AutoNetClassification(\"tiny_cs\",  # config preset\n",
        "                                    log_level='info',\n",
        "                                    max_runtime=999999999**10000000,\n",
        "                                    min_budget=30,\n",
        "                                    max_budget=999999999*100000)\n",
        "    \n",
        "    autoPyTorch.fit(X_train, y_train, validation_split=0.3)\n",
        "    y_pred = autoPyTorch.predict(X_test)\n",
        "    print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred))\n",
        "    #Fit.\n",
        "    clf = KNeighborsClassifier(n_neighbors=1).fit(X_train, y_train)\n",
        "    #Predict.\n",
        "    y_pred = clf.predict(X_test)\n",
        "    #Print the confusion matrix and classification report.\n",
        "    print(metrics.confusion_matrix(y_test, y_pred))\n",
        "    print(metrics.classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za7WXoJC3GGv"
      },
      "source": [
        "**ConfigSpace**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTdAFe3oqrxX"
      },
      "source": [
        "@article{\n",
        "    title   = {BOAH: A Tool Suite for Multi-Fidelity Bayesian Optimization & Analysis of Hyperparameters},\n",
        "    author  = {M. Lindauer and K. Eggensperger and M. Feurer and A. Biedenkapp and J. Marben and P. Müller and F. Hutter},\n",
        "    journal = {arXiv:1908.06756 {[cs.LG]}},\n",
        "    date    = {2019},\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIjx3N083ZKY"
      },
      "source": [
        "**Install ConfigSpace:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwwP_p9Z3XFw"
      },
      "source": [
        "!pip install ConfigSpace"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMiBntXc32IQ"
      },
      "source": [
        "import ConfigSpace as CS\n",
        "cs = CS.ConfigurationSpace(seed=1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23dfBRHc4LOT"
      },
      "source": [
        "#choose hyperparameter alpha\n",
        "import ConfigSpace.hyperparameters as CSH\n",
        "alpha = CSH.UniformFloatHyperparameter(name='alpha', lower=0, upper=1)\n",
        "#create a ConfigurationSpace object\n",
        "import ConfigSpace as CS\n",
        "import ConfigSpace.hyperparameters as CSH\n",
        "cs = CS.ConfigurationSpace(seed=1234)\n",
        "\n",
        "a = CSH.UniformIntegerHyperparameter('a', lower=10, upper=100, log=False)\n",
        "b = CSH.CategoricalHyperparameter('b', choices=['red', 'green', 'blue'])\n",
        "cs.add_hyperparameters([a, b])\n",
        "cs.sample_configuration()\n",
        "#Add ordinal hyper-parameter.\n",
        "ord_hp = CSH.OrdinalHyperparameter('ordinal_hp', sequence=['10', '20', '30'])\n",
        "cs.add_hyperparameter(ord_hp)\n",
        "#Sample a configuration from the ConfigurationSpace object.\n",
        "cs.sample_configuration()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sVPgiNJuXdX"
      },
      "source": [
        "**Install Auto-PyTorch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgf_wv36-BIT"
      },
      "source": [
        "!pip install autopytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo-Zy8_ncntw"
      },
      "source": [
        "from autoPyTorch import AutoNetClassification\n",
        "#Perform the data and metric imports\n",
        "import sklearn.model_selection\n",
        "import sklearn.metrics\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import geopandas as gpd\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "#Specify the URL for Web Scraping.\n",
        "url = \"https://www.kaggle.com/sudalairajkumar/covid19-in-india\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "    stat = extract_contents(row.find_all('td'))\n",
        "\n",
        "# Notice that the data that we require is now a list of length 5.\n",
        "    if len(stat) == 5:\n",
        "        stats.append(stat)\n",
        "#Now convert the data into a pandas dataframe for further processing.\n",
        "new_cols = [\"Sr.No\", \"States/UT\",\"Confirmed\",\"Recovered\",\"Deceased\"]\n",
        "state_data = pd.DataFrame(data = stats, columns = new_cols)\n",
        "state_data.head()\n",
        "\n",
        "#Scraped data columns are of ‘string’ datatype.\n",
        "#Convert them into ‘int' datatype.\n",
        "state_data[\"Confirmed\"] = state_data[\"Confirmed\"].map(int)\n",
        "state_data[\"Recovered\"] = state_data[\"Recovered\"].map(int)\n",
        "state_data[\"Deceased\"] = state_data[\"Deceased\"].map(int)\n",
        "\n",
        "X, y = state_data\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "        sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
        "\n",
        "#Auto-PyTorch\n",
        "autoPyTorch = AutoNetClassification(\"tiny_cs\",  # config preset\n",
        "                                    log_level='info',\n",
        "                                    max_runtime=999999999**10000000,\n",
        "                                    min_budget=30,\n",
        "                                    max_budget=999999999*100000)\n",
        "#Fit.\n",
        "autoPyTorch.fit(X_train, y_train, validation_split=0.3)\n",
        "#Predict.\n",
        "y_pred = autoPyTorch.predict(X_test)\n",
        "#Print the accuracy score.\n",
        "print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7_BlrG-e-Qi"
      },
      "source": [
        "from autoPyTorch import AutoNetClassification\n",
        "\n",
        "# Perform the data and metric imports\n",
        "import sklearn.model_selection\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "X, y = sklearn.datasets.load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "        sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
        "\n",
        "#Auto-PyTorch\n",
        "autoPyTorch = AutoNetClassification(\"tiny_cs\",  # config preset\n",
        "                                    log_level='info',\n",
        "                                    max_runtime=999999999**10000000,\n",
        "                                    min_budget=30,\n",
        "                                    max_budget=999999999*100000)\n",
        "#Fit.\n",
        "autoPyTorch.fit(X_train, y_train, validation_split=0.3)\n",
        "#Predict.\n",
        "y_pred = autoPyTorch.predict(X_test)\n",
        "#Print the accuracy score.\n",
        "print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LazAKGwKtblC"
      },
      "source": [
        "from autoPyTorch import AutoNetClassification\n",
        "\n",
        "# Perform the data and metric imports\n",
        "import sklearn.model_selection\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "X, y = sklearn.datasets.load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "        sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
        "\n",
        "#Auto-PyTorch\n",
        "autoPyTorch = AutoNetClassification(\"tiny_cs\",  # Config preset\n",
        "                                    log_level='info',\n",
        "                                    max_runtime=999999999**10000000,\n",
        "                                    min_budget=30,\n",
        "                                    max_budget=999999999*100000)\n",
        "#Fit.\n",
        "autoPyTorch.fit(X_train, y_train, validation_split=0.3)\n",
        "#Predict.\n",
        "y_pred = autoPyTorch.predict(X_test)\n",
        "#Print the accuracy score.\n",
        "print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLECuGvLta1a"
      },
      "source": [
        "from autoPyTorch import AutoNetClassification\n",
        "# Perform the data and metric imports\n",
        "import sklearn.model_selection\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "X, y = sklearn.datasets.load_linnerud(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "        sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
        "\n",
        "#Auto-PyTorch\n",
        "autoPyTorch = AutoNetClassification(\"tiny_cs\",  # config preset\n",
        "                                    log_level='info',\n",
        "                                    max_runtime=999999999**10000000,\n",
        "                                    min_budget=30,\n",
        "                                    max_budget=999999999*100000)\n",
        "#Fit.\n",
        "autoPyTorch.fit(X_train, y_train, validation_split=0.3)\n",
        "#Predict.\n",
        "y_pred = autoPyTorch.predict(X_test)\n",
        "#Print the accuracy score.\n",
        "print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ot4uPRLillj4"
      },
      "source": [
        "# Perform the data and metric imports.\n",
        "import sklearn.model_selection\n",
        "import sklearn.metrics\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import geopandas as gpd\n",
        "from prettytable import PrettyTable\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#Specify the URL.\n",
        "url = \"https://www.kaggle.com/brsdincer/ozone-tendency-new-data-20182021-nasa\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "\n",
        "for row in all_rows:\n",
        "    stat = extract_contents(row.find_all('td')) \n",
        "    # Notice that the data that we require is now a list of length 5.\n",
        "    if len(stat) == 5:\n",
        "        stats.append(stat)\n",
        "    #Now convert the data into a pandas dataframe for further processing.\n",
        "    new_cols=[]\n",
        "    ozone_data = pd.DataFrame(data = stats, columns = new_cols)\n",
        "    ozone_data.head()\n",
        "\n",
        "    #Scraped data columns are of ‘string’ datatype.\n",
        "    #Convert them into ‘int' datatype.\n",
        "    ozone_data[new_cols] = ozone_data[new_cols].map(int)\n",
        "    X, y = ozone_data\n",
        "    X_train, X_test, y_train, y_test = \\\n",
        "        sklearn.model_selection.train_test_split(X, y, random_state=110011)\n",
        "\n",
        "    #Auto-PyTorch\n",
        "    autoPyTorch = AutoNetClassification(\"tiny_cs\",  # config preset\n",
        "                                    log_level='info',\n",
        "                                    max_runtime=999999999**10000000,\n",
        "                                    min_budget=30,\n",
        "                                    max_budget=999999999*100000)\n",
        "    \n",
        "    #Fit. Predict. Get the accuracy score.\n",
        "    autoPyTorch.fit(X_train, y_train, validation_split=0.3)\n",
        "    y_pred = autoPyTorch.predict(X_test)\n",
        "    print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnDjGDpv8MvI"
      },
      "source": [
        "# Perform the data and metric imports.\n",
        "import sklearn.model_selection\n",
        "import sklearn.metrics\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import geopandas as gpd\n",
        "from prettytable import PrettyTable\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#Specify the URL.\n",
        "url = \"https://www.kaggle.com/kiva/data-science-for-good-kiva-crowdfunding\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "\n",
        "for row in all_rows:\n",
        "    stat = extract_contents(row.find_all('td')) \n",
        "    # Notice that the data that we require is now a list of length 5.\n",
        "    if len(stat) == 5:\n",
        "        stats.append(stat)\n",
        "\n",
        "    #Now convert the data into a pandas dataframe for further processing.\n",
        "    new_cols=[]\n",
        "    coll_data = pd.DataFrame(data = stats, columns = new_cols)\n",
        "    coll_data.head()\n",
        "\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    coll_data[new_cols] = coll_data[new_cols].map(int)\n",
        "    X, y = coll_data\n",
        "    X_train, X_test, y_train, y_test = \\\n",
        "        sklearn.model_selection.train_test_split(X, y, random_state=11001100)\n",
        "\n",
        "    #Auto-PyTorch\n",
        "    autoPyTorch = AutoNetClassification(\"tiny_cs\",  # config preset\n",
        "                                    log_level='info',\n",
        "                                    max_runtime=999999999**10000000,\n",
        "                                    min_budget=333,\n",
        "                                    max_budget=999999999*100000)\n",
        "    \n",
        "    #Fit. Predict. Get the accuracy score.\n",
        "    autoPyTorch.fit(X_train, y_train, validation_split=0.3)\n",
        "    y_pred = autoPyTorch.predict(X_test)\n",
        "    print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Km3lWBn9uDuL"
      },
      "source": [
        "# Perform the data and metric imports.\n",
        "import sklearn.model_selection\n",
        "import sklearn.metrics\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import geopandas as gpd\n",
        "from prettytable import PrettyTable\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#Specify the URL.\n",
        "url = \"https://www.kaggle.com/nipunarora8/age-gender-and-ethnicity-face-data-csv\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "    stat = extract_contents(row.find_all('td')) \n",
        "    # Notice that the data that we require is now a list of length 5.\n",
        "    if len(stat) == 5:\n",
        "        stats.append(stat)\n",
        "    #Now convert the data into a pandas dataframe for further processing.\n",
        "    new_cols=[]\n",
        "    face_data = pd.DataFrame(data = stats, columns = new_cols)\n",
        "    face_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype.\n",
        "    #Convert them into ‘int' datatype.\n",
        "    face_data[new_cols] = face_data[new_cols].map(int)\n",
        "    X, y = face_data\n",
        "    X_train, X_test, y_train, y_test = \\\n",
        "        sklearn.model_selection.train_test_split(X, y, random_state=11001100)\n",
        "\n",
        "    #Auto-PyTorch\n",
        "    autoPyTorch = AutoNetClassification(\"tiny_cs\",  # config preset\n",
        "                                    log_level='info',\n",
        "                                    max_runtime=999999999**10000000,\n",
        "                                    min_budget=333,\n",
        "                                    max_budget=999999999*100000)\n",
        "    \n",
        "    #Fit. Predict. Get the accuracy score. \n",
        "    autoPyTorch.fit(X_train, y_train, validation_split=0.3)\n",
        "    y_pred = autoPyTorch.predict(X_test)\n",
        "    print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ5krsSAxAMT"
      },
      "source": [
        "Install AutoGluon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wysi7YekxN5_"
      },
      "source": [
        "#Install AutoGluon.\n",
        "python3 -m pip install -U pip\n",
        "python3 -m pip install -U setuptools wheel\n",
        "python3 -m pip install -U \"mxnet<2.0.0\"\n",
        "python3 -m pip install autogluon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woP4-jC9MWrh"
      },
      "source": [
        "#Image Prediction with AutoGluon\n",
        "#Import AutoGluon.\n",
        "%matplotlib inline\n",
        "import autogluon.core as ag\n",
        "from autogluon.vision import ImageDataset\n",
        "import pandas as pd\n",
        "#Celeb faces (celebA) dataset\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "#Specify the URL.\n",
        "url = \"https://www.kaggle.com/c/petfinder-adoption-prediction/data\"\n",
        "\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  # Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  \n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    olympics2021_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = stats_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1221)\n",
        "    csv_file_list=[\"BreedLabels.csv\", \"ColorLabels.csv\", \"PetFinder-BreedLabels.csv\", \n",
        "                   \"PetFinder-ColorLabels.csv\", \"PetFinder-StateLabels.csv\", \"PetFinder-StateLabels.csv (285 B)\", \n",
        "                   \"StateLabels.csv\", \"breed_labels.csv\", \"color_labels.csv\", \"state_labels.csv\"]\n",
        "    \n",
        "    for each_csv_file in csv_file_list:\n",
        "      csv_file = ag.utils.download(each_csv_file)\n",
        "      df = pd.read_csv(csv_file)\n",
        "      df.head()\n",
        "      df = ImageDataset.from_csv(csv_file)\n",
        "      df.head()\n",
        "      train_data, _, test_data = ImageDataset.from_folders(each_csv_file, train='train', test='test')\n",
        "      print('train #', len(train_data), 'test #', len(test_data))\n",
        "      train_data.head()\n",
        "\n",
        "      #Load the splits with from_folder.\n",
        "      root = os.path.join(os.path.dirname(train_data.iloc[0]['image']), '..')\n",
        "      all_data = ImageDataset.from_folder(root)\n",
        "      all_data.head()\n",
        "\n",
        "      #Split the dataset.\n",
        "      train, val, test = all_data.random_split(val_size=0.1, test_size=0.1)\n",
        "      print('train #:', len(train), 'test #:', len(test))\n",
        "\n",
        "      #Convert a list of images to dataset.\n",
        "      pets = ag.utils.download(each_csv_file)\n",
        "      pets = ag.utils.unzip(pets)\n",
        "      image_list = [x for x in os.listdir(os.path.join(pets, 'images')) if x.endswith('jpg')]\n",
        "      new_data = ImageDataset.from_name_func(image_list, label_fn, root=os.path.join(os.getcwd(), pets, 'images'))\n",
        "      new_data\n",
        "\n",
        "    #Visualize the images.\n",
        "    new_data.show_images()\n",
        "\n",
        "    #Image prediction\n",
        "    import autogluon.core as ag\n",
        "    from autogluon.vision import ImagePredictor, ImageDataset\n",
        "    train_dataset, _, test_dataset = ImageDataset.from_folders(\"img_align_pets.zip\")\n",
        "    print(train_dataset)\n",
        "\n",
        "    #Fit a classifier.\n",
        "    predictor = ImagePredictor()\n",
        "\n",
        "    # Since the original dataset does not provide validation split, the `fit` function splits it randomly with 90/10 ratio.\n",
        "    predictor.fit(train_dataset, hyperparameters={'epochs': 1000})\n",
        "\n",
        "    #The best Top-1 accuracy achieved on the validation set is:\n",
        "    fit_result = predictor.fit_summary()\n",
        "    print('Top-1 train acc: %.3f, val acc: %.3f' %(fit_result['train_acc'], fit_result['valid_acc']))\n",
        "    \n",
        "    #Predict on a new image.\n",
        "    image_path = test_dataset.iloc[0]['image']\n",
        "    result = predictor.predict(image_path)\n",
        "    print(result)\n",
        "    bulk_result = predictor.predict(test_dataset)\n",
        "    print(bulk_result)\n",
        "\n",
        "    #Generate image features with a classifier.\n",
        "    image_path = test_dataset.iloc[0]['image']\n",
        "    feature = predictor.predict_feature(image_path)\n",
        "    print(feature)\n",
        "\n",
        "    #Validation and test top-1 accuracy is:\n",
        "    test_acc = predictor.evaluate(test_dataset)\n",
        "    print('Top-1 test acc: %.3f' % test_acc['top1'])\n",
        "\n",
        "    #Save and load the classifiers.\n",
        "    filename = 'predictor.ag'\n",
        "    predictor.save(filename)\n",
        "    predictor_loaded = ImagePredictor.load(filename)\n",
        "\n",
        "    # Use predictor_loaded as usual.\n",
        "    result = predictor_loaded.predict(image_path)\n",
        "    print(result)\n",
        "\n",
        "    #Use AutoGluon to produce an ImagePredictor to classify images.\n",
        "    import autogluon.core as ag\n",
        "    from autogluon.vision import ImagePredictor, ImageDataset\n",
        "    train_data, _, test_data = ImageDataset.from_folders(\"img_align_celeba.zip\")\n",
        "    model = ag.Categorical('resnet18_v1b', 'mobilenetv3_small')\n",
        "    model_list = ImagePredictor.list_models()\n",
        "\n",
        "    #Specify the training hyper-parameters.\n",
        "    batch_size = 8\n",
        "    lr = ag.Categorical(1e-2, 1e-3)\n",
        "\n",
        "    #Bayesian Optimization\n",
        "    hyperparameters={'model': model, 'batch_size': batch_size, 'lr': lr, 'epochs': 2}\n",
        "    predictor = ImagePredictor()\n",
        "    predictor.fit(train_data, time_limit=60*10, hyperparameters=hyperparameters,\n",
        "                  hyperparameter_tune_kwargs={'searcher': 'bayesopt', 'num_trials': 2})\n",
        "    print('Top-1 val acc: %.3f' % predictor.fit_summary()['valid_acc'])\n",
        "\n",
        "    #Load the test dataset and evaluate.\n",
        "    results = predictor.evaluate(test_data)\n",
        "    print('Test acc on hold-out data:', results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mwAsCQMSg6k"
      },
      "source": [
        "#Global Superstore Orders 2016 dataset\n",
        "#Tabular prediction with AutoGluon:\n",
        "\n",
        "#Predict columns in a table.\n",
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "train_data = TabularDataset('Global Superstore Orders 2016.csv')\n",
        "subsample_size = 999000000000  # Subsample subset of data for faster demo, try setting this to much larger values.\n",
        "train_data = train_data.sample(n=subsample_size, random_state=0)\n",
        "train_data.head()\n",
        "label = 'class'\n",
        "print(\"Summary of class variable: \\n\", train_data[label].describe())\n",
        "\n",
        "#Use AutoGluon to train multiple models.\n",
        "save_path = 'agModels-predictClass'  # Specifies folder to store trained models.\n",
        "predictor = TabularPredictor(label=label, path=save_path).fit(train_data)\n",
        "test_data = TabularDataset('Global Superstore Orders 2016.csv')\n",
        "y_test = test_data[label]  # Values to predict.\n",
        "test_data_nolab = test_data.drop(columns=[label])  # Delete label column to prove we're not cheating.\n",
        "test_data_nolab.head()\n",
        "\n",
        "#predictor = TabularPredictor.load(save_path)\n",
        "y_pred = predictor.predict(test_data_nolab)\n",
        "print(\"Predictions:  \\n\", y_pred)\n",
        "perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)\n",
        "predictor.leaderboard(test_data, silent=True)\n",
        "\n",
        "from autogluon.tabular import TabularPredictor\n",
        "predictor = TabularPredictor(label=label).fit(train_data='Global Superstore Orders 2016.csv')\n",
        "\n",
        "#.fit() returns a predictor object.\n",
        "pred_probs = predictor.predict_proba(test_data_nolab)\n",
        "pred_probs.head(5)\n",
        "\n",
        "#Summarize what happened during fit.\n",
        "results = predictor.fit_summary(show_plot=True)\n",
        "print(\"AutoGluon infers problem type is: \", predictor.problem_type)\n",
        "print(\"AutoGluon identified the following types of features:\")\n",
        "print(predictor.feature_metadata)\n",
        "predictor.leaderboard(test_data, silent=True)\n",
        "predictor.predict(test_data, model='LightGBM')\n",
        "\n",
        "#Maximize the predictive performance.\n",
        "time_limit = 11  \n",
        "metric = 'roc_auc'  # Specify the evaluation metric here.\n",
        "\n",
        "predictor = TabularPredictor(label, eval_metric=metric).fit(train_data, time_limit=time_limit, presets='best_quality')\n",
        "predictor.leaderboard(test_data, silent=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXtk2gPQxbrM"
      },
      "source": [
        "#Tabular prediction with AutoGluon:\n",
        "\n",
        "#Predict columns in a table.\n",
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "train_data = TabularDataset('pueblosMagicos.csv')\n",
        "subsample_size = 999000000000  # Subsample subset of data for faster demo, try setting this to much larger values.\n",
        "train_data = train_data.sample(n=subsample_size, random_state=0)\n",
        "train_data.head()\n",
        "label = 'class'\n",
        "print(\"Summary of class variable: \\n\", train_data[label].describe())\n",
        "\n",
        "#Use AutoGluon to train multiple models.\n",
        "save_path = 'agModels-predictClass'  # specifies folder to store trained models\n",
        "predictor = TabularPredictor(label=label, path=save_path).fit(train_data)\n",
        "test_data = TabularDataset('pueblosMagicos.csv')\n",
        "y_test = test_data[label]  # values to predict\n",
        "test_data_nolab = test_data.drop(columns=[label])  # delete label column to prove we're not cheating\n",
        "test_data_nolab.head()\n",
        "\n",
        "#predictor = TabularPredictor.load(save_path)\n",
        "y_pred = predictor.predict(test_data_nolab)\n",
        "print(\"Predictions:  \\n\", y_pred)\n",
        "perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)\n",
        "predictor.leaderboard(test_data, silent=True)\n",
        "\n",
        "from autogluon.tabular import TabularPredictor\n",
        "predictor = TabularPredictor(label=label).fit(train_data='pueblosMagicos.csv')\n",
        "\n",
        "#.fit() returns a predictor object.\n",
        "pred_probs = predictor.predict_proba(test_data_nolab)\n",
        "pred_probs.head(5)\n",
        "\n",
        "#Summarize what happened during fit.\n",
        "results = predictor.fit_summary(show_plot=True)\n",
        "print(\"AutoGluon infers problem type is: \", predictor.problem_type)\n",
        "print(\"AutoGluon identified the following types of features:\")\n",
        "print(predictor.feature_metadata)\n",
        "predictor.leaderboard(test_data, silent=True)\n",
        "predictor.predict(test_data, model='LightGBM')\n",
        "\n",
        "#Maximizing predictive performance.\n",
        "time_limit = 11  \n",
        "metric = 'roc_auc'  # Specify the evaluation metric here.\n",
        "predictor = TabularPredictor(label, eval_metric=metric).fit(train_data, time_limit=time_limit, presets='best_quality')\n",
        "predictor.leaderboard(test_data, silent=True)\n",
        "\n",
        "#Regression (predict numeric table columns)\n",
        "pueblo_column = 'PUEBLO'\n",
        "print(\"Summary of PUEBLO variable: \\n\", train_data[pueblo_column].describe())\n",
        "\n",
        "predictor_pueblo = TabularPredictor(label=pueblo_column, path=\"agModels-predictAge\").fit(train_data, time_limit=60)\n",
        "performance = predictor_pueblo.evaluate(test_data)\n",
        "\n",
        "#See the per-model performance.\n",
        "predictor_pueblo.leaderboard(test_data, silent=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL-x-IK0_8EW"
      },
      "source": [
        "#All NeurIPS (NIPS) Papers dataset\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "#Specify the URL.\n",
        "url = \"https://www.kaggle.com/rowhitswami/nips-papers-1987-2019-updated\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  # Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    weather_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = weather_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1313)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=100,\n",
        "                                    max_depth=4,\n",
        "                                    learn_rate=0.1)\n",
        "    \n",
        "    #Specify the predictor set and response.\n",
        "    x = list(weather_data.columns)\n",
        "    x\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)\n",
        "    \n",
        "    #TPOT-NN\n",
        "    from tpot import TPOTClassifier\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)\n",
        "\n",
        "    clf = TPOTClassifier(config_dict='TPOT NN', template='Selector-Transformer-PytorchLRClassifier',\n",
        "                        verbosity=2, population_size=10, generations=10)\n",
        "    clf.fit(X_train, y_train)\n",
        "    print(clf.score(X_test, y_test))\n",
        "    clf.export('tpot_nn_weather_data_pipeline.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW9xd-v-Uv1n"
      },
      "source": [
        "AutoKeras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahTgRczzUo2V"
      },
      "source": [
        "#Install AutoKeras\n",
        "!pip install autokeras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5XVgUkuU0DX"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import autokeras as ak\n",
        "input_node = ak.ImageInput()\n",
        "output_node = ak.Normalization()(input_node)\n",
        "output_node1 = ak.ConvBlock()(output_node)\n",
        "output_node2 = ak.ResNetBlock(version=\"v2\")(output_node)\n",
        "output_node = ak.Merge()([output_node1, output_node2])\n",
        "output_node = ak.ClassificationHead()(output_node)\n",
        "\n",
        "auto_model = ak.AutoModel(\n",
        "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=100\n",
        ")\n",
        "\n",
        "#Prepare data to run the model.\n",
        "x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(y_train[:3])\n",
        "\n",
        "# Feed the AutoModel with training data.\n",
        "auto_model.fit(x_train[:100], y_train[:100], epochs=1000)\n",
        "# Predict with the best model.\n",
        "predicted_y = auto_model.predict(x_test)\n",
        "# Evaluate the best model with testing data.\n",
        "print(auto_model.evaluate(x_test, y_test))\n",
        "\n",
        "#Implement new block.\n",
        "class SingleDenseLayerBlock(ak.Block):\n",
        "    def build(self, hp, inputs=None):\n",
        "        # Get the input_node from inputs.\n",
        "        input_node = tf.nest.flatten(inputs)[0]\n",
        "        layer = tf.keras.layers.Dense(\n",
        "            hp.Int(\"num_units\", min_value=32, max_value=512, step=32)\n",
        "        )\n",
        "        output_node = layer(input_node)\n",
        "        return output_node\n",
        "\n",
        "# Build the AutoModel.\n",
        "input_node = ak.Input()\n",
        "output_node = SingleDenseLayerBlock()(input_node)\n",
        "output_node = ak.RegressionHead()(output_node)\n",
        "auto_model = ak.AutoModel(input_node, output_node, overwrite=True, max_trials=100)\n",
        "\n",
        "# Prepare the data.\n",
        "num_instances = 100\n",
        "x_train = np.random.rand(num_instances, 20).astype(np.float32)\n",
        "y_train = np.random.rand(num_instances, 1).astype(np.float32)\n",
        "x_test = np.random.rand(num_instances, 20).astype(np.float32)\n",
        "y_test = np.random.rand(num_instances, 1).astype(np.float32)\n",
        "\n",
        "# Train the model.\n",
        "auto_model.fit(x_train, y_train, epochs=1000)\n",
        "print(auto_model.evaluate(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaWpQVvBnVgf"
      },
      "source": [
        "#Google VPP dataset\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from autokeras import StructuredDataClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import tensorflow as tf\n",
        "import autokeras as ak\n",
        "\n",
        "input_node = ak.ImageInput()\n",
        "output_node = ak.Normalization()(input_node)\n",
        "output_node1 = ak.ConvBlock()(output_node)\n",
        "output_node2 = ak.ResNetBlock(version=\"v2\")(output_node)\n",
        "output_node = ak.Merge()([output_node1, output_node2])\n",
        "output_node = ak.ClassificationHead()(output_node)\n",
        "\n",
        "#Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "#Specify the URL.\n",
        "url = \"https://www.kaggle.com/kalaikumarr/google-vpp-comparing-with-28-models/data\"\n",
        "\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  #Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    kaggle_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = stats_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=11)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=1000,\n",
        "                                    max_depth=4,\n",
        "                                    learn_rate=0.1)\n",
        "    #Specify the predictor set and response.\n",
        "    x = list(stats_train.columns)\n",
        "    x\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)\n",
        "\n",
        "    auto_model = ak.AutoModel(\n",
        "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=100)\n",
        "    \n",
        "    #Prepare data to run the model.\n",
        "    (x_train, y_train), (x_test, y_test) = kaggle_data\n",
        "    print(x_train.shape)\n",
        "    print(y_train.shape)\n",
        "    print(y_train[:3])\n",
        "\n",
        "    #Feed the AutoModel with training data.\n",
        "    auto_model.fit(x_train[:100], y_train[:100], epochs=100000)\n",
        "    # Predict with the best model.\n",
        "    predicted_y = auto_model.predict(x_test)\n",
        "    # Evaluate the best model with testing data.\n",
        "    print(auto_model.evaluate(x_test, y_test))\n",
        "\n",
        "    #Implement new block.\n",
        "    class SingleDenseLayerBlock(ak.Block):\n",
        "        def build(self, hp, inputs=None):\n",
        "            # Get the input_node from inputs.\n",
        "            input_node = tf.nest.flatten(inputs)[0]\n",
        "            layer = tf.keras.layers.Dense(\n",
        "                hp.Int(\"num_units\", min_value=32, max_value=512, step=32))\n",
        "            output_node = layer(input_node)\n",
        "            return output_node\n",
        "\n",
        "    # Build the AutoModel.\n",
        "    input_node = ak.Input()\n",
        "    output_node = SingleDenseLayerBlock()(input_node)\n",
        "    output_node = ak.RegressionHead()(output_node)\n",
        "    auto_model = ak.AutoModel(input_node, output_node, overwrite=True, max_trials=100)\n",
        "   \n",
        "    # Prepare the data.\n",
        "    num_instances = 100\n",
        "    x_train = np.random.rand(num_instances, 20).astype(np.float32)\n",
        "    y_train = np.random.rand(num_instances, 1).astype(np.float32)\n",
        "    x_test = np.random.rand(num_instances, 20).astype(np.float32)\n",
        "    y_test = np.random.rand(num_instances, 1).astype(np.float32)\n",
        "    \n",
        "    # Train the model.\n",
        "    auto_model.fit(x_train, y_train, epochs=100000)\n",
        "    print(auto_model.evaluate(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfa83T9PcPK2"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "from autokeras import StructuredDataClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import tensorflow as tf\n",
        "import autokeras as ak\n",
        "\n",
        "input_node = ak.ImageInput()\n",
        "output_node = ak.Normalization()(input_node)\n",
        "output_node1 = ak.ConvBlock()(output_node)\n",
        "output_node2 = ak.ResNetBlock(version=\"v2\")(output_node)\n",
        "output_node = ak.Merge()([output_node1, output_node2])\n",
        "output_node = ak.ClassificationHead()(output_node)\n",
        "\n",
        "#Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "#Specify the URL.\n",
        "url = \"https://www.kaggle.com/kannan1314/apple-stock-price-all-time?select=Apple.csv\"\n",
        "\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  #Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    kaggle_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = stats_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=11)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=100,\n",
        "                                    max_depth=4,\n",
        "                                    learn_rate=0.1)\n",
        "    #Specify the predictor set and response.\n",
        "    x = list(stats_train.columns)\n",
        "    x\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)\n",
        "\n",
        "    auto_model = ak.AutoModel(\n",
        "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=100)\n",
        "    \n",
        "    #Prepare data to run the model.\n",
        "    (x_train, y_train), (x_test, y_test) = kaggle_data\n",
        "    print(x_train.shape)\n",
        "    print(y_train.shape)\n",
        "    print(y_train[:3])\n",
        "\n",
        "    #Feed the AutoModel with training data.\n",
        "    auto_model.fit(x_train[:100], y_train[:100], epochs=1000)\n",
        "    # Predict with the best model.\n",
        "    predicted_y = auto_model.predict(x_test)\n",
        "    # Evaluate the best model with testing data.\n",
        "    print(auto_model.evaluate(x_test, y_test))\n",
        "\n",
        "    #Implement new block.\n",
        "    class SingleDenseLayerBlock(ak.Block):\n",
        "        def build(self, hp, inputs=None):\n",
        "            # Get the input_node from inputs.\n",
        "            input_node = tf.nest.flatten(inputs)[0]\n",
        "            layer = tf.keras.layers.Dense(\n",
        "                hp.Int(\"num_units\", min_value=32, max_value=512, step=32)\n",
        "            )\n",
        "            output_node = layer(input_node)\n",
        "            return output_node\n",
        "\n",
        "    # Build the AutoModel.\n",
        "    input_node = ak.Input()\n",
        "    output_node = SingleDenseLayerBlock()(input_node)\n",
        "    output_node = ak.RegressionHead()(output_node)\n",
        "    auto_model = ak.AutoModel(input_node, output_node, overwrite=True, max_trials=100)\n",
        "   \n",
        "    # Prepare the data.\n",
        "    num_instances = 100\n",
        "    x_train = np.random.rand(num_instances, 20).astype(np.float32)\n",
        "    y_train = np.random.rand(num_instances, 1).astype(np.float32)\n",
        "    x_test = np.random.rand(num_instances, 20).astype(np.float32)\n",
        "    y_test = np.random.rand(num_instances, 1).astype(np.float32)\n",
        "    \n",
        "    # Train the model.\n",
        "    auto_model.fit(x_train, y_train, epochs=1000)\n",
        "    print(auto_model.evaluate(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAc3QuV6CN9z"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "\n",
        "import autokeras as ak\n",
        "import tensorflow_cloud as tfc\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description=\"Model save path arguments.\")\n",
        "parser.add_argument(\"--path\", required=True, type=str, help=\"Keras model save path\")\n",
        "args = parser.parse_args()\n",
        "\n",
        "tfc.run(\n",
        "    chief_config=tfc.COMMON_MACHINE_CONFIGS[\"V100_1X\"],\n",
        "    docker_base_image=\"haifengjin/autokeras:1.0.3\",\n",
        ")\n",
        "\n",
        "# Prepare the dataset.\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "print(x_train.shape)  \n",
        "print(y_train.shape) \n",
        "print(y_train[:3])\n",
        "\n",
        "# Initialize the ImageClassifier.\n",
        "clf = ak.ImageClassifier(max_trials=2)\n",
        "# Search for the best model.\n",
        "clf.fit(x_train, y_train, epochs=10)\n",
        "# Evaluate on the testing data.\n",
        "print(\"Accuracy: {accuracy}\".format(accuracy=clf.evaluate(x_test, y_test)[1]))\n",
        "\n",
        "clf.export_model().save(os.path.join(args.path, \"model.h5\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG7WbDsKbdgK"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "from autokeras import StructuredDataClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import tensorflow as tf\n",
        "import autokeras as ak\n",
        "\n",
        "input_node = ak.ImageInput()\n",
        "output_node = ak.Normalization()(input_node)\n",
        "output_node1 = ak.ConvBlock()(output_node)\n",
        "output_node2 = ak.ResNetBlock(version=\"v2\")(output_node)\n",
        "output_node = ak.Merge()([output_node1, output_node2])\n",
        "output_node = ak.ClassificationHead()(output_node)\n",
        "\n",
        "#Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "#Specify the URL.\n",
        "url = \"https://www.kaggle.com/c/petfinder-adoption-prediction/data\"\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "stat = extract_contents(row.find_all('td')) \n",
        "#Notice that the data that we require is now a list of length 5.\n",
        "if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "\n",
        "#Now convert the data into a pandas dataframe for further processing.\n",
        "new_cols = []\n",
        "for each_new_col in row:\n",
        "    petfinder_data = pd.DataFrame(data = petfinder_data, columns = each_new_col)\n",
        "    petfinder_data.head()\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = petfinder_data\n",
        "x_train = x_train[:100]\n",
        "y_train = y_train[:100]\n",
        "print(x_train.shape)  \n",
        "print(y_train.shape)  \n",
        "print(y_train[:3]) \n",
        "\n",
        "# Initialize the image regressor.\n",
        "reg = ak.ImageRegressor(overwrite=True, max_trials=1)\n",
        "\n",
        "# Feed the image regressor with training data.\n",
        "reg.fit(x_train, y_train, epochs=2)\n",
        "\n",
        "# Predict with the best model.\n",
        "predicted_y = reg.predict(x_test)\n",
        "print(predicted_y)\n",
        "\n",
        "# Evaluate the best model with testing data.\n",
        "print(reg.evaluate(x_test, y_test))\n",
        "\n",
        "#Validation data\n",
        "reg.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    # Split the training data and use the last 15% as validation data.\n",
        "    validation_split=0.15,\n",
        "    epochs=2000,\n",
        ")\n",
        "\n",
        "#Customized search space\n",
        "input_node = ak.ImageInput()\n",
        "output_node = ak.ImageBlock(\n",
        "    # Only search ResNet architectures.\n",
        "    block_type=\"resnet\",\n",
        "    # Normalize the dataset.\n",
        "    normalize=False,\n",
        "    # Do not do data augmentation.\n",
        "    augment=False,\n",
        ")(input_node)\n",
        "output_node = ak.RegressionHead()(output_node)\n",
        "reg = ak.AutoModel(\n",
        "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=100\n",
        ")\n",
        "\n",
        "reg.fit(x_train, y_train, epochs=2000)\n",
        "\n",
        "input_node = ak.ImageInput()\n",
        "output_node = ak.Normalization()(input_node)\n",
        "output_node = ak.ImageAugmentation(horizontal_flip=False)(output_node)\n",
        "output_node = ak.ResNetBlock(version=\"v2\")(output_node)\n",
        "output_node = ak.RegressionHead()(output_node)\n",
        "\n",
        "reg = ak.AutoModel(\n",
        "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=100\n",
        ")\n",
        "\n",
        "reg.fit(x_train, y_train, epochs=2000)\n",
        "\n",
        "#Data format\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape the images to have the channel dimension.\n",
        "x_train = x_train.reshape(x_train.shape + (1,))\n",
        "x_test = x_test.reshape(x_test.shape + (1,))\n",
        "y_train = y_train.reshape(y_train.shape + (1,))\n",
        "y_test = y_test.reshape(y_test.shape + (1,))\n",
        "\n",
        "print(x_train.shape) \n",
        "print(y_train.shape)  \n",
        "\n",
        "train_set = tf.data.Dataset.from_tensor_slices(((x_train,), (y_train,)))\n",
        "test_set = tf.data.Dataset.from_tensor_slices(((x_test,), (y_test,)))\n",
        "\n",
        "reg = ak.ImageRegressor(overwrite=True, max_trials=100)\n",
        "\n",
        "# Feed the tensorflow Dataset to the regressor.\n",
        "reg.fit(train_set, epochs=2000)\n",
        "\n",
        "# Predict with the best model.\n",
        "predicted_y = reg.predict(test_set)\n",
        "\n",
        "# Evaluate the best model with testing data.\n",
        "print(reg.evaluate(test_set))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ9iD7yNhDa7"
      },
      "source": [
        "**Deliverables:**\n",
        "\n",
        "*   Python package with an Automated ML function to be called using any data frame (dataset) to give a good trained model.\n",
        "*   Details on the steps automated and the scenario in which they execute.\n",
        "*   Any scenario which is not automated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyIRiBWrkk2g"
      },
      "source": [
        "TPOT cannot solve multi-label regression problems at this point of time."
      ]
    }
  ]
}