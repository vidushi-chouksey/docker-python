{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"#install\n!pip install --upgrade pip\n!pip install example --use-feature=2020-resolver\n!pip install torch\n#upgrade pytorch\n!pip install --upgrade torch torchvision\n#install h2o\n!pip install h2o\n#install autokeras\n!pip install autokeras\n#upgrade tensorflow\n!pip install --ignore-installed --upgrade tensorflow\n#install pytorch\n!pip install pytorch\n#install mlbox\n!pip install mlbox\n#install tpot\n!pip install tpot\n#install autogluon\n!pip install autogluon\n\n#NFL Big Data Bowl 2022 kaggle dataset\nimport numpy as np \nimport pandas as pd\nfrom autokeras import StructuredDataClassifier\nfrom sklearn.model_selection import train_test_split\nimport requests\nfrom bs4 import BeautifulSoup\nimport tensorflow as tf\nimport autokeras as ak\nimport plotly.graph_objects as go\n\n#for eda\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#MLBOX\n#dataset\nfrom mlbox.optimisation import Optimiser, Regressor\n    \n#evaluating the pipeline\nopt = Optimiser()\nparams = {\"ne__numerical_strategy\" : 0, \"ce__strategy\" : \"label_encoding\", \"fs__threshold\" : 0.1, \"stck__base_estimators\" : [Regressor(strategy=\"RandomForest\"), Regressor(strategy=\"ExtraTrees\")], \"est__strategy\" : \"Linear\"}\ndf = {\"train\" : pd.DataFrame(train_data.iloc[:,:-1]), \"target\" : pd.Series(test_data.iloc[:,-1])}\n\n#build a keras model\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\nmodel = keras.Sequential()\n#relu: Rectified Linear Unit\n# Adds a densely-connected layer with 64 units to the model:\nmodel.add(keras.layers.Dense(64, activation='relu'))\n# Add another:\nmodel.add(keras.layers.Dense(64, activation='relu'))\n# Add a softmax layer with 10 output units:\nmodel.add(keras.layers.Dense(10, activation='softmax'))\n#define a ConvModel\nclass ConvModel(tf.keras.Model):\n    def __init__(self, nfs, input_shape, output_shape, use_bn=False, use_dp=False):\n        super(ConvModel, self).__init__(name='mlp')\n        self.use_bn = use_bn\n        self.use_dp = use_dp\n        self.num_classes = num_classes\n\n    # backbone layers\n    self.convs = [ConvLayer(nfs[0], s=1, input_shape=input_shape)]\n    self.convs += [ConvLayer(nf) for nf in nfs[1:]]\n    # classification layers\n    self.convs.append(AveragePooling2D())\n    self.convs.append(Dense(output_shape, activation='softmax'))\n\n    def call(self, inputs):\n        for layer in self.convs: inputs = layer(inputs)\n        return inputs\n    #compile the model\n    model.compile(loss='categorical crossentropy', metrics=['accuracy'], optimizer='rmsprop')\n    model.build((None, 32, 32, 3))\n\n    model.summary()\n\n#Auto-Sklearn\nfrom sklearn import datasets\nimport autosklearn.classification\ncls = autosklearn.classification.AutoSklearnClassifier()\n        \nX, y = each_dataset(return_X_y=True)\nX_train, X_test, y_train, y_test = \\\n                sklearn.model_selection.train_test_split(X, y, random_state=1)\n\ncls.fit(X_train, y_train)\npredictions = cls.predict(X_test)\n\nimport sklearn.model_selection\nimport sklearn.metrics\nif __name__ == \"__main__\":\n            X, y = each_dataset(return_X_y=True)\n            X_train, X_test, y_train, y_test = \\\n                    sklearn.model_selection.train_test_split(X, y, random_state=1)\n            automl = autosklearn.classification.AutoSklearnClassifier()\n\n            automl.fit(X_train, y_train)\n            y_hat = automl.predict(X_test)\n            print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_hat))\n\nimport numpy as np\nimport tensorflow as tf\nimport autokeras as ak\ninput_node = ak.ImageInput()\noutput_node = ak.Normalization()(input_node)\noutput_node1 = ak.ConvBlock()(output_node)\noutput_node2 = ak.ResNetBlock(version=\"v2\")(output_node)\noutput_node = ak.Merge()([output_node1, output_node2])\noutput_node = ak.ClassificationHead()(output_node)\n\nauto_model = ak.AutoModel(inputs=input_node, outputs=output_node, overwrite=True, max_trials=100)\n#prepare data to run the model\n(x_train, y_train), (x_test, y_test) = each_datset\nprint(x_train.shape)\nprint(y_train.shape) \nprint(y_train[:3])\n\n# Feed the AutoModel with training data.\nauto_model.fit(x_train[:100], y_train[:100], epochs=1000)\n# Predict with the best model\npredicted_y = auto_model.predict(x_test)\n# Evaluate the best model with testing data\nprint(auto_model.evaluate(x_test, y_test))\n\n#implement new block\nclass SingleDenseLayerBlock(ak.Block):\n    def build(self, hp, inputs=None):\n        # Get the input_node from inputs.\n        input_node = tf.nest.flatten(inputs)[0]\n        layer = tf.keras.layers.Dense(\n        hp.Int(\"num_units\", min_value=32, max_value=512, step=32)\n        )\n        output_node = layer(input_node)\n        return output_node\n\n# Build the AutoModel\ninput_node = ak.Input()\noutput_node = SingleDenseLayerBlock()(input_node)\noutput_node = ak.RegressionHead()(output_node)\nauto_model = ak.AutoModel(input_node, output_node, overwrite=True, max_trials=100)\n# Prepare Data\nnum_instances = 100\nx_train = np.random.rand(num_instances, 20).astype(np.float32)\ny_train = np.random.rand(num_instances, 1).astype(np.float32)\nx_test = np.random.rand(num_instances, 20).astype(np.float32)\ny_test = np.random.rand(num_instances, 1).astype(np.float32)\n# Train the model\nauto_model.fit(x_train, y_train, epochs=1000)\nprint(auto_model.evaluate(x_test, y_test))\n\n#Auto-Sklearn\nfrom sklearn import datasets\nimport autosklearn.classification\ncls = autosklearn.classification.AutoSklearnClassifier()\n        \nX, y = each_dataset(return_X_y=True)\nX_train, X_test, y_train, y_test = \\\nsklearn.model_selection.train_test_split(X, y, random_state=1)\ncls.fit(X_train, y_train)\npredictions = cls.predict(X_test)\n\nimport sklearn.model_selection\nimport sklearn.metrics\nif __name__ == \"__main__\":\n    X, y = each_dataset(return_X_y=True)\n    X_train, X_test, y_train, y_test = \\\n    sklearn.model_selection.train_test_split(X, y, random_state=1)\n    automl = autosklearn.classification.AutoSklearnClassifier()\n\n    automl.fit(X_train, y_train)\n    y_hat = automl.predict(X_test)\n    print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_hat))\n\nimport numpy as np\nimport tensorflow as tf\nimport autokeras as ak\ninput_node = ak.ImageInput()\noutput_node = ak.Normalization()(input_node)\noutput_node1 = ak.ConvBlock()(output_node)\noutput_node2 = ak.ResNetBlock(version=\"v2\")(output_node)\noutput_node = ak.Merge()([output_node1, output_node2])\noutput_node = ak.ClassificationHead()(output_node)\n\nauto_model = ak.AutoModel(inputs=input_node, outputs=output_node, overwrite=True, max_trials=100)\n#prepare data to run the model\n(x_train, y_train), (x_test, y_test) = each_datset\nprint(x_train.shape)\nprint(y_train.shape)\nprint(y_train[:3])\n\n# Feed the AutoModel with training data.\nauto_model.fit(x_train[:100], y_train[:100], epochs=1000)\n# Predict with the best model\npredicted_y = auto_model.predict(x_test)\n# Evaluate the best model with testing data\nprint(auto_model.evaluate(x_test, y_test))\n\n#implement new block\nclass SingleDenseLayerBlock(ak.Block):\n    def build(self, hp, inputs=None):\n        # Get the input_node from inputs.\n        input_node = tf.nest.flatten(inputs)[0]\n        layer = tf.keras.layers.Dense(\n        hp.Int(\"num_units\", min_value=32, max_value=512, step=32)\n                )\n        output_node = layer(input_node)\n        return output_node\n\n    # Build the AutoModel\n    input_node = ak.Input()\n    output_node = SingleDenseLayerBlock()(input_node)\n    output_node = ak.RegressionHead()(output_node)\n    auto_model = ak.AutoModel(input_node, output_node, overwrite=True, max_trials=100)\n    #Prepare Data\n    num_instances = 100\n    x_train = np.random.rand(num_instances, 20).astype(np.float32)\n    y_train = np.random.rand(num_instances, 1).astype(np.float32)\n    x_test = np.random.rand(num_instances, 20).astype(np.float32)\n    y_test = np.random.rand(num_instances, 1).astype(np.float32)\n    # Train the model\n    auto_model.fit(x_train, y_train, epochs=1000)\n    print(auto_model.evaluate(x_test, y_test))\n        \n# Import H2O GBM:\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\nurl = \"https://www.kaggle.com/c/nfl-big-data-bowl-2022/data\"\n# make a GET request to fetch the raw HTML content\nweb_content = requests.get(url).content\n# parse the html content\nsoup = BeautifulSoup(web_content, \"html.parser\")\n# remove any newlines and extra spaces from left and right\nextract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n# find all table rows and data cells within\nstats = [] \nall_rows = soup.find_all('tr')\nfor row in all_rows:\n  stat = extract_contents(row.find_all('td')) \n  # notice that the data that we require is now a list of length 5\n  if len(stat) == 5:\n    stats.append(stat)\n  #now convert the data into a pandas dataframe for further processing\n  new_cols = []\n  for each_new_col in row:\n    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n    stats_data.head()\n    #scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype\n    nfl_2022_data[each_new_col] = stats_data[each_new_col].map(int)\n    X, y = stats_data\n    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1221)\n    csv_file_list=[\"PFFScoutingData.csv\", \"games.csv\", \"players.csv\", \"plays.csv\", \"tracking2018.csv\", \"tracking2019.csv\", \"tracking2020.csv\"]\n    for each_csv_file in csv_file_list:\n      csv_file = ag.utils.download(each_csv_file)\n      df = pd.read_csv(csv_file)\n      df.head()\n      df = ImageDataset.from_csv(csv_file)\n      df.head()\n    \n      train_df = pd.read_csv(each_csv_file)\n      test_df = pd.read_csv(each_csv_file)\n      train_df.head(5)\n      test_df.head(5)\n      train_df.isnull().sum()\n      print(train_df['R'].value_counts())\n      print(train_df['C'].value_counts()) \n        \n      fig = go.Figure()\n      num_id = list(train_df['breath_id'].unique())\n      sample_num_id = num_id[0:200]\n\n      for i in sample_num_id:\n\n        idx = train_df.loc[train_df[['gameId','playId','nflId']] == i,['gameId','playId','nflId']].unique()[0]\n        R = train_df.loc[train_df[['gameId','playId','nflId']] == i,'R'].unique()[0]\n        C = train_df.loc[train_df[['gameId','playId','nflId']] == i,'C'].unique()[0]\n\n      if (R == 5) & (C == 10): \n        fig.add_trace(go.Scatter(x=train_df.loc[train_df[['gameId','playId','nflId']] == i,'time_step'],\n        y=train_df.loc[train_df[['gameId','playId','nflId']] == i,'pressure'],\n        name=f'breath_id : {idx}, R : {R}, C : {C}'))\n      fig.show()\n    \n      fig = go.Figure()\n      num_id = list(train_df['gameId','playId','nflId'].unique())\n      sample_num_id = num_id[0:200]\n\n      for i in sample_num_id:\n\n        idx = train_df.loc[train_df[['gameId','playId','nflId']] == i,['gameId', 'playId','nflId']].unique()[0]\n        R = train_df.loc[train_df['gameId','playId','nflId'] == i,'R'].unique()[0]\n        C = train_df.loc[train_df['gameId','playId','nflId'] == i,'C'].unique()[0]\n\n      if (R == 5) & (C == 20): \n        fig.add_trace(go.Scatter(x=train_df.loc[train_df['gameId', 'playId','nflId'] == i,'time_step'],\n                                     y=train_df.loc[train_df['gameId', 'playId','nflId'] == i,'pressure'],\n                                     name=f'gameId_playId_nflId : {idx}, R : {R}, C : {C}'))\n      fig.show()\n    \n      fig = go.Figure()\n      num_id = list(train_df[['gameId', 'playId','nflId']].unique())\n      sample_num_id = num_id[0:200]\n\n      for i in sample_num_id:\n    \n          idx = train_df.loc[train_df[['gameId', 'playId','nflId']] == i,['gameId', 'playId','nflId']].unique()[0]\n          R = train_df.loc[train_df[['gameId', 'playId','nflId']] == i,'R'].unique()[0]\n          C = train_df.loc[train_df[['gameId', 'playId','nflId']] == i,'C'].unique()[0]\n    \n      if (R == 5) & (C == 50): \n        fig.add_trace(go.Scatter(x=train_df.loc[train_df[['gameId', 'playId','nflId']] == i,'time_step'],\n                                 y=train_df.loc[train_df[['gameId', 'playId','nflId']] == i,'pressure'],\n                                 name=f'gameId_playId_nflId : {idx}, R : {R}, C : {C}'))\n\n      fig.show()\n\n        \n#StructuredDataClassifier\nautokeras.StructuredDataClassifier(\n    column_names=None,\n    column_types=None,\n    num_classes=None,\n    multi_label=False,\n    loss=None,\n    metrics=None,\n    project_name=\"structured_data_classifier\",\n    max_trials=100,\n    directory=None,\n    objective=\"val_accuracy\",\n    tuner=None,\n    overwrite=False,\n    seed=None,\n    max_model_size=None,\n    **kwargs)\n\n#fit\nStructuredDataClassifier.fit(\n    x=None, y=None, epochs=1000, callbacks=None, validation_split=0.2, validation_data=None, **kwargs)\n\n#predict\nStructuredDataClassifier.predict(x, **kwargs)\n\n#evaluate\nStructuredDataClassifier.evaluate(x, y=None, **kwargs)\n\n#export_model\nStructuredDataClassifier.export_model()\n\nfrom tpot import TPOTClassifier\nfrom sklearn.datasets import make_blobs\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_blobs(n_samples=100, centers=2, n_features=3, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)\n\nclf = TPOTClassifier(config_dict='TPOT NN', template='Selector-Transformer-PytorchLRClassifier',\n                     verbosity=2, population_size=100, generations=100)\nclf.fit(X_train, y_train)\nprint(clf.score(X_test, y_test))\nclf.export('tpot_nn_nfl_2022_pipeline.py')","metadata":{"execution":{"iopub.status.busy":"2021-09-25T16:05:44.078487Z","iopub.execute_input":"2021-09-25T16:05:44.078852Z","iopub.status.idle":"2021-09-25T16:08:11.298265Z","shell.execute_reply.started":"2021-09-25T16:05:44.078815Z","shell.execute_reply":"2021-09-25T16:08:11.296086Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"#Music dataset using TPOT-NN\nimport requests\nfrom bs4 import BeautifulSoup\n# Import H2O GBM:\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\nurl = \"https://www.kaggle.com/imsparsh/musicnet-dataset?select=musicnet_metadata.csv\"\n# make a GET request to fetch the raw HTML content\nweb_content = requests.get(url).content\n# parse the html content\nsoup = BeautifulSoup(web_content, \"html.parser\")\n# remove any newlines and extra spaces from left and right\nextract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n# find all table rows and data cells within\nstats = [] \nall_rows = soup.find_all('tr')\nfor row in all_rows:\n  stat = extract_contents(row.find_all('td')) \n  # notice that the data that we require is now a list of length 5\n  if len(stat) == 5:\n    stats.append(stat)\n  #now convert the data into a pandas dataframe for further processing\n  new_cols = []\n  for each_new_col in row:\n    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n    stats_data.head()\n    #scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype\n    music_data[each_new_col] = stats_data[each_new_col].map(int)\n    X, y = music_data\n    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1313)\n    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n                                    ntrees=100,\n                                    max_depth=4,\n                                    learn_rate=0.1)\n    #Specify the predictor set and response\n    x = list(music_data.columns)\n    x\n    #train the model\n    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n    print(model)\n    #using TPOT-NN\n    from tpot import TPOTClassifier\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)\n\n    clf = TPOTClassifier(config_dict='TPOT NN', template='Selector-Transformer-PytorchLRClassifier',\n                        verbosity=2, population_size=10, generations=10)\n    clf.fit(X_train, y_train)\n    print(clf.score(X_test, y_test))\n    clf.export('tpot_nn_music_data_pipeline.py')","metadata":{"execution":{"iopub.status.busy":"2021-09-25T16:08:11.299925Z","iopub.status.idle":"2021-09-25T16:08:11.300374Z","shell.execute_reply.started":"2021-09-25T16:08:11.300173Z","shell.execute_reply":"2021-09-25T16:08:11.300198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#AutoGluon:\n#Tabular prediction with AutoGluon\n#Predicting Columns in a Table\nfrom autogluon.tabular import TabularDataset, TabularPredictor\ntrain_data = TabularDataset(each_dataset)\nsubsample_size = 55500000  # subsample subset of data for faster demo\ntrain_data = train_data.sample(n=subsample_size, random_state=0)\ntrain_data.head()\nlabel = 'class' \nprint(\"Summary of class variable: \\n\", train_data[label].describe())\n#use AutoGluon to train multiple models\nsave_path = 'agModels-predictClass'  # specifies folder to store trained models\npredictor = TabularPredictor(label=label, path=save_path).fit(train_data)\ntest_data = TabularDataset(each_dataset)\ny_test = test_data[label]  # values to predict\ntest_data_nolab = test_data.drop(columns=[label])  # delete label column to prove we're not cheating\ntest_data_nolab.head()\n#predictor = TabularPredictor.load(save_path)\ny_pred = predictor.predict(test_data_nolab)\nprint(\"Predictions:  \\n\", y_pred)\nperf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)\npredictor.leaderboard(test_data, silent=True)\nfrom autogluon.tabular import TabularPredictor\npredictor = TabularPredictor(label=label).fit(train_data=each_dataset)\n#.fit() returns a predictor object\npred_probs = predictor.predict_proba(test_data_nolab)\npred_probs.head(5)\n#summarize what happened during fit\nresults = predictor.fit_summary(show_plot=True)\nprint(\"AutoGluon infers problem type is: \", predictor.problem_type)\nprint(\"AutoGluon identified the following types of features:\")\nprint(predictor.feature_metadata)\npredictor.leaderboard(test_data, silent=True)\npredictor.predict(test_data, model='LightGBM')\n#Maximizing predictive performance\ntime_limit = 11  \nmetric = 'roc_auc'  # specify the evaluation metric here\npredictor = TabularPredictor(label, eval_metric=metric).fit(train_data, time_limit=time_limit, presets='best_quality')\npredictor.leaderboard(test_data, silent=True)\n        \n#install h2o\n!pip install h2o\nimport requests\nfrom bs4 import BeautifulSoup\n# Import H2O GBM:\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\nurl = \"https://www.kaggle.com/fatiimaezzahra/famous-iconic-women\"\n# make a GET request to fetch the raw HTML content\nweb_content = requests.get(url).content\n# parse the html content\nsoup = BeautifulSoup(web_content, \"html.parser\")\n# remove any newlines and extra spaces from left and right\nextract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n# find all table rows and data cells within\nstats = [] \nall_rows = soup.find_all('tr')\nfor row in all_rows:\n  stat = extract_contents(row.find_all('td')) \n  # notice that the data that we require is now a list of length 5\n  if len(stat) == 5:\n    stats.append(stat)\n  #now convert the data into a pandas dataframe for further processing\n  new_cols = []\n  for each_new_col in row:\n    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n    stats_data.head()\n    #scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype\n    kaggle_data[each_new_col] = stats_data[each_new_col].map(int)\n    X, y = stats_data\n    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=11)\n    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n                                    ntrees=100,\n                                    max_depth=4,\n                                    learn_rate=0.1)\n    #Specify the predictor set and response\n    x = list(stats_train.columns)\n    x\n    #train the model\n    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n    print(model)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T16:08:11.304717Z","iopub.status.idle":"2021-09-25T16:08:11.305055Z","shell.execute_reply.started":"2021-09-25T16:08:11.304877Z","shell.execute_reply":"2021-09-25T16:08:11.304893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from distutils.core import setup\nfrom setuptools import find_packages\nimport autokeras\nsetup(\nname='autokeras',\npackages=find_packages(exclude=('tests',)),\ninstall_requires=['scipy==1.2.0',\n'tensorflow==1.13.1',\n'torch==1.1.0',\n'torchvision==0.2.2.post3',\n'numpy==1.16.1',\n'scikit-learn==0.20.2',\n'scikit-image==0.14.2',\n'tqdm==4.31.0',\n'imageio==2.5.0',\n'requests==2.21.0'\n],\nversion='0.4.0',\ndescription='AutoML for deep learning',\nauthor='DATA Lab at Texas A&M University',\nauthor_email='jhfjhfj1@gmail.com',\nurl='http://autokeras.com',\ndownload_url='https://github.com/keras-team/autokeras/archive/0.3.7.tar.gz',\nkeywords=['AutoML', 'keras'],\nclassifiers=[]\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T16:08:11.308979Z","iopub.status.idle":"2021-09-25T16:08:11.309293Z","shell.execute_reply.started":"2021-09-25T16:08:11.309130Z","shell.execute_reply":"2021-09-25T16:08:11.309145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#AutoML function\ndef automl(**kwargs):\n  from sklearn import datasets\n  import numpy as np\n  import pandas as pd\n  import matplotlib.pyplot as plt\n  import seaborn as sns\n  import requests\n  from bs4 import BeautifulSoup\n  import geopandas as gpd\n  from prettytable import PrettyTable\n  from autokeras import StructuredDataClassifier\n  from sklearn.model_selection import train_test_split\n\n  #Web Scraping\n  url_list = [\"https://www.kaggle.com/datasets?datasetsOnly=true\", \"https://public.knoema.com/\", \"https://www.kaggle.com/c/nfl-health-and-safety-helmet-assignment/data\", \"http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets/\", \"https://www.kaggle.com/mysarahmadbhat/bmw-used-car-listing\", \"https://www.climate.gov/maps-data/datasets\"]\n  for url in url_list:\n    # make a GET request to fetch the raw HTML content\n    web_content = requests.get(url).content\n    # parse the html content\n    soup = BeautifulSoup(web_content, \"html.parser\")\n    # remove any newlines and extra spaces from left and right\n    extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n    # find all table rows and data cells within\n    stats = [] \n    all_rows = soup.find_all('tr')\n    for row in all_rows:\n        stat = extract_contents(row.find_all('td')) \n    # notice that the data that we require is now a list of length 5\n        if len(stat) == 5:\n            stats.append(stat)\n    #now convert the data into a pandas dataframe for further processing\n    new_cols = []\n    for each_new_col in row:\n      kaggle_data = pd.DataFrame(data = kaggle_data, columns = each_new_col)\n      kaggle_data.head()\n      #scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype\n      kaggle_data[each_new_col] = state_data[each_new_col].map(int)\n\n    #dataset\n    from mlbox.optimisation import Optimiser, Regressor\n    \n    #evaluating the pipeline\n    opt = Optimiser()\n    params = {\"ne__numerical_strategy\" : 0, \"ce__strategy\" : \"label_encoding\", \"fs__threshold\" : 0.1, \"stck__base_estimators\" : [Regressor(strategy=\"RandomForest\"), Regressor(strategy=\"ExtraTrees\")], \"est__strategy\" : \"Linear\"}\n    df = {\"train\" : pd.DataFrame(train_data.iloc[:,:-1]), \"target\" : pd.Series(test_data.iloc[:,-1])}\n\n    #build a keras model\n    import tensorflow as tf\n    from tensorflow import keras\n    from tensorflow.keras import layers\n    model = keras.Sequential()\n    #relu: Rectified Linear Unit\n    # Adds a densely-connected layer with 64 units to the model:\n    model.add(keras.layers.Dense(64, activation='relu'))\n    # Add another:\n    model.add(keras.layers.Dense(64, activation='relu'))\n    # Add a softmax layer with 10 output units:\n    model.add(keras.layers.Dense(10, activation='softmax'))\n    #define a ConvModel\n    class ConvModel(tf.keras.Model):\n        def __init__(self, nfs, input_shape, output_shape, use_bn=False, use_dp=False):\n            super(ConvModel, self).__init__(name='mlp')\n            self.use_bn = use_bn\n            self.use_dp = use_dp\n            self.num_classes = num_classes\n\n            # backbone layers\n            self.convs = [ConvLayer(nfs[0], s=1, input_shape=input_shape)]\n            self.convs += [ConvLayer(nf) for nf in nfs[1:]]\n            # classification layers\n            self.convs.append(AveragePooling2D())\n            self.convs.append(Dense(output_shape, activation='softmax'))\n\n        def call(self, inputs):\n            for layer in self.convs: inputs = layer(inputs)\n            return inputs\n    #compile the model\n    model.compile(loss='categorical crossentropy', metrics=['accuracy'], optimizer='rmsprop')\n    model.build((None, 32, 32, 3))\n\n    model.summary()\n\n    import requests\n    from bs4 import BeautifulSoup\n    # Import H2O GBM:\n    from h2o.estimators.gbm import H2OGradientBoostingEstimator\n    # make a GET request to fetch the raw HTML content\n    web_content = requests.get(url).content\n    # parse the html content\n    soup = BeautifulSoup(web_content, \"html.parser\")\n    # remove any newlines and extra spaces from left and right\n    extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n    # find all table rows and data cells within\n    stats = [] \n    all_rows = soup.find_all('tr')\n    for row in all_rows:\n      stat = extract_contents(row.find_all('td')) \n      # notice that the data that we require is now a list of length 5\n      if len(stat) == 5:\n        stats.append(stat)\n      #now convert the data into a pandas dataframe for further processing\n      new_cols = []\n      for each_new_col in row:\n        stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n        stats_data.head()\n        #scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype\n        kaggle_data[each_new_col] = stats_data[each_new_col].map(int)\n        X, y = stats_data\n        X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=11)\n        \n        history = model.fit(x_train, y_train,\n                        batch_size=64,\n                        epochs=1000)\n\n        model.summary()\n        input_shape = (2, 3, 4)\n        x1 = tf.random.normal(input_shape)\n        x2 = tf.random.normal(input_shape)\n        y = tf.keras.layers.Add()([x1, x2])\n        print(y.shape)\n\n        tf.keras.layers.LSTM(3, activation='tanh', recurrent_activation='sigmoid',\n            use_bias=True, kernel_initializer='glorot_uniform',\n            recurrent_initializer='orthogonal',\n            bias_initializer='zeros', unit_forget_bias=True, dropout=0.0, recurrent_dropout=0.0,\n            return_sequences=False, return_state=False, go_backwards=False, stateful=False,\n            time_major=False, unroll=False)\n\n        #define a ConvLayer\n        class ConvLayer(Layer) :\n            def __init__(self, nf, ks=3, s=2, **kwargs):\n                self.nf = nf\n                self.grelu = GeneralReLU(leak=0.01)\n                self.conv = (Conv2D(filters     = nf,\n                                    kernel_size = ks,\n                                    strides     = s,\n                                    padding     = \"same\",\n                                    use_bias    = False,\n                                    activation  = \"linear\"))\n                super(ConvLayer, self).__init__(**kwargs)\n\n            def rsub(self): return -self.grelu.sub\n            def set_sub(self, v): self.grelu.sub = -v\n            def conv_weights(self): return self.conv.weight[0]\n\n            def build(self, input_shape):\n                # No weight to train.\n                super(ConvLayer, self).build(input_shape)  # Be sure to call this at the end\n\n            def compute_output_shape(self, input_shape):\n                output_shape = (input_shape[0],\n                                input_shape[1]/2,\n                                input_shape[2]/2,\n                                self.nf)\n                return output_shape\n\n        def call(self, x):\n            return self.grelu(self.conv(x))\n\n        def __repr__(self):\n            return f'ConvLayer(nf={self.nf}, activation={self.grelu})'\n\n    opt.evaluate(params, df)\n\n    datasets_dict = {\n    \"iris\": datasets.load_iris(), \n    \"boston\": datasets.load_boston(),\n    \"breast_cancer\": datasets.load_breast_cancer(),\n    \"diabetes\": datasets.load_diabetes(),\n    \"wine\": datasets.load_wine(),\n    \"linnerud\": datasets.load_linnerud(),\n    \"digits\": datasets.load_digits(),\n    \"kaggle_data_list\": \n    pd.DataFrame({\n    \"Latest_Covid-19_India_Status\":pd.read_csv(\"Latest Covid-19 India Status.csv\", sep=','),\n    \"Pueblos_Magicos\": pd.read_csv(\"pueblosMagicos.csv\", sep=','),\n    \"Apple_iphone_SE_reviews&ratings\": pd.read_csv(\"APPLE_iPhone_SE.csv\", sep=',')\n    })\n                  }\n\n    if len(datasets_dict[\"kaggle_data_list\"])!=0:\n      for i in range(len(datasets_dict.get(\"kaggle_data_list\"))):\n        df=df.iloc[:]\n        print(df.head())\n        print(df.tail())\n        print(df.info())\n        print(df.describe())\n\n        from autoPyTorch import AutoNetClassification\n        # data and metric imports\n        import sklearn.model_selection\n        import sklearn.metrics\n        X, y = df.to_numpy()\n        X_train, X_test, y_train, y_test = \\\n                sklearn.model_selection.train_test_split(X, y, random_state=1)\n\n        # running Auto-PyTorch\n        autoPyTorch = AutoNetClassification(\"tiny_cs\",  # config preset\n                                            log_level='info',\n                                            max_runtime=999999999**10000000,\n                                            min_budget=30,\n                                            max_budget=999999999*100000)\n        autoPyTorch.fit(X_train, y_train, validation_split=0.3)\n        y_pred = autoPyTorch.predict(X_test)\n\n        print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred))\n\n\n    else:\n      for each_dataset in datasets_dict:\n        print(each_dataset,\" dataset:\")\n        print(\"Data: \",each_dataset.data)\n        print(\"Target: \", each_dataset.target)\n        print(\"Target names: \", each_dataset.target_names)\n        print(\"Description: \", each_dataset.DESCR)\n        #shape\n        print(\"Shape of the data: \", each_dataset.data.shape)\n        print(\"Shape of the target: \",each_dataset.target.shape)\n        #type\n        print(\"Type of the data: \", type(each_dataset.data.shape))\n        print(\"Type of the data: \", type(each_dataset.target.shape))\n        #dimensions\n        print(\"Number of dimensions of the data: \", each_dataset.data.ndim)\n        print(\"Number of dimensions of the target: \",each_dataset.target.ndim)\n        #number of samples and features\n        n_samples, n_features = each_dataset.data.shape\n        print(\"Number of samples: \", n_samples)\n        print(\"Number of features: \", n_features)\n        #keys\n        print(\"Keys: \", each_dataset.keys())\n        X, y = digits.data, digits.target\n        #view the first and last 5 rows of the pandas dataframe\n        df=pd.DataFrame(X, columns=digits.feature_names)\n        print(df.head())\n        print(df.tail())\n        #print(digits.data[0])\n\n        #visualize data on its principal components\n        #PCA\n        from sklearn.decomposition import PCA\n        import matplotlib.pyplot as plt\n\n        pca = PCA(n_components=2)\n        proj = pca.fit_transform(each_dataset.data)\n        plt.scatter(proj[:,0], proj[:,1], c=each_dataset.target, cmap=\"Paired\")\n        plt.colorbar()\n\n        #Gaussian Naive-Bayes classification\n        from sklearn.naive_bayes import GaussianNB\n        from sklearn.model_selection import train_test_split\n\n        # split the dataset into training and validation sets\n        X_train, X_test, y_train, y_test = train_test_split(\n            each_dataset.data, each_dataset.target)\n\n        # train the model\n        clf = GaussianNB()\n        clf.fit(X_train, y_train)\n\n        # use the model to predict the labels of the test data\n        predicted = clf.predict(X_test)\n        expected = y_test\n\n        # Plot the prediction\n        fig = plt.figure(figsize=(6, 6))  # figure size in inches\n        fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n\n        # plot the digits: each image is 8x8 pixels\n        for i in range(64):\n            ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n            ax.imshow(X_test.reshape(-1, 8, 8)[i], cmap=plt.cm.binary,\n                      interpolation='nearest')\n\n            # label the image with the target value\n            if predicted[i] == expected[i]:\n                ax.text(0, 7, str(predicted[i]), color='green')\n            else:\n                ax.text(0, 7, str(predicted[i]), color='red')\n\n        #Quantify performance:\n        #number of correct matches\n        matches = (predicted == expected)\n        print(matches.sum())\n        #total nunber of data points\n        print(len(matches))\n        #ratio of correct predictions\n        matches.sum() / float(len(matches))\n\n        #Classification report\n        from sklearn import metrics\n        print(metrics.classification_report(expected, predicted))\n        #Obtain the confusion matrix\n        print(metrics.confusion_matrix(expected, predicted))\n        plt.show() \n\n\n        #AutoGluon:\n        #Tabular prediction with AutoGluon\n        #Predicting Columns in a Table\n        from autogluon.tabular import TabularDataset, TabularPredictor\n        train_data = TabularDataset(each_dataset)\n        subsample_size = 55500000  # subsample subset of data for faster demo\n        train_data = train_data.sample(n=subsample_size, random_state=0)\n        train_data.head()\n        label = 'class'\n        print(\"Summary of class variable: \\n\", train_data[label].describe())\n        #use AutoGluon to train multiple models\n        save_path = 'agModels-predictClass'  # specifies folder to store trained models\n        predictor = TabularPredictor(label=label, path=save_path).fit(train_data)\n        test_data = TabularDataset(each_dataset)\n        y_test = test_data[label]  # values to predict\n        test_data_nolab = test_data.drop(columns=[label])  # delete label column to prove we're not cheating\n        test_data_nolab.head()\n        #predictor = TabularPredictor.load(save_path)\n        y_pred = predictor.predict(test_data_nolab)\n        print(\"Predictions:  \\n\", y_pred)\n        perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)\n        predictor.leaderboard(test_data, silent=True)\n        from autogluon.tabular import TabularPredictor\n        predictor = TabularPredictor(label=label).fit(train_data=each_dataset)\n        #.fit() returns a predictor object\n        pred_probs = predictor.predict_proba(test_data_nolab)\n        pred_probs.head(5)\n        #summarize what happened during fit\n        results = predictor.fit_summary(show_plot=True)\n        print(\"AutoGluon infers problem type is: \", predictor.problem_type)\n        print(\"AutoGluon identified the following types of features:\")\n        print(predictor.feature_metadata)\n        predictor.leaderboard(test_data, silent=True)\n        predictor.predict(test_data, model='LightGBM')\n        #Maximizing predictive performance\n        time_limit = 11  \n        metric = 'roc_auc'  # specify the evaluation metric here\n        predictor = TabularPredictor(label, eval_metric=metric).fit(train_data, time_limit=time_limit, presets='best_quality')\n        predictor.leaderboard(test_data, silent=True)\n\n        #Regression (predicting numeric table columns)\n        column = 'column'\n        print(\"Summary of PUEBLO variable: \\n\", train_data[column].describe())\n        predictor_column = TabularPredictor(label=column, path=\"agModels-predictAge\").fit(train_data, time_limit=60)\n        performance = predictor_column.evaluate(test_data)\n        #see the per-model performance\n        predictor_column.leaderboard(test_data, silent=True)\n        \n\n        #MLbox:\n        from mlbox.optimisation import Optimiser\n        from sklearn import datasets\n        best = opt.optimise(space, df, 3)\n        #optimising the pipeline\n        opt = Optimiser()\n        space = {\n        'fs__strategy':{\"search\":\"choice\",\"space\":[\"variance\",\"rf_feature_importance\"]},\n        'est__colsample_bytree':{\"search\":\"uniform\", \"space\":[0.3,0.7]}\n        }\n        df = {\"train\" : pd.DataFrame(each_dataset.data), \"target\" : pd.Series(each_dataset.target)} \n        #evaluating the pipeline\n        opt = Optimiser()\n        params = {\n        \"ne__numerical_strategy\" : 0,\n        \"ce__strategy\" : \"label_encoding\",\n        \"fs__threshold\" : 0.1,\n        \"stck__base_estimators\" : [Regressor(strategy=\"RandomForest\"), Regressor(strategy=\"ExtraTrees\")],\n        \"est__strategy\" : \"Linear\"\n        }\n        df = {\"train\" : pd.DataFrame(each_dataset.data), \"target\" : pd.Series(each_dataset.target)}\n        opt.evaluate(params, df)\n\n\n        #TPOT\n        #Classification\n        from tpot import TPOTClassifier\n        from sklearn.model_selection import train_test_split\n        #perform a train test split\n        X_train, X_test, y_train, y_test = train_test_split(each_dataset.data, each_dataset.target, train_size=0.75, test_size=0.25)\n        \n        tpot=TPOTClassifier(generations=99, population_size=99, mutation_rate=0.7, crossover_rate=0.3, random_state=111, cv=5, subsample=0.98, verbosity=2, n_jobs=-2, max_eval_time_mins=0.00000001, config_dict='TPOT light', memory='áuto', log_file='tpot_datasets_logs')\n        tpot.fit(X_train, y_train)\n        print(tpot.score(X_test, y_test))\n        tpot.export('tpot_datasets_pipeline.py')\n\n        plt.hist(each_dataset.target)\n\n        for index, feature_name in enumerate(each_dataset.feature_names):\n          plt.figure()\n          plt.scatter(each_dataset.data[:, index], each_dataset.target) \n          plt.show()\n\n        from sklearn import model_selection\n        X = each_dataset.data\n        y = each_dataset.target\n\n        X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y,\n                                                test_size=0.25, random_state=0)\n\n        print(\"%r, %r, %r\" % (X.shape, X_train.shape, X_test.shape))\n\n        clf = KNeighborsClassifier(n_neighbors=1).fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        print(metrics.confusion_matrix(y_test, y_pred))\n        print(metrics.classification_report(y_test, y_pred))\n\n        #Auto-Pytorch\n        from autoPyTorch import AutoNetClassification\n\n        # data and metric imports\n        import sklearn.model_selection\n        import sklearn.datasets\n        import sklearn.metrics\n        X, y = each_dataset(return_X_y=True)\n        X_train, X_test, y_train, y_test = \\\n                sklearn.model_selection.train_test_split(X, y, random_state=1)\n\n        # running Auto-PyTorch on the datasets\n        autoPyTorch = AutoNetClassification(\"tiny_cs\",  # config preset\n                                            log_level='info',\n                                            max_runtime=999999999**10000000,\n                                            min_budget=30,\n                                            max_budget=999999999*100000)\n        autoPyTorch.fit(X_train, y_train, validation_split=0.3)\n        y_pred = autoPyTorch.predict(X_test)\n\n        print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred))\n\n        #Auto-Sklearn\n        from sklearn import datasets\n        import autosklearn.classification\n        cls = autosklearn.classification.AutoSklearnClassifier()\n        \n        X, y = each_dataset(return_X_y=True)\n        X_train, X_test, y_train, y_test = \\\n                sklearn.model_selection.train_test_split(X, y, random_state=1)\n\n        cls.fit(X_train, y_train)\n        predictions = cls.predict(X_test)\n\n        import sklearn.model_selection\n        import sklearn.metrics\n        if __name__ == \"__main__\":\n            X, y = each_dataset(return_X_y=True)\n            X_train, X_test, y_train, y_test = \\\n                    sklearn.model_selection.train_test_split(X, y, random_state=1)\n            automl = autosklearn.classification.AutoSklearnClassifier()\n\n            automl.fit(X_train, y_train)\n            y_hat = automl.predict(X_test)\n            print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_hat))\n\n        import numpy as np\n        import tensorflow as tf\n        import autokeras as ak\n        input_node = ak.ImageInput()\n        output_node = ak.Normalization()(input_node)\n        output_node1 = ak.ConvBlock()(output_node)\n        output_node2 = ak.ResNetBlock(version=\"v2\")(output_node)\n        output_node = ak.Merge()([output_node1, output_node2])\n        output_node = ak.ClassificationHead()(output_node)\n\n        auto_model = ak.AutoModel(inputs=input_node, outputs=output_node, overwrite=True, max_trials=100)\n        #prepare data to run the model\n        (x_train, y_train), (x_test, y_test) = each_datset\n        print(x_train.shape)\n        print(y_train.shape)\n        print(y_train[:3])\n\n        # Feed the AutoModel with training data.\n        auto_model.fit(x_train[:100], y_train[:100], epochs=1000)\n        # Predict with the best model\n        predicted_y = auto_model.predict(x_test)\n        # Evaluate the best model with testing data\n        print(auto_model.evaluate(x_test, y_test))\n\n        #implement new block\n        class SingleDenseLayerBlock(ak.Block):\n            def build(self, hp, inputs=None):\n                # Get the input_node from inputs.\n                input_node = tf.nest.flatten(inputs)[0]\n                layer = tf.keras.layers.Dense(\n                    hp.Int(\"num_units\", min_value=32, max_value=512, step=32)\n                )\n                output_node = layer(input_node)\n                return output_node\n\n        # Build the AutoModel\n        input_node = ak.Input()\n        output_node = SingleDenseLayerBlock()(input_node)\n        output_node = ak.RegressionHead()(output_node)\n        auto_model = ak.AutoModel(input_node, output_node, overwrite=True, max_trials=100)\n        # Prepare Data\n        num_instances = 100\n        x_train = np.random.rand(num_instances, 20).astype(np.float32)\n        y_train = np.random.rand(num_instances, 1).astype(np.float32)\n        x_test = np.random.rand(num_instances, 20).astype(np.float32)\n        y_test = np.random.rand(num_instances, 1).astype(np.float32)\n        # Train the model\n        auto_model.fit(x_train, y_train, epochs=1000)\n        print(auto_model.evaluate(x_test, y_test))\n\n  print(kwargs)\n  #return the trained model\n  return trained_model","metadata":{"execution":{"iopub.status.busy":"2021-09-25T16:08:11.311850Z","iopub.status.idle":"2021-09-25T16:08:11.312165Z","shell.execute_reply.started":"2021-09-25T16:08:11.312006Z","shell.execute_reply":"2021-09-25T16:08:11.312021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datasets_dict = {\"pets\":\"https://www.kaggle.com/c/petfinder-pawpularity-score/data\", \"rsna\":\"https://www.kaggle.com/c/rsna-miccai-brain-tumor-radiogenomic-classification/data\", nfl_health&safety\":\"https://www.kaggle.com/c/nfl-health-and-safety-helmet-assignment\", \"nfl_big_data_bowl\":\"https://www.kaggle.com/c/nfl-big-data-bowl-2022/data\"}\ntrained_model = automl(dasets_dict)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T16:08:11.314547Z","iopub.status.idle":"2021-09-25T16:08:11.314886Z","shell.execute_reply.started":"2021-09-25T16:08:11.314713Z","shell.execute_reply":"2021-09-25T16:08:11.314733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\n# Import H2O GBM:\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\n# make a GET request to fetch the raw HTML content\nweb_content = requests.get(url).content\n# parse the html content\nsoup = BeautifulSoup(web_content, \"html.parser\")\n# remove any newlines and extra spaces from left and right\nextract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n# find all table rows and data cells within\nstats = [] \nall_rows = soup.find_all('tr')\nurl = \"https://www.kaggle.com/c/nfl-health-and-safety-helmet-assignment/data?select=train_baseline_helmets.csv\")\nfor row in all_rows:\n      stat = extract_contents(row.find_all('td')) \n      # notice that the data that we require is now a list of length 5\n      if len(stat) == 5:\n        stats.append(stat)\n      #now convert the data into a pandas dataframe for further processing\n      new_cols = []\n      for each_new_col in row:\n        stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n        stats_data.head()\n        #scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype\n        kaggle_data[each_new_col] = stats_data[each_new_col].map(int)\n        X, y = stats_data\n        X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=11)\n        \n        history = model.fit(x_train, y_train,\n                        batch_size=64,\n                        epochs=1000)\n\n        model.summary()\n        input_shape = (2, 3, 4)\n        x1 = tf.random.normal(input_shape)\n        x2 = tf.random.normal(input_shape)\n        y = tf.keras.layers.Add()([x1, x2])\n        print(y.shape)\n\n        tf.keras.layers.LSTM(3, activation='tanh', recurrent_activation='sigmoid',\n            use_bias=True, kernel_initializer='glorot_uniform',\n            recurrent_initializer='orthogonal',\n            bias_initializer='zeros', unit_forget_bias=True, dropout=0.0, recurrent_dropout=0.0,\n            return_sequences=False, return_state=False, go_backwards=False, stateful=False,\n            time_major=False, unroll=False)\n\n        #define a ConvLayer\n        class ConvLayer(Layer) :\n            def __init__(self, nf, ks=3, s=2, **kwargs):\n                self.nf = nf\n                self.grelu = GeneralReLU(leak=0.01)\n                self.conv = (Conv2D(filters     = nf,\n                                    kernel_size = ks,\n                                    strides     = s,\n                                    padding     = \"same\",\n                                    use_bias    = False,\n                                    activation  = \"linear\"))\n                super(ConvLayer, self).__init__(**kwargs)\n\n            def rsub(self): return -self.grelu.sub\n            def set_sub(self, v): self.grelu.sub = -v\n            def conv_weights(self): return self.conv.weight[0]\n\n            def build(self, input_shape):\n                # No weight to train.\n                super(ConvLayer, self).build(input_shape)  # Be sure to call this at the end\n\n            def compute_output_shape(self, input_shape):\n                output_shape = (input_shape[0],\n                                input_shape[1]/2,\n                                input_shape[2]/2,\n                                self.nf)\n                return output_shape\n\n        def call(self, x):\n            return self.grelu(self.conv(x))\n\n        def __repr__(self):\n            return f'ConvLayer(nf={self.nf}, activation={self.grelu})'\n\n    opt.evaluate(params, df)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T16:08:11.317306Z","iopub.status.idle":"2021-09-25T16:08:11.317639Z","shell.execute_reply.started":"2021-09-25T16:08:11.317476Z","shell.execute_reply":"2021-09-25T16:08:11.317491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(trained_model)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T16:08:11.320751Z","iopub.status.idle":"2021-09-25T16:08:11.321110Z","shell.execute_reply.started":"2021-09-25T16:08:11.320904Z","shell.execute_reply":"2021-09-25T16:08:11.320919Z"},"trusted":true},"execution_count":null,"outputs":[]}]}